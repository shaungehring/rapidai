{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"RapidAI \u26a1The Python Framework for Lightning-Fast AI Prototypes","text":"<p>     Build production-ready AI applications in under an hour. Zero-config LLM integration, streaming by default, batteries included.   </p> Get Started View Tutorial GitHub"},{"location":"#features","title":"\u2728 Features","text":"\ud83d\ude80 Zero-Config LLM Integration <p>Built-in clients for OpenAI, Anthropic, Cohere, and local models with a unified interface. Swap providers with one line of code.</p> \ud83d\udce1 Streaming by Default <p>Server-Sent Events (SSE) and WebSocket streaming built into routes, not bolted on. Real-time AI responses out of the box.</p> \ud83e\udde0 Smart Memory <p>Conversation tracking per user with multiple backend options. Redis, PostgreSQL, or in-memory - you choose.</p> \ud83d\udcbe Intelligent Caching <p>LLM response caching that understands semantic similarity. Save money and improve response times automatically.</p> \ud83d\udcdd Prompt Management <p>Version, test, and swap prompts without code changes. Jinja2 templating with hot reloading in development.</p> \ud83d\udd0d RAG in Minutes <p>Document parsing, vector DB, and retrieval with 2-line setup. Built-in support for PDFs, DOCX, and more.</p>"},{"location":"#quick-example","title":"\ud83c\udfaf Quick Example","text":"app.py<pre><code>from rapidai import App, LLM, stream\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/chat\", methods=[\"POST\"])\n@stream\nasync def chat(message: str):\n    \"\"\"Stream a chat response.\"\"\"\n    response = await llm.chat(message, stream=True)\n    async for chunk in response:\n        yield chunk\n\nif __name__ == \"__main__\":\n    app.run()\n</code></pre> python app.py Test it<pre><code>curl -X POST http://localhost:8000/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"message\": \"Hello, AI!\"}'\n</code></pre>"},{"location":"#why-rapidai","title":"\ud83c\udf1f Why RapidAI?","text":"<p>Convention over Configuration</p> <p>Sensible defaults everywhere. Get started in minutes, not hours.</p> <p>Provider Agnostic</p> <p>Never get locked into a single LLM provider. Switch between OpenAI, Anthropic, or local models with ease.</p> <p>Async-First</p> <p>Built from the ground up with async/await for maximum performance.</p> <p>Type-Safe</p> <p>Full type hints for excellent IDE support and fewer runtime errors.</p> <p>Production Ready</p> <p>Error handling, rate limiting, monitoring, and deployment templates included.</p>"},{"location":"#perfect-for","title":"\ud83d\udcca Perfect For","text":"<ul> <li>\ud83d\ude80 Rapid POCs - Test AI features in minutes</li> <li>\ud83c\udfe2 Internal Tools - Build dashboards and automation</li> <li>\ud83d\udcac Chat Interfaces - Customer support and assistants</li> <li>\ud83d\udcda RAG Applications - Document Q&amp;A systems</li> <li>\ud83d\udd04 Document Processing - Automated pipelines</li> <li>\ud83c\udf10 AI-Powered APIs - Production-grade endpoints</li> </ul>"},{"location":"#learn-more","title":"\ud83c\udf93 Learn More","text":"\ud83d\udcd8 Tutorial <p>Step-by-step guide from simple chatbot to production deployment.</p> Start Learning \u2192 \ud83d\udcd6 API Reference <p>Complete documentation of all classes, methods, and decorators.</p> Browse Reference \u2192 \ud83d\ude80 Deployment <p>Deploy to Docker, AWS, GCP, Azure, and more with confidence.</p> Deploy Now \u2192"},{"location":"#community","title":"\ud83e\udd1d Community","text":"<ul> <li>\ud83d\udc1b Report Bugs</li> <li>\ud83d\udca1 Request Features</li> <li>\ud83d\udce3 Join Discord (Coming Soon)</li> <li>\ud83d\udc26 Follow on Twitter</li> </ul>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>RapidAI is licensed under the MIT License. See LICENSE for details.</p> <p>Ready to build something amazing?</p> Get Started Now \u2192"},{"location":"about/changelog/","title":"changelog","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"about/contributing/","title":"contriuuting","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"about/license/","title":"license","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"about/roadmap/","title":"roadmap","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"advanced/background/","title":"Background Jobs","text":"<p>RapidAI provides a powerful background job system for running long-running tasks asynchronously with automatic retry logic and status tracking.</p>"},{"location":"advanced/background/#quick-start","title":"Quick Start","text":"<pre><code>from rapidai import App\nfrom rapidai.background import background\n\napp = App()\n\n@background(max_retries=3)\nasync def process_document(doc_id: str, user_id: str):\n    # Long-running task\n    # Process document, call APIs, etc.\n    return {\"doc_id\": doc_id, \"status\": \"processed\"}\n\n@app.route(\"/process\", methods=[\"POST\"])\nasync def enqueue_processing(doc_id: str, user_id: str):\n    # Enqueue the job\n    job_id = await process_document.enqueue(doc_id=doc_id, user_id=user_id)\n    return {\"job_id\": job_id, \"status\": \"queued\"}\n\n@app.route(\"/status/&lt;job_id&gt;\", methods=[\"GET\"])\nasync def check_status(job_id: str):\n    # Check job status\n    result = await process_document.get_result(job_id)\n\n    return {\n        \"job_id\": result.job_id,\n        \"status\": result.status,\n        \"result\": result.result,\n        \"error\": result.error,\n        \"duration\": result.duration\n    }\n</code></pre>"},{"location":"advanced/background/#features","title":"Features","text":"<ul> <li>Async Execution - Run tasks in the background</li> <li>Automatic Retries - Exponential backoff on failures</li> <li>Status Tracking - Monitor job progress</li> <li>Multiple Backends - In-memory or Redis</li> <li>Job Cancellation - Cancel running jobs</li> <li>Result Retrieval - Get job results when complete</li> </ul>"},{"location":"advanced/background/#job-decorator","title":"Job Decorator","text":""},{"location":"advanced/background/#basic-usage","title":"Basic Usage","text":"<pre><code>from rapidai.background import background\n\n@background()\nasync def send_email(to: str, subject: str, body: str):\n    # Send email via API\n    await email_service.send(to, subject, body)\n    return {\"sent\": True}\n\n# Enqueue job\njob_id = await send_email.enqueue(\n    to=\"user@example.com\",\n    subject=\"Hello\",\n    body=\"Welcome!\"\n)\n</code></pre>"},{"location":"advanced/background/#with-retry-logic","title":"With Retry Logic","text":"<pre><code>@background(max_retries=5)\nasync def call_external_api(url: str):\n    # Will retry up to 5 times with exponential backoff\n    response = await http_client.get(url)\n    return response.json()\n\n# Retry delays: 2s, 4s, 8s, 16s, 32s\n</code></pre>"},{"location":"advanced/background/#custom-queue","title":"Custom Queue","text":"<pre><code>from rapidai.background import background, RedisQueue\n\n# Use Redis queue for persistence\nredis_queue = RedisQueue(url=\"redis://localhost:6379\")\n\n@background(max_retries=3, queue=redis_queue)\nasync def important_task(data: dict):\n    # Task persists across restarts\n    return process(data)\n</code></pre>"},{"location":"advanced/background/#job-status","title":"Job Status","text":"<p>Jobs can be in one of five states:</p> Status Description <code>pending</code> Job queued, waiting to start <code>running</code> Job currently executing <code>completed</code> Job finished successfully <code>failed</code> Job failed after all retries <code>cancelled</code> Job was cancelled"},{"location":"advanced/background/#checking-status","title":"Checking Status","text":"<pre><code>@app.route(\"/jobs/&lt;job_id&gt;\", methods=[\"GET\"])\nasync def get_job_status(job_id: str):\n    result = await my_task.get_result(job_id)\n\n    if result is None:\n        return {\"error\": \"Job not found\"}, 404\n\n    response = {\n        \"job_id\": result.job_id,\n        \"status\": result.status,\n        \"created_at\": result.created_at.isoformat(),\n        \"attempts\": result.attempts,\n        \"max_retries\": result.max_retries\n    }\n\n    if result.is_done:\n        response[\"completed_at\"] = result.completed_at.isoformat()\n        response[\"duration\"] = result.duration\n\n        if result.status == \"completed\":\n            response[\"result\"] = result.result\n        elif result.status == \"failed\":\n            response[\"error\"] = result.error\n\n    return response\n</code></pre>"},{"location":"advanced/background/#job-backends","title":"Job Backends","text":""},{"location":"advanced/background/#in-memory-queue","title":"In-Memory Queue","text":"<p>Default backend, suitable for development and single-server deployments:</p> <pre><code>from rapidai.background import InMemoryQueue\n\nqueue = InMemoryQueue()\n\n@background(queue=queue)\nasync def my_task(data: str):\n    return {\"processed\": data}\n</code></pre> <p>Pros: - Fast - No external dependencies - Simple setup</p> <p>Cons: - Jobs lost on restart - Single-server only - No persistence</p>"},{"location":"advanced/background/#redis-queue","title":"Redis Queue","text":"<p>Production backend with persistence:</p> <pre><code>from rapidai.background import RedisQueue\n\nqueue = RedisQueue(\n    url=\"redis://localhost:6379\",\n    prefix=\"myapp:jobs:\"\n)\n\n@background(queue=queue)\nasync def my_task(data: str):\n    return {\"processed\": data}\n</code></pre> <p>Pros: - Persistent storage - Survives restarts - Multi-server support - Production-ready</p> <p>Cons: - Requires Redis - Slightly slower</p>"},{"location":"advanced/background/#job-management","title":"Job Management","text":""},{"location":"advanced/background/#cancelling-jobs","title":"Cancelling Jobs","text":"<pre><code>@app.route(\"/jobs/&lt;job_id&gt;/cancel\", methods=[\"POST\"])\nasync def cancel_job(job_id: str):\n    cancelled = await my_task.cancel(job_id)\n\n    if cancelled:\n        return {\"status\": \"cancelled\"}\n    else:\n        return {\"error\": \"Job not found or already completed\"}, 404\n</code></pre>"},{"location":"advanced/background/#listing-jobs","title":"Listing Jobs","text":"<pre><code>from rapidai.background import get_queue, JobStatus\n\n@app.route(\"/jobs\", methods=[\"GET\"])\nasync def list_jobs(status: str = None):\n    queue = get_queue()\n\n    # Filter by status if provided\n    job_status = JobStatus(status) if status else None\n    jobs = await queue.list_jobs(status=job_status)\n\n    return {\n        \"jobs\": [\n            {\n                \"job_id\": job.job_id,\n                \"status\": job.status,\n                \"created_at\": job.created_at.isoformat()\n            }\n            for job in jobs\n        ]\n    }\n</code></pre>"},{"location":"advanced/background/#complete-example-document-processing","title":"Complete Example: Document Processing","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.background import background\nfrom rapidai.rag import RAG\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\nrag = RAG()\n\n@background(max_retries=3)\nasync def process_document(filepath: str, user_id: str):\n    \"\"\"Process document in background with RAG.\"\"\"\n    try:\n        # Load and chunk document\n        chunks = await rag.add_document(filepath)\n\n        # Generate summary\n        summary_prompt = f\"Summarize this document: {chunks[0].content[:1000]}\"\n        summary = await llm.complete(summary_prompt)\n\n        return {\n            \"filepath\": filepath,\n            \"chunks\": len(chunks),\n            \"summary\": summary,\n            \"user_id\": user_id\n        }\n    except Exception as e:\n        raise Exception(f\"Processing failed: {str(e)}\")\n\n@app.route(\"/upload\", methods=[\"POST\"])\nasync def upload_document(filepath: str, user_id: str):\n    \"\"\"Upload and process document.\"\"\"\n    job_id = await process_document.enqueue(\n        filepath=filepath,\n        user_id=user_id\n    )\n\n    return {\n        \"job_id\": job_id,\n        \"message\": \"Document processing started\",\n        \"status_url\": f\"/jobs/{job_id}\"\n    }\n\n@app.route(\"/jobs/&lt;job_id&gt;\", methods=[\"GET\"])\nasync def get_job(job_id: str):\n    \"\"\"Get job status and result.\"\"\"\n    result = await process_document.get_result(job_id)\n\n    if not result:\n        return {\"error\": \"Job not found\"}, 404\n\n    response = {\n        \"job_id\": result.job_id,\n        \"status\": result.status,\n        \"attempts\": result.attempts\n    }\n\n    if result.status == \"completed\":\n        response[\"result\"] = result.result\n        response[\"duration\"] = result.duration\n    elif result.status == \"failed\":\n        response[\"error\"] = result.error\n\n    return response\n\nif __name__ == \"__main__\":\n    app.run(port=8000)\n</code></pre>"},{"location":"advanced/background/#best-practices","title":"Best Practices","text":""},{"location":"advanced/background/#1-make-tasks-idempotent","title":"1. Make Tasks Idempotent","text":"<p>Ensure tasks can be safely retried:</p> <pre><code>@background(max_retries=3)\nasync def process_order(order_id: str):\n    # Check if already processed\n    if await db.is_processed(order_id):\n        return {\"already_processed\": True}\n\n    # Process order\n    result = await process(order_id)\n\n    # Mark as processed\n    await db.mark_processed(order_id)\n\n    return result\n</code></pre>"},{"location":"advanced/background/#2-use-appropriate-retry-counts","title":"2. Use Appropriate Retry Counts","text":"<pre><code># Quick tasks - few retries\n@background(max_retries=2)\nasync def send_notification(user_id: str):\n    ...\n\n# Critical tasks - more retries\n@background(max_retries=5)\nasync def process_payment(order_id: str):\n    ...\n\n# Best-effort tasks - no retries\n@background(max_retries=0)\nasync def log_analytics(event: dict):\n    ...\n</code></pre>"},{"location":"advanced/background/#3-handle-errors-gracefully","title":"3. Handle Errors Gracefully","text":"<pre><code>@background(max_retries=3)\nasync def risky_task(data: dict):\n    try:\n        result = await external_api.call(data)\n        return {\"success\": True, \"result\": result}\n    except TemporaryError as e:\n        # Retry on temporary errors\n        raise\n    except PermanentError as e:\n        # Don't retry on permanent errors\n        return {\"success\": False, \"error\": str(e)}\n</code></pre>"},{"location":"advanced/background/#4-provide-status-endpoints","title":"4. Provide Status Endpoints","text":"<pre><code>@app.route(\"/jobs/&lt;job_id&gt;\", methods=[\"GET\"])\nasync def job_status(job_id: str):\n    result = await my_task.get_result(job_id)\n\n    # Provide clear status information\n    return {\n        \"job_id\": job_id,\n        \"status\": result.status,\n        \"progress\": calculate_progress(result),\n        \"estimated_completion\": estimate_time(result)\n    }\n</code></pre>"},{"location":"advanced/background/#5-use-redis-in-production","title":"5. Use Redis in Production","text":"<pre><code>import os\n\n# Use in-memory for development, Redis for production\nbackend = \"redis\" if os.getenv(\"PRODUCTION\") else \"memory\"\n\nfrom rapidai.background import get_queue\n\nqueue = get_queue(\n    backend=backend,\n    url=os.getenv(\"REDIS_URL\") if backend == \"redis\" else None\n)\n</code></pre>"},{"location":"advanced/background/#6-monitor-job-metrics","title":"6. Monitor Job Metrics","text":"<pre><code>from rapidai.monitoring import get_collector\n\n@background(max_retries=3)\nasync def monitored_task(data: dict):\n    collector = get_collector()\n    collector.record_metric(\"jobs.started\", 1)\n\n    try:\n        result = await process(data)\n        collector.record_metric(\"jobs.completed\", 1)\n        return result\n    except Exception as e:\n        collector.record_metric(\"jobs.failed\", 1)\n        raise\n</code></pre>"},{"location":"advanced/background/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/background/#jobs-not-processing","title":"Jobs Not Processing","text":"<pre><code># Ensure job is actually queued\njob_id = await my_task.enqueue(data=\"test\")\nresult = await my_task.get_result(job_id)\nprint(f\"Status: {result.status}\")  # Should be \"running\" or \"completed\"\n</code></pre>"},{"location":"advanced/background/#redis-connection-issues","title":"Redis Connection Issues","text":"<pre><code>from rapidai.background import RedisQueue\n\ntry:\n    queue = RedisQueue(url=\"redis://localhost:6379\")\nexcept Exception as e:\n    print(f\"Redis connection failed: {e}\")\n    # Fall back to in-memory\n    queue = InMemoryQueue()\n</code></pre>"},{"location":"advanced/background/#job-stuck-in-running-state","title":"Job Stuck in Running State","text":"<pre><code># Check if job is actually running\nresult = await my_task.get_result(job_id)\n\nif result.status == \"running\":\n    # Check how long it's been running\n    import datetime\n    running_time = datetime.datetime.now() - result.started_at\n\n    if running_time.seconds &gt; 300:  # 5 minutes\n        # Consider cancelling and retrying\n        await my_task.cancel(job_id)\n</code></pre>"},{"location":"advanced/background/#next-steps","title":"Next Steps","text":"<ul> <li>See API Reference for complete API documentation</li> <li>Learn about Monitoring to track job performance</li> <li>Check Testing for testing background jobs</li> </ul>"},{"location":"advanced/caching/","title":"Caching","text":"<p>RapidAI provides built-in caching to reduce API calls, improve response times, and lower costs. Supports both standard caching and semantic caching for intelligent similarity matching.</p>"},{"location":"advanced/caching/#quick-start","title":"Quick Start","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.cache import cache\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/chat\", methods=[\"POST\"])\n@cache(ttl=3600)  # Cache for 1 hour\nasync def chat(message: str):\n    response = await llm.complete(message)\n    return {\"response\": response}\n</code></pre>"},{"location":"advanced/caching/#features","title":"Features","text":"<ul> <li>Automatic Caching - Decorator-based caching</li> <li>Multiple Backends - In-memory or Redis</li> <li>TTL Support - Time-to-live configuration</li> <li>Semantic Caching - AI-powered similarity matching</li> <li>Cache Keys - Automatic key generation</li> <li>Manual Control - Direct cache access</li> </ul>"},{"location":"advanced/caching/#cache-decorator","title":"Cache Decorator","text":""},{"location":"advanced/caching/#basic-usage","title":"Basic Usage","text":"<pre><code>from rapidai.cache import cache\n\n@app.route(\"/expensive\", methods=[\"POST\"])\n@cache(ttl=3600)\nasync def expensive_operation(data: str):\n    # Expensive computation\n    result = await process(data)\n    return {\"result\": result}\n</code></pre> <p>How it works:</p> <ol> <li>Request arrives with parameters</li> <li>Cache key generated from function name + parameters</li> <li>Check cache for existing result</li> <li>If hit: return cached result</li> <li>If miss: execute function, cache result, return</li> </ol>"},{"location":"advanced/caching/#with-ttl","title":"With TTL","text":"<pre><code>from rapidai.cache import cache\n\n# Cache for 1 hour\n@cache(ttl=3600)\nasync def short_lived(data: str):\n    return await process(data)\n\n# Cache for 1 day\n@cache(ttl=86400)\nasync def long_lived(data: str):\n    return await process(data)\n\n# Cache forever (until manual clear)\n@cache(ttl=None)\nasync def permanent(data: str):\n    return await process(data)\n</code></pre>"},{"location":"advanced/caching/#with-redis-backend","title":"With Redis Backend","text":"<pre><code>from rapidai.cache import cache, RedisCache\n\n# Use Redis for persistence\nredis_cache = RedisCache(url=\"redis://localhost:6379\")\n\n@cache(ttl=7200, backend=redis_cache)\nasync def persistent_cache(data: str):\n    return await process(data)\n</code></pre>"},{"location":"advanced/caching/#semantic-caching","title":"Semantic Caching","text":"<p>Semantic caching uses embeddings to find similar queries instead of exact matches.</p>"},{"location":"advanced/caching/#basic-semantic-cache","title":"Basic Semantic Cache","text":"<pre><code>from rapidai.cache import cache\n\n@app.route(\"/chat\", methods=[\"POST\"])\n@cache(ttl=3600, semantic=True, threshold=0.85)\nasync def chat(message: str):\n    response = await llm.complete(message)\n    return {\"response\": response}\n</code></pre> <p>Example:</p> <pre><code># First request\nresponse1 = await chat(message=\"What is Python?\")\n# Cache miss, calls LLM\n\n# Similar request\nresponse2 = await chat(message=\"Can you explain Python?\")\n# Cache hit! Returns cached response (similarity &gt; 0.85)\n\n# Different request\nresponse3 = await chat(message=\"What is JavaScript?\")\n# Cache miss, different topic\n</code></pre>"},{"location":"advanced/caching/#similarity-threshold","title":"Similarity Threshold","text":"<p>Control how similar queries need to be:</p> <pre><code># Strict matching (default)\n@cache(semantic=True, threshold=0.85)\nasync def strict(query: str):\n    return await llm.complete(query)\n\n# Loose matching\n@cache(semantic=True, threshold=0.70)\nasync def loose(query: str):\n    return await llm.complete(query)\n\n# Very strict matching\n@cache(semantic=True, threshold=0.95)\nasync def very_strict(query: str):\n    return await llm.complete(query)\n</code></pre> <p>Threshold guide:</p> <ul> <li><code>0.95+</code> - Nearly identical queries</li> <li><code>0.85-0.94</code> - Similar questions (recommended)</li> <li><code>0.70-0.84</code> - Related topics</li> <li><code>&lt;0.70</code> - Too loose, may return unrelated results</li> </ul>"},{"location":"advanced/caching/#custom-embedding-model","title":"Custom Embedding Model","text":"<pre><code>from rapidai.cache import SemanticCache\n\n# Use custom model\nsemantic_cache = SemanticCache(\n    model=\"all-mpnet-base-v2\",  # Better accuracy\n    threshold=0.85\n)\n\n@cache(backend=semantic_cache, ttl=3600)\nasync def chat(message: str):\n    return await llm.complete(message)\n</code></pre>"},{"location":"advanced/caching/#cache-backends","title":"Cache Backends","text":""},{"location":"advanced/caching/#in-memory-cache","title":"In-Memory Cache","text":"<p>Default backend, fast but not persistent:</p> <pre><code>from rapidai.cache import InMemoryCache\n\ncache_backend = InMemoryCache()\n\n@cache(backend=cache_backend, ttl=3600)\nasync def my_function(data: str):\n    return await process(data)\n</code></pre> <p>Pros: - Very fast - No external dependencies - Simple setup</p> <p>Cons: - Lost on restart - Single-server only - Limited by RAM</p>"},{"location":"advanced/caching/#redis-cache","title":"Redis Cache","text":"<p>Production backend with persistence:</p> <pre><code>from rapidai.cache import RedisCache\n\ncache_backend = RedisCache(\n    url=\"redis://localhost:6379\",\n    prefix=\"myapp:cache:\"\n)\n\n@cache(backend=cache_backend, ttl=7200)\nasync def my_function(data: str):\n    return await process(data)\n</code></pre> <p>Pros: - Persistent storage - Survives restarts - Multi-server support - Production-ready</p> <p>Cons: - Requires Redis - Slightly slower than in-memory</p>"},{"location":"advanced/caching/#manual-cache-control","title":"Manual Cache Control","text":""},{"location":"advanced/caching/#direct-cache-access","title":"Direct Cache Access","text":"<pre><code>from rapidai.cache import get_cache\n\ncache = get_cache()\n\n# Set value\nawait cache.set(\"key\", {\"data\": \"value\"}, ttl=3600)\n\n# Get value\nresult = await cache.get(\"key\")\n\n# Delete value\nawait cache.delete(\"key\")\n\n# Clear all\nawait cache.clear()\n</code></pre>"},{"location":"advanced/caching/#with-custom-keys","title":"With Custom Keys","text":"<pre><code>from rapidai.cache import get_cache\n\ncache = get_cache()\n\n@app.route(\"/user/&lt;user_id&gt;\", methods=[\"GET\"])\nasync def get_user(user_id: str):\n    # Try cache first\n    cache_key = f\"user:{user_id}\"\n    cached = await cache.get(cache_key)\n\n    if cached:\n        return cached\n\n    # Fetch from database\n    user = await db.get_user(user_id)\n\n    # Cache for 1 hour\n    await cache.set(cache_key, user, ttl=3600)\n\n    return user\n</code></pre>"},{"location":"advanced/caching/#cache-key-generation","title":"Cache Key Generation","text":""},{"location":"advanced/caching/#automatic-keys","title":"Automatic Keys","text":"<p>The decorator generates keys from function name and arguments:</p> <pre><code>@cache(ttl=3600)\nasync def get_weather(city: str, units: str = \"metric\"):\n    return await fetch_weather(city, units)\n\n# Calls with same arguments use same cache\nresult1 = await get_weather(\"NYC\", \"metric\")\n# Key: get_weather:NYC:metric\n\nresult2 = await get_weather(\"NYC\", \"metric\")\n# Same key, cache hit!\n\nresult3 = await get_weather(\"NYC\", \"imperial\")\n# Different key: get_weather:NYC:imperial\n</code></pre>"},{"location":"advanced/caching/#custom-keys","title":"Custom Keys","text":"<pre><code>from rapidai.cache import cache\n\n@cache(ttl=3600, key=lambda city, units: f\"weather:{city}\")\nasync def get_weather(city: str, units: str = \"metric\"):\n    return await fetch_weather(city, units)\n\n# Both use same cache key (units ignored)\nresult1 = await get_weather(\"NYC\", \"metric\")\nresult2 = await get_weather(\"NYC\", \"imperial\")\n# Both hit same cache!\n</code></pre>"},{"location":"advanced/caching/#complete-examples","title":"Complete Examples","text":""},{"location":"advanced/caching/#cached-chat-application","title":"Cached Chat Application","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.cache import cache\nfrom rapidai.memory import ConversationMemory\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\nmemory = ConversationMemory()\n\n@app.route(\"/chat\", methods=[\"POST\"])\n@cache(ttl=3600, semantic=True, threshold=0.85)\nasync def chat(message: str):\n    \"\"\"Chat with semantic caching.\"\"\"\n    response = await llm.complete(message)\n    return {\"response\": response, \"cached\": False}\n\n@app.route(\"/chat/user\", methods=[\"POST\"])\nasync def chat_with_memory(user_id: str, message: str):\n    \"\"\"Chat with memory (no caching - each conversation unique).\"\"\"\n    memory.add_message(user_id, \"user\", message)\n    history = memory.get_history(user_id)\n\n    response = await llm.chat(history)\n\n    memory.add_message(user_id, \"assistant\", response)\n\n    return {\"response\": response}\n\nif __name__ == \"__main__\":\n    app.run()\n</code></pre>"},{"location":"advanced/caching/#cached-rag-system","title":"Cached RAG System","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.rag import RAG\nfrom rapidai.cache import cache\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\nrag = RAG()\n\n@app.on_startup\nasync def load_docs():\n    await rag.add_document(\"docs/manual.pdf\")\n    await rag.add_document(\"docs/faq.txt\")\n\n@cache(ttl=7200, semantic=True, threshold=0.85)\nasync def cached_retrieval(query: str):\n    \"\"\"Cache RAG retrievals - similar questions use cached context.\"\"\"\n    return await rag.retrieve(query, top_k=3)\n\n@app.route(\"/ask\", methods=[\"POST\"])\nasync def ask(question: str):\n    # Use cached retrieval\n    retrieval = await cached_retrieval(question)\n\n    # Build prompt\n    prompt = f\"\"\"Context:\n{retrieval.text}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n\n    # Generate response\n    response = await llm.complete(prompt)\n\n    return {\n        \"response\": response,\n        \"sources\": [s[\"source\"] for s in retrieval.sources]\n    }\n</code></pre>"},{"location":"advanced/caching/#multi-tier-caching","title":"Multi-Tier Caching","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.cache import cache, InMemoryCache, RedisCache\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n# Fast in-memory cache for common queries\nmemory_cache = InMemoryCache()\n\n# Persistent Redis cache for all queries\nredis_cache = RedisCache(url=\"redis://localhost:6379\")\n\n@app.route(\"/chat/fast\", methods=[\"POST\"])\n@cache(backend=memory_cache, ttl=300)  # 5 min memory cache\nasync def fast_chat(message: str):\n    \"\"\"Frequent queries cached in memory.\"\"\"\n    return {\"response\": await llm.complete(message)}\n\n@app.route(\"/chat/persistent\", methods=[\"POST\"])\n@cache(backend=redis_cache, ttl=86400)  # 1 day Redis cache\nasync def persistent_chat(message: str):\n    \"\"\"All queries cached in Redis.\"\"\"\n    return {\"response\": await llm.complete(message)}\n\n@app.route(\"/chat/semantic\", methods=[\"POST\"])\n@cache(semantic=True, threshold=0.85, ttl=3600)\nasync def semantic_chat(message: str):\n    \"\"\"Similar queries share cache.\"\"\"\n    return {\"response\": await llm.complete(message)}\n</code></pre>"},{"location":"advanced/caching/#best-practices","title":"Best Practices","text":""},{"location":"advanced/caching/#1-choose-appropriate-ttl","title":"1. Choose Appropriate TTL","text":"<pre><code># Short TTL for changing data\n@cache(ttl=300)  # 5 minutes\nasync def get_stock_price(symbol: str):\n    return await fetch_price(symbol)\n\n# Long TTL for static data\n@cache(ttl=86400)  # 1 day\nasync def get_company_info(symbol: str):\n    return await fetch_info(symbol)\n\n# No TTL for permanent data\n@cache(ttl=None)\nasync def get_currency_codes():\n    return [\"USD\", \"EUR\", \"GBP\"]\n</code></pre>"},{"location":"advanced/caching/#2-use-semantic-caching-for-llm-calls","title":"2. Use Semantic Caching for LLM Calls","text":"<pre><code># \u2705 Good - semantic caching for similar questions\n@cache(semantic=True, threshold=0.85, ttl=3600)\nasync def chat(message: str):\n    return await llm.complete(message)\n\n# \u274c Avoid - exact matching misses similar queries\n@cache(ttl=3600)  # Only caches identical messages\nasync def chat(message: str):\n    return await llm.complete(message)\n</code></pre>"},{"location":"advanced/caching/#3-use-redis-in-production","title":"3. Use Redis in Production","text":"<pre><code>import os\n\n# Development: in-memory\n# Production: Redis\nbackend = \"redis\" if os.getenv(\"PRODUCTION\") else \"memory\"\n\nfrom rapidai.cache import get_cache\n\ncache_backend = get_cache(backend=backend)\n</code></pre>"},{"location":"advanced/caching/#4-cache-expensive-operations-only","title":"4. Cache Expensive Operations Only","text":"<pre><code># \u2705 Good - cache LLM calls\n@cache(ttl=3600)\nasync def generate_summary(text: str):\n    return await llm.complete(f\"Summarize: {text}\")\n\n# \u274c Avoid - don't cache trivial operations\n@cache(ttl=3600)\nasync def add_numbers(a: int, b: int):\n    return {\"result\": a + b}  # Too fast to benefit\n</code></pre>"},{"location":"advanced/caching/#5-monitor-cache-hit-rates","title":"5. Monitor Cache Hit Rates","text":"<pre><code>from rapidai.cache import get_cache\nfrom rapidai.monitoring import get_collector\n\n@app.route(\"/stats\")\nasync def cache_stats():\n    cache = get_cache()\n    collector = get_collector()\n\n    # Track cache metrics\n    collector.record_metric(\"cache.size\", len(cache._cache))\n\n    return {\n        \"cache_size\": len(cache._cache),\n        \"metrics\": collector.get_summary()\n    }\n</code></pre>"},{"location":"advanced/caching/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/caching/#cache-not-working","title":"Cache Not Working","text":"<pre><code># Ensure decorator is applied correctly\n@cache(ttl=3600)  # \u2705 Correct\nasync def my_function():\n    pass\n\nasync def my_function():  # \u274c Missing decorator\n    pass\n</code></pre>"},{"location":"advanced/caching/#semantic-cache-misses","title":"Semantic Cache Misses","text":"<pre><code># Threshold may be too high\n@cache(semantic=True, threshold=0.95)  # Too strict\nasync def chat(message: str):\n    pass\n\n# Try lower threshold\n@cache(semantic=True, threshold=0.80)  # More permissive\nasync def chat(message: str):\n    pass\n</code></pre>"},{"location":"advanced/caching/#redis-connection-issues","title":"Redis Connection Issues","text":"<pre><code>from rapidai.cache import RedisCache\n\ntry:\n    cache = RedisCache(url=\"redis://localhost:6379\")\nexcept Exception as e:\n    print(f\"Redis connection failed: {e}\")\n    # Fall back to in-memory\n    from rapidai.cache import InMemoryCache\n    cache = InMemoryCache()\n</code></pre>"},{"location":"advanced/caching/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Guide - Optimize with caching</li> <li>Monitoring - Track cache performance</li> <li>Testing - Test cached endpoints</li> </ul>"},{"location":"advanced/cli/","title":"CLI Tool","text":"<p>RapidAI includes a powerful CLI tool for scaffolding projects, running development servers, and deploying to cloud platforms.</p>"},{"location":"advanced/cli/#installation","title":"Installation","text":"<p>The CLI is installed automatically with RapidAI:</p> <pre><code>pip install rapidai\n</code></pre> <p>Verify installation:</p> <pre><code>rapidai --version\n</code></pre>"},{"location":"advanced/cli/#quick-start","title":"Quick Start","text":"<p>Create a new chatbot project:</p> <pre><code>rapidai new my-chatbot\ncd my-chatbot\npip install -r requirements.txt\nrapidai dev\n</code></pre> <p>Your app is now running at <code>http://localhost:8000</code>!</p>"},{"location":"advanced/cli/#commands","title":"Commands","text":""},{"location":"advanced/cli/#rapidai-new","title":"<code>rapidai new</code>","text":"<p>Create a new RapidAI project from a template.</p> <pre><code>rapidai new &lt;project-name&gt; [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>project-name</code> - Name of the project to create</li> </ul> <p>Options:</p> <ul> <li><code>-t, --template</code> - Template to use (chatbot, rag, agent, api) [default: chatbot]</li> <li><code>-d, --directory</code> - Directory to create project in [default: .]</li> </ul> <p>Examples:</p> <pre><code># Create a chatbot project\nrapidai new my-bot\n\n# Create a RAG project\nrapidai new doc-qa --template rag\n\n# Create in a specific directory\nrapidai new my-api --template api --directory ~/projects\n</code></pre> <p>Templates:</p> <ol> <li>chatbot - Simple chatbot with conversation memory</li> <li>rag - RAG application with document upload and Q&amp;A</li> <li>agent - AI agent with analysis and generation endpoints</li> <li>api - REST API with authentication and CORS</li> </ol>"},{"location":"advanced/cli/#rapidai-dev","title":"<code>rapidai dev</code>","text":"<p>Run the development server with hot reload.</p> <pre><code>rapidai dev [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-p, --port</code> - Port to run on [default: 8000]</li> <li><code>-h, --host</code> - Host to bind to [default: 127.0.0.1]</li> <li><code>--reload/--no-reload</code> - Enable/disable auto-reload [default: reload]</li> <li><code>-a, --app</code> - Application module path [default: app:app]</li> </ul> <p>Examples:</p> <pre><code># Run on default port 8000\nrapidai dev\n\n# Run on custom port\nrapidai dev --port 3000\n\n# Disable hot reload\nrapidai dev --no-reload\n\n# Custom app module\nrapidai dev --app myapp:application\n</code></pre> <p>Features:</p> <ul> <li>Auto-reload on file changes</li> <li>Colored console output</li> <li>Clear error messages</li> <li>Powered by Uvicorn</li> </ul>"},{"location":"advanced/cli/#rapidai-deploy","title":"<code>rapidai deploy</code>","text":"<p>Deploy your application to a cloud platform.</p> <pre><code>rapidai deploy &lt;platform&gt; [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>platform</code> - Cloud platform (fly, heroku, vercel, aws)</li> </ul> <p>Options:</p> <ul> <li><code>-n, --app-name</code> - Application name for deployment</li> <li><code>-r, --region</code> - Region to deploy to</li> </ul> <p>Examples:</p> <pre><code># Deploy to Fly.io\nrapidai deploy fly\n\n# Deploy to Heroku with app name\nrapidai deploy heroku --app-name my-app\n\n# Deploy to Vercel\nrapidai deploy vercel\n\n# Deploy to AWS (shows instructions)\nrapidai deploy aws\n</code></pre> <p>Supported Platforms:</p> <ol> <li>Fly.io - Automatic deployment with generated fly.toml</li> <li>Heroku - Git-based deployment with Procfile</li> <li>Vercel - Serverless deployment with vercel.json</li> <li>AWS - Manual deployment instructions</li> </ol>"},{"location":"advanced/cli/#rapidai-test","title":"<code>rapidai test</code>","text":"<p>Run tests for your application.</p> <pre><code>rapidai test [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--coverage/--no-coverage</code> - Run with coverage [default: coverage]</li> <li><code>-v, --verbose</code> - Verbose output</li> </ul> <p>Examples:</p> <pre><code># Run tests with coverage\nrapidai test\n\n# Run without coverage\nrapidai test --no-coverage\n\n# Verbose output\nrapidai test --verbose\n</code></pre> <p>Requirements:</p> <ul> <li>pytest must be installed</li> <li>Tests should be in <code>tests/</code> directory</li> </ul>"},{"location":"advanced/cli/#rapidai-docs","title":"<code>rapidai docs</code>","text":"<p>Generate and serve project documentation.</p> <pre><code>rapidai docs [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--serve/--build</code> - Serve locally or build for production [default: serve]</li> <li><code>-p, --port</code> - Port to serve docs on [default: 8001]</li> </ul> <p>Examples:</p> <pre><code># Serve docs locally\nrapidai docs\n\n# Build for production\nrapidai docs --build\n\n# Serve on custom port\nrapidai docs --port 3000\n</code></pre> <p>Requirements:</p> <ul> <li>mkdocs and mkdocs-material must be installed</li> <li>Docs should be in <code>docs/</code> directory</li> </ul>"},{"location":"advanced/cli/#project-templates","title":"Project Templates","text":""},{"location":"advanced/cli/#chatbot-template","title":"Chatbot Template","text":"<p>A simple chatbot with conversation memory.</p> <p>Features:</p> <ul> <li>Conversation memory per user</li> <li>Clear history endpoint</li> <li>Environment-based configuration</li> <li>Claude or GPT support</li> </ul> <p>Files created:</p> <pre><code>my-chatbot/\n\u251c\u2500\u2500 app.py              # Main application\n\u251c\u2500\u2500 .env                # Environment variables\n\u251c\u2500\u2500 requirements.txt    # Dependencies\n\u251c\u2500\u2500 README.md          # Project documentation\n\u2514\u2500\u2500 tests/             # Test directory\n</code></pre> <p>Endpoints:</p> <ul> <li><code>POST /chat</code> - Chat with the bot</li> <li><code>POST /clear</code> - Clear conversation history</li> </ul>"},{"location":"advanced/cli/#rag-template","title":"RAG Template","text":"<p>RAG application with document upload and Q&amp;A.</p> <p>Features:</p> <ul> <li>Upload PDFs, DOCX, TXT, HTML, Markdown</li> <li>Semantic search with embeddings</li> <li>Context-aware answers</li> <li>ChromaDB vector storage</li> </ul> <p>Files created:</p> <pre><code>my-rag/\n\u251c\u2500\u2500 app.py              # Main application\n\u251c\u2500\u2500 .env                # Environment variables\n\u251c\u2500\u2500 requirements.txt    # Dependencies (includes RAG extras)\n\u251c\u2500\u2500 README.md          # Project documentation\n\u251c\u2500\u2500 tests/             # Test directory\n\u2514\u2500\u2500 docs/              # Document storage\n</code></pre> <p>Endpoints:</p> <ul> <li><code>POST /upload</code> - Upload documents</li> <li><code>POST /ask</code> - Ask questions</li> <li><code>POST /search</code> - Search documents</li> </ul>"},{"location":"advanced/cli/#agent-template","title":"Agent Template","text":"<p>AI agent with analysis and content generation.</p> <p>Features:</p> <ul> <li>Text analysis with caching</li> <li>Content generation</li> <li>Interactive agent chat</li> <li>Configurable styles</li> </ul> <p>Files created:</p> <pre><code>my-agent/\n\u251c\u2500\u2500 app.py              # Main application\n\u251c\u2500\u2500 .env                # Environment variables\n\u251c\u2500\u2500 requirements.txt    # Dependencies\n\u251c\u2500\u2500 README.md          # Project documentation\n\u2514\u2500\u2500 tests/             # Test directory\n</code></pre> <p>Endpoints:</p> <ul> <li><code>POST /analyze</code> - Analyze text</li> <li><code>POST /generate</code> - Generate content</li> <li><code>POST /chat</code> - Interactive chat</li> </ul>"},{"location":"advanced/cli/#api-template","title":"API Template","text":"<p>REST API with authentication and CORS.</p> <p>Features:</p> <ul> <li>API key authentication</li> <li>CORS enabled</li> <li>Health check endpoint</li> <li>Clean REST design</li> </ul> <p>Files created:</p> <pre><code>my-api/\n\u251c\u2500\u2500 app.py              # Main application\n\u251c\u2500\u2500 .env                # Environment variables\n\u251c\u2500\u2500 requirements.txt    # Dependencies\n\u251c\u2500\u2500 README.md          # Project documentation\n\u2514\u2500\u2500 tests/             # Test directory\n</code></pre> <p>Endpoints:</p> <ul> <li><code>GET /</code> - API information</li> <li><code>POST /complete</code> - Complete prompts</li> <li><code>POST /chat</code> - Chat endpoint</li> <li><code>GET /health</code> - Health check</li> </ul>"},{"location":"advanced/cli/#deployment-guide","title":"Deployment Guide","text":""},{"location":"advanced/cli/#flyio-deployment","title":"Fly.io Deployment","text":"<ol> <li>Install flyctl:</li> </ol> <pre><code>curl -L https://fly.io/install.sh | sh\n</code></pre> <ol> <li>Login:</li> </ol> <pre><code>flyctl auth login\n</code></pre> <ol> <li>Deploy:</li> </ol> <pre><code>rapidai deploy fly\n</code></pre> <p>What happens:</p> <ul> <li>Generates <code>fly.toml</code> configuration</li> <li>Creates or updates Fly.io app</li> <li>Deploys using Paketo buildpacks</li> <li>Configures health checks</li> <li>Provides deployment URL</li> </ul> <p>Environment variables:</p> <p>Set secrets with:</p> <pre><code>flyctl secrets set ANTHROPIC_API_KEY=your-key\n</code></pre>"},{"location":"advanced/cli/#heroku-deployment","title":"Heroku Deployment","text":"<ol> <li>Install Heroku CLI:</li> </ol> <pre><code># macOS\nbrew install heroku/brew/heroku\n\n# Or download from https://devcenter.heroku.com/articles/heroku-cli\n</code></pre> <ol> <li>Login:</li> </ol> <pre><code>heroku login\n</code></pre> <ol> <li>Deploy:</li> </ol> <pre><code>rapidai deploy heroku --app-name my-app\n</code></pre> <ol> <li>Push code:</li> </ol> <pre><code>git remote add heroku https://git.heroku.com/my-app.git\ngit push heroku main\n</code></pre> <p>What happens:</p> <ul> <li>Generates <code>Procfile</code></li> <li>Creates Heroku app</li> <li>Sets Python buildpack</li> <li>Provides deployment instructions</li> </ul> <p>Environment variables:</p> <pre><code>heroku config:set ANTHROPIC_API_KEY=your-key -a my-app\n</code></pre>"},{"location":"advanced/cli/#vercel-deployment","title":"Vercel Deployment","text":"<ol> <li>Install Vercel CLI:</li> </ol> <pre><code>npm install -g vercel\n</code></pre> <ol> <li>Login:</li> </ol> <pre><code>vercel login\n</code></pre> <ol> <li>Deploy:</li> </ol> <pre><code>rapidai deploy vercel\n</code></pre> <p>What happens:</p> <ul> <li>Generates <code>vercel.json</code> configuration</li> <li>Deploys as serverless function</li> <li>Provides deployment URL</li> </ul> <p>Environment variables:</p> <p>Set in Vercel dashboard or:</p> <pre><code>vercel env add ANTHROPIC_API_KEY\n</code></pre>"},{"location":"advanced/cli/#aws-deployment","title":"AWS Deployment","text":"<p>AWS deployment is manual. Use:</p> <pre><code>rapidai deploy aws\n</code></pre> <p>This provides instructions for:</p> <ul> <li>AWS Lambda + API Gateway (serverless)</li> <li>AWS ECS/Fargate (containers)</li> <li>AWS Elastic Beanstalk (platform)</li> </ul>"},{"location":"advanced/cli/#development-workflow","title":"Development Workflow","text":""},{"location":"advanced/cli/#1-create-project","title":"1. Create Project","text":"<pre><code>rapidai new my-project --template chatbot\ncd my-project\n</code></pre>"},{"location":"advanced/cli/#2-install-dependencies","title":"2. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"advanced/cli/#3-configure-environment","title":"3. Configure Environment","text":"<p>Edit <code>.env</code>:</p> <pre><code>ANTHROPIC_API_KEY=your-api-key-here\nDEBUG=true\n</code></pre>"},{"location":"advanced/cli/#4-run-development-server","title":"4. Run Development Server","text":"<pre><code>rapidai dev\n</code></pre>"},{"location":"advanced/cli/#5-test","title":"5. Test","text":"<pre><code>rapidai test\n</code></pre>"},{"location":"advanced/cli/#6-deploy","title":"6. Deploy","text":"<pre><code>rapidai deploy fly\n</code></pre>"},{"location":"advanced/cli/#best-practices","title":"Best Practices","text":""},{"location":"advanced/cli/#project-structure","title":"Project Structure","text":"<pre><code>my-project/\n\u251c\u2500\u2500 app.py              # Main application\n\u251c\u2500\u2500 .env                # Environment variables (gitignored)\n\u251c\u2500\u2500 .env.example        # Example environment variables\n\u251c\u2500\u2500 requirements.txt    # Dependencies\n\u251c\u2500\u2500 README.md          # Documentation\n\u251c\u2500\u2500 tests/             # Tests\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_app.py\n\u251c\u2500\u2500 prompts/           # Prompt templates (optional)\n\u2514\u2500\u2500 docs/              # Documents for RAG (optional)\n</code></pre>"},{"location":"advanced/cli/#environment-variables","title":"Environment Variables","text":"<p>Always use <code>.env</code> for secrets:</p> <pre><code># .env\nANTHROPIC_API_KEY=sk-ant-...\nOPENAI_API_KEY=sk-...\nDEBUG=true\n</code></pre> <p>Add <code>.env.example</code> for documentation:</p> <pre><code># .env.example\nANTHROPIC_API_KEY=your-api-key-here\nDEBUG=true\n</code></pre>"},{"location":"advanced/cli/#testing","title":"Testing","text":"<p>Create tests in <code>tests/</code>:</p> <pre><code># tests/test_app.py\nimport pytest\nfrom rapidai.testing import TestClient\n\ndef test_chat():\n    from app import app\n    client = TestClient(app)\n    response = client.post(\"/chat\", json={\"user_id\": \"test\", \"message\": \"hi\"})\n    assert response.status_code == 200\n</code></pre> <p>Run tests:</p> <pre><code>rapidai test\n</code></pre>"},{"location":"advanced/cli/#documentation","title":"Documentation","text":"<p>Keep README.md updated:</p> <pre><code># My Project\n\n## Setup\n\n1. Install dependencies\n2. Set environment variables\n3. Run the dev server\n\n## Endpoints\n\n- POST /chat - Description\n- GET /health - Description\n</code></pre>"},{"location":"advanced/cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/cli/#port-already-in-use","title":"Port Already in Use","text":"<pre><code># Use a different port\nrapidai dev --port 3000\n</code></pre>"},{"location":"advanced/cli/#module-not-found","title":"Module Not Found","text":"<pre><code># Make sure you're in the project directory\ncd my-project\n\n# Specify the module path\nrapidai dev --app app:app\n</code></pre>"},{"location":"advanced/cli/#deployment-fails","title":"Deployment Fails","text":"<pre><code># Check requirements.txt\ncat requirements.txt\n\n# Ensure all files are committed\ngit status\ngit add .\ngit commit -m \"Deploy\"\n</code></pre>"},{"location":"advanced/cli/#missing-dependencies","title":"Missing Dependencies","text":"<pre><code># Install all dependencies\npip install -r requirements.txt\n\n# For development\npip install rapidai[dev]\n</code></pre>"},{"location":"advanced/cli/#next-steps","title":"Next Steps","text":"<ul> <li>See API Reference for complete CLI API documentation</li> <li>Check Configuration for environment variables</li> <li>Learn about Deployment best practices</li> </ul>"},{"location":"advanced/configuration/","title":"configuration","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"advanced/middleware/","title":"middleware","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"advanced/monitoring/","title":"Monitoring","text":"<p>RapidAI includes built-in monitoring and observability features for tracking token usage, costs, latency, and performance metrics.</p>"},{"location":"advanced/monitoring/#quick-start","title":"Quick Start","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.monitoring import monitor, get_dashboard_html\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/chat\", methods=[\"POST\"])\n@monitor(track_tokens=True, track_cost=True)\nasync def chat(message: str):\n    response = await llm.complete(message)\n    return {\n        \"response\": response,\n        \"tokens_used\": 150,  # Add if available\n        \"model\": \"claude-3-haiku-20240307\"\n    }\n\n@app.route(\"/metrics\")\nasync def metrics():\n    # Serve built-in dashboard\n    return get_dashboard_html()\n\nif __name__ == \"__main__\":\n    app.run(port=8000)\n</code></pre> <p>Visit <code>http://localhost:8000/metrics</code> to see the dashboard!</p>"},{"location":"advanced/monitoring/#features","title":"Features","text":"<ul> <li>Token Tracking - Monitor token usage per model</li> <li>Cost Calculation - Automatic cost tracking for all providers</li> <li>Latency Metrics - Request duration and performance</li> <li>Error Tracking - Success rates and error monitoring</li> <li>Built-in Dashboard - Beautiful HTML dashboard with auto-refresh</li> <li>Metrics API - Programmatic access to all metrics</li> </ul>"},{"location":"advanced/monitoring/#monitor-decorator","title":"@monitor Decorator","text":""},{"location":"advanced/monitoring/#basic-usage","title":"Basic Usage","text":"<pre><code>from rapidai.monitoring import monitor\n\n@app.route(\"/endpoint\", methods=[\"POST\"])\n@monitor()\nasync def my_endpoint(data: str):\n    # Request metrics automatically tracked\n    return {\"result\": \"success\"}\n</code></pre>"},{"location":"advanced/monitoring/#with-token-tracking","title":"With Token Tracking","text":"<pre><code>@app.route(\"/chat\", methods=[\"POST\"])\n@monitor(track_tokens=True, track_cost=True)\nasync def chat(message: str):\n    response = await llm.complete(message)\n\n    # Include these fields for tracking\n    return {\n        \"response\": response,\n        \"tokens_used\": 100,  # Total tokens\n        \"model\": \"claude-3-haiku-20240307\"\n    }\n</code></pre>"},{"location":"advanced/monitoring/#metrics-collector","title":"Metrics Collector","text":""},{"location":"advanced/monitoring/#recording-custom-metrics","title":"Recording Custom Metrics","text":"<pre><code>from rapidai.monitoring import get_collector\n\ncollector = get_collector()\n\n# Record a metric\ncollector.record_metric(\n    name=\"custom.metric\",\n    value=42.5,\n    tags={\"environment\": \"production\"}\n)\n</code></pre>"},{"location":"advanced/monitoring/#recording-requests","title":"Recording Requests","text":"<pre><code>collector.record_request(\n    endpoint=\"/chat\",\n    method=\"POST\",\n    duration=0.523,  # seconds\n    status_code=200,\n    tokens_used=150,\n    model=\"claude-3-haiku-20240307\",\n    error=None\n)\n</code></pre>"},{"location":"advanced/monitoring/#getting-metrics","title":"Getting Metrics","text":"<pre><code># Get all metrics\nmetrics = collector.get_metrics()\n\n# Get filtered metrics\nrecent_metrics = collector.get_metrics(\n    name=\"custom.metric\",\n    since=datetime.now() - timedelta(hours=1)\n)\n\n# Get request metrics\nrequests = collector.get_requests(\n    endpoint=\"/chat\",\n    since=datetime.now() - timedelta(minutes=30)\n)\n</code></pre>"},{"location":"advanced/monitoring/#model-usage-tracking","title":"Model Usage Tracking","text":""},{"location":"advanced/monitoring/#automatic-tracking","title":"Automatic Tracking","text":"<p>When using <code>@monitor(track_tokens=True, track_cost=True)</code>, token usage and costs are automatically tracked:</p> <pre><code>@monitor(track_tokens=True, track_cost=True)\nasync def generate_text(prompt: str):\n    response = await llm.complete(prompt)\n    return {\n        \"response\": response,\n        \"tokens_used\": 200,\n        \"model\": \"gpt-4o-mini\"\n    }\n</code></pre>"},{"location":"advanced/monitoring/#get-model-usage","title":"Get Model Usage","text":"<pre><code>collector = get_collector()\n\n# Get all model usage\nusage = collector.get_model_usage()\n\nfor model, stats in usage.items():\n    print(f\"Model: {model}\")\n    print(f\"  Total tokens: {stats.total_tokens}\")\n    print(f\"  Total cost: ${stats.total_cost:.4f}\")\n    print(f\"  Requests: {stats.request_count}\")\n</code></pre>"},{"location":"advanced/monitoring/#get-specific-model","title":"Get Specific Model","text":"<pre><code># Get usage for specific model\nclaude_usage = collector.get_model_usage(model=\"claude-3-haiku-20240307\")\n</code></pre>"},{"location":"advanced/monitoring/#cost-calculation","title":"Cost Calculation","text":"<p>RapidAI includes pricing for major providers:</p>"},{"location":"advanced/monitoring/#supported-models","title":"Supported Models","text":"<p>Anthropic Claude: - claude-3-opus-20240229: $15/$75 per 1M tokens - claude-3-sonnet-20240229: $3/$15 per 1M tokens - claude-3-haiku-20240307: $0.25/$1.25 per 1M tokens - claude-3-5-sonnet-20241022: $3/$15 per 1M tokens</p> <p>OpenAI: - gpt-4o: $5/$15 per 1M tokens - gpt-4o-mini: $0.15/$0.60 per 1M tokens - gpt-4-turbo: $10/$30 per 1M tokens - gpt-3.5-turbo: $0.50/$1.50 per 1M tokens</p>"},{"location":"advanced/monitoring/#manual-cost-calculation","title":"Manual Cost Calculation","text":"<pre><code>from rapidai.monitoring import calculate_cost\n\ncost = calculate_cost(\n    model=\"claude-3-haiku-20240307\",\n    prompt_tokens=100,\n    completion_tokens=50\n)\nprint(f\"Cost: ${cost:.6f}\")\n</code></pre>"},{"location":"advanced/monitoring/#dashboard","title":"Dashboard","text":""},{"location":"advanced/monitoring/#built-in-html-dashboard","title":"Built-in HTML Dashboard","text":"<pre><code>from rapidai.monitoring import get_dashboard_html\n\n@app.route(\"/metrics\")\nasync def metrics_dashboard():\n    html = get_dashboard_html()\n    return {\"body\": html, \"headers\": {\"Content-Type\": \"text/html\"}}\n</code></pre> <p>Dashboard Features: - Real-time metrics with auto-refresh (30s) - Overview statistics - LLM usage breakdown - Model-specific metrics - Request counts and success rates - Average latency - Total costs</p>"},{"location":"advanced/monitoring/#custom-dashboard","title":"Custom Dashboard","text":"<pre><code>from rapidai.monitoring import get_collector\n\n@app.route(\"/api/metrics\")\nasync def api_metrics():\n    collector = get_collector()\n    summary = collector.get_summary()\n\n    return {\n        \"uptime\": summary[\"uptime_seconds\"],\n        \"total_requests\": summary[\"total_requests\"],\n        \"success_rate\": summary[\"success_rate\"],\n        \"avg_duration\": summary[\"average_duration\"],\n        \"total_tokens\": summary[\"total_tokens\"],\n        \"total_cost\": summary[\"total_cost\"]\n    }\n</code></pre>"},{"location":"advanced/monitoring/#summary-statistics","title":"Summary Statistics","text":"<pre><code>collector = get_collector()\nsummary = collector.get_summary()\n\n# Returns:\n# {\n#     \"uptime_seconds\": 3600.0,\n#     \"total_requests\": 150,\n#     \"successful_requests\": 145,\n#     \"failed_requests\": 5,\n#     \"success_rate\": 0.967,\n#     \"average_duration\": 0.523,\n#     \"total_tokens\": 15000,\n#     \"total_cost\": 0.45,\n#     \"models_used\": 2\n# }\n</code></pre>"},{"location":"advanced/monitoring/#complete-example-monitored-chat-application","title":"Complete Example: Monitored Chat Application","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.monitoring import monitor, get_collector, get_dashboard_html\nfrom rapidai.memory import ConversationMemory\n\napp = App(title=\"Monitored Chat\")\nllm = LLM(\"claude-3-haiku-20240307\")\nmemory = ConversationMemory()\n\n@app.route(\"/chat\", methods=[\"POST\"])\n@monitor(track_tokens=True, track_cost=True)\nasync def chat(user_id: str, message: str):\n    \"\"\"Chat endpoint with monitoring.\"\"\"\n    # Add to memory\n    memory.add_message(user_id, \"user\", message)\n\n    # Get history\n    history = memory.get_history(user_id)\n\n    # Generate response\n    response = await llm.chat(history)\n\n    # Add response to memory\n    memory.add_message(user_id, \"assistant\", response)\n\n    # Return with tracking info\n    return {\n        \"response\": response,\n        \"tokens_used\": 150,  # Would come from LLM\n        \"model\": \"claude-3-haiku-20240307\"\n    }\n\n@app.route(\"/metrics/dashboard\")\nasync def dashboard():\n    \"\"\"Serve metrics dashboard.\"\"\"\n    return get_dashboard_html()\n\n@app.route(\"/metrics/api\")\nasync def metrics_api():\n    \"\"\"Get metrics as JSON.\"\"\"\n    collector = get_collector()\n\n    return {\n        \"summary\": collector.get_summary(),\n        \"model_usage\": {\n            model: {\n                \"tokens\": usage.total_tokens,\n                \"cost\": usage.total_cost,\n                \"requests\": usage.request_count\n            }\n            for model, usage in collector.get_model_usage().items()\n        }\n    }\n\n@app.route(\"/metrics/clear\", methods=[\"POST\"])\nasync def clear_metrics():\n    \"\"\"Clear all metrics.\"\"\"\n    collector = get_collector()\n    collector.clear()\n    return {\"status\": \"cleared\"}\n\nif __name__ == \"__main__\":\n    print(\"Dashboard: http://localhost:8000/metrics/dashboard\")\n    print(\"API: http://localhost:8000/metrics/api\")\n    app.run(port=8000)\n</code></pre>"},{"location":"advanced/monitoring/#best-practices","title":"Best Practices","text":""},{"location":"advanced/monitoring/#1-monitor-all-llm-endpoints","title":"1. Monitor All LLM Endpoints","text":"<pre><code># Always use @monitor for LLM endpoints\n@app.route(\"/generate\", methods=[\"POST\"])\n@monitor(track_tokens=True, track_cost=True)\nasync def generate(prompt: str):\n    ...\n</code></pre>"},{"location":"advanced/monitoring/#2-include-token-information","title":"2. Include Token Information","text":"<pre><code>@monitor(track_tokens=True, track_cost=True)\nasync def chat(message: str):\n    response = await llm.complete(message)\n\n    # Always return token info if available\n    return {\n        \"response\": response,\n        \"tokens_used\": response.usage.total_tokens,  # From LLM\n        \"model\": llm.model\n    }\n</code></pre>"},{"location":"advanced/monitoring/#3-set-up-alerts","title":"3. Set Up Alerts","text":"<pre><code>from rapidai.monitoring import get_collector\n\n@app.route(\"/health\")\nasync def health_check():\n    collector = get_collector()\n    summary = collector.get_summary()\n\n    # Alert if error rate too high\n    if summary[\"success_rate\"] &lt; 0.95:\n        # Send alert\n        await send_alert(\"High error rate!\")\n\n    # Alert if costs too high\n    if summary[\"total_cost\"] &gt; 10.0:\n        await send_alert(\"High costs!\")\n\n    return {\"status\": \"healthy\", \"metrics\": summary}\n</code></pre>"},{"location":"advanced/monitoring/#4-export-metrics","title":"4. Export Metrics","text":"<pre><code>@app.route(\"/metrics/export\")\nasync def export_metrics():\n    \"\"\"Export metrics for external monitoring.\"\"\"\n    collector = get_collector()\n\n    return {\n        \"timestamp\": datetime.now().isoformat(),\n        \"metrics\": collector.get_summary(),\n        \"models\": collector.get_model_usage()\n    }\n</code></pre>"},{"location":"advanced/monitoring/#5-track-custom-business-metrics","title":"5. Track Custom Business Metrics","text":"<pre><code>from rapidai.monitoring import get_collector\n\n@app.route(\"/purchase\", methods=[\"POST\"])\nasync def process_purchase(amount: float):\n    collector = get_collector()\n\n    # Track business metrics\n    collector.record_metric(\"revenue\", amount)\n    collector.record_metric(\"purchases\", 1)\n\n    return {\"status\": \"success\"}\n</code></pre>"},{"location":"advanced/monitoring/#6-use-tags-for-filtering","title":"6. Use Tags for Filtering","text":"<pre><code>collector.record_metric(\n    name=\"api.latency\",\n    value=0.523,\n    tags={\n        \"endpoint\": \"/chat\",\n        \"environment\": \"production\",\n        \"region\": \"us-east-1\"\n    }\n)\n</code></pre>"},{"location":"advanced/monitoring/#integration-with-external-services","title":"Integration with External Services","text":""},{"location":"advanced/monitoring/#prometheus-export","title":"Prometheus Export","text":"<pre><code>@app.route(\"/metrics/prometheus\")\nasync def prometheus_metrics():\n    \"\"\"Export metrics in Prometheus format.\"\"\"\n    collector = get_collector()\n    summary = collector.get_summary()\n\n    metrics = []\n    metrics.append(f\"# HELP rapidai_requests_total Total number of requests\")\n    metrics.append(f\"# TYPE rapidai_requests_total counter\")\n    metrics.append(f\"rapidai_requests_total {summary['total_requests']}\")\n\n    metrics.append(f\"# HELP rapidai_tokens_total Total tokens used\")\n    metrics.append(f\"# TYPE rapidai_tokens_total counter\")\n    metrics.append(f\"rapidai_tokens_total {summary['total_tokens']}\")\n\n    metrics.append(f\"# HELP rapidai_cost_total Total cost in USD\")\n    metrics.append(f\"# TYPE rapidai_cost_total counter\")\n    metrics.append(f\"rapidai_cost_total {summary['total_cost']}\")\n\n    return {\"body\": \"\\\\n\".join(metrics), \"headers\": {\"Content-Type\": \"text/plain\"}}\n</code></pre>"},{"location":"advanced/monitoring/#cloudwatch-integration","title":"CloudWatch Integration","text":"<pre><code>import boto3\n\ncloudwatch = boto3.client('cloudwatch')\n\n@app.on_interval(seconds=60)\nasync def push_to_cloudwatch():\n    \"\"\"Push metrics to AWS CloudWatch every minute.\"\"\"\n    collector = get_collector()\n    summary = collector.get_summary()\n\n    cloudwatch.put_metric_data(\n        Namespace='RapidAI',\n        MetricData=[\n            {\n                'MetricName': 'TotalTokens',\n                'Value': summary['total_tokens'],\n                'Unit': 'Count'\n            },\n            {\n                'MetricName': 'TotalCost',\n                'Value': summary['total_cost'],\n                'Unit': 'None'\n            }\n        ]\n    )\n</code></pre>"},{"location":"advanced/monitoring/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/monitoring/#metrics-not-showing","title":"Metrics Not Showing","text":"<pre><code># Ensure decorator is applied\n@monitor(track_tokens=True)  # \u2705 Correct\nasync def endpoint():\n    ...\n\n# Not decorated\nasync def endpoint():  # \u274c Won't track\n    ...\n</code></pre>"},{"location":"advanced/monitoring/#token-costs-incorrect","title":"Token Costs Incorrect","text":"<pre><code># Ensure you return the correct model name\n@monitor(track_tokens=True, track_cost=True)\nasync def chat(message: str):\n    return {\n        \"response\": response,\n        \"tokens_used\": 100,\n        \"model\": llm.model  # \u2705 Use actual model\n    }\n</code></pre>"},{"location":"advanced/monitoring/#dashboard-not-loading","title":"Dashboard Not Loading","text":"<pre><code># Ensure correct content type\n@app.route(\"/metrics\")\nasync def metrics():\n    html = get_dashboard_html()\n    # Return with HTML content type\n    return {\n        \"body\": html,\n        \"headers\": {\"Content-Type\": \"text/html; charset=utf-8\"}\n    }\n</code></pre>"},{"location":"advanced/monitoring/#next-steps","title":"Next Steps","text":"<ul> <li>See API Reference for complete API documentation</li> <li>Learn about Background Jobs to monitor async tasks</li> <li>Check Testing for testing monitored endpoints</li> </ul>"},{"location":"advanced/performance/","title":"performance","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"advanced/prompts/","title":"Prompt Management","text":"<p>RapidAI provides a powerful prompt management system with Jinja2 templating, version tracking, and hot reloading for development.</p>"},{"location":"advanced/prompts/#quick-start","title":"Quick Start","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.prompts import PromptManager\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n# Initialize prompt manager\nprompts = PromptManager(prompt_dir=\"prompts\", auto_reload=True)\n\n@app.route(\"/greet\", methods=[\"POST\"])\nasync def greet(name: str):\n    # Render prompt with variables\n    prompt_text = prompts.render(\"greeting\", name=name)\n    response = await llm.complete(prompt_text)\n    return {\"response\": response}\n</code></pre> <p>Create <code>prompts/greeting.txt</code>:</p> <pre><code>Hello {{ name }}! How can I assist you today?\n</code></pre>"},{"location":"advanced/prompts/#features","title":"Features","text":"<ul> <li>Jinja2 Templates: Full Jinja2 support with variables, loops, conditionals</li> <li>Hot Reloading: Auto-reload prompts in development when files change</li> <li>Version Tracking: Track and retrieve different versions of prompts</li> <li>Frontmatter Metadata: YAML frontmatter for prompt metadata</li> <li>@prompt Decorator: Automatic prompt injection into route handlers</li> <li>Programmatic Registration: Register prompts in code without files</li> </ul>"},{"location":"advanced/prompts/#promptmanager","title":"PromptManager","text":""},{"location":"advanced/prompts/#initialization","title":"Initialization","text":"<pre><code>from rapidai.prompts import PromptManager\n\n# Basic usage\nprompts = PromptManager()\n\n# Custom directory with hot reloading\nprompts = PromptManager(\n    prompt_dir=\"my_prompts\",\n    auto_reload=True,           # Enable hot reloading\n    reload_interval=5           # Check for changes every 5 seconds\n)\n</code></pre>"},{"location":"advanced/prompts/#loading-from-files","title":"Loading from Files","text":"<p>The PromptManager automatically loads <code>.txt</code> and <code>.md</code> files from the prompt directory:</p> <pre><code>prompts/\n  \u251c\u2500\u2500 greeting.txt\n  \u251c\u2500\u2500 analyze.txt\n  \u2514\u2500\u2500 support.md\n</code></pre> <p>Access prompts by filename (without extension):</p> <pre><code>greeting_prompt = prompts.get(\"greeting\")\nanalyze_prompt = prompts.get(\"analyze\")\nsupport_prompt = prompts.get(\"support\")\n</code></pre>"},{"location":"advanced/prompts/#yaml-frontmatter","title":"YAML Frontmatter","text":"<p>Add metadata to prompts using YAML frontmatter:</p> <pre><code>---\nversion: \"1.0\"\ndescription: \"Customer support prompt\"\nauthor: \"Support Team\"\ntags: [\"support\", \"customer-service\"]\n---\nHello {{ customer_name }},\n\nThank you for contacting support. How can I help you today?\n</code></pre> <p>Access metadata:</p> <pre><code>prompt = prompts.get(\"support\")\nprint(prompt.metadata)\n# {\"version\": \"1.0\", \"description\": \"Customer support prompt\", ...}\n</code></pre>"},{"location":"advanced/prompts/#programmatic-registration","title":"Programmatic Registration","text":"<p>Register prompts in code without files:</p> <pre><code>prompts.register(\n    name=\"dynamic_prompt\",\n    template=\"Analyze this: {{ text }}\",\n    metadata={\"type\": \"analysis\"}\n)\n\n# Use it\nresult = prompts.render(\"dynamic_prompt\", text=\"Sample content\")\n</code></pre>"},{"location":"advanced/prompts/#templates-with-jinja2","title":"Templates with Jinja2","text":""},{"location":"advanced/prompts/#variables","title":"Variables","text":"<pre><code>template = \"Hello {{ name }}, you have {{ count }} messages.\"\n\nprompt = prompts.register(\"greeting\", template)\nresult = prompt.render(name=\"Alice\", count=5)\n# \"Hello Alice, you have 5 messages.\"\n</code></pre>"},{"location":"advanced/prompts/#conditionals","title":"Conditionals","text":"<pre><code>template = \"\"\"\n{% if premium %}\nWelcome, premium member {{ name }}!\n{% else %}\nWelcome {{ name }}! Upgrade to premium for more features.\n{% endif %}\n\"\"\"\n\nprompts.register(\"welcome\", template)\nresult = prompts.render(\"welcome\", name=\"Bob\", premium=True)\n</code></pre>"},{"location":"advanced/prompts/#loops","title":"Loops","text":"<pre><code>template = \"\"\"\nTopics to cover:\n{% for topic in topics %}\n- {{ topic }}\n{% endfor %}\n\"\"\"\n\nprompts.register(\"topics\", template)\nresult = prompts.render(\"topics\", topics=[\"AI\", \"ML\", \"NLP\"])\n</code></pre>"},{"location":"advanced/prompts/#filters","title":"Filters","text":"<pre><code>template = \"User: {{ name | upper }}\"\n\nprompts.register(\"user\", template)\nresult = prompts.render(\"user\", name=\"alice\")\n# \"User: ALICE\"\n</code></pre>"},{"location":"advanced/prompts/#version-tracking","title":"Version Tracking","text":"<p>Track multiple versions of prompts:</p> <pre><code>prompt = prompts.get(\"greeting\")\n\n# Add a new version\nprompt.add_version(\n    version=\"2.0\",\n    template=\"Hi {{ name }}! What can I do for you?\",\n    metadata={\"changelog\": \"More casual tone\"}\n)\n\n# Get specific version\nv1_prompt = prompts.get(\"greeting\", version=\"1.0\")\nv2_prompt = prompts.get(\"greeting\", version=\"2.0\")\n</code></pre>"},{"location":"advanced/prompts/#hot-reloading","title":"Hot Reloading","text":"<p>Enable hot reloading in development:</p> <pre><code>prompts = PromptManager(\n    prompt_dir=\"prompts\",\n    auto_reload=True,\n    reload_interval=5  # Check every 5 seconds\n)\n</code></pre> <p>When <code>auto_reload=True</code>, the manager checks for file changes at the specified interval and reloads modified prompts automatically.</p> <p>Environment-based auto-reload:</p> <pre><code>from rapidai.prompts import get_prompt_manager\n\n# Auto-enables reload if DEBUG=true in environment\nprompts = get_prompt_manager()\n</code></pre>"},{"location":"advanced/prompts/#prompt-decorator","title":"@prompt Decorator","text":"<p>Automatically inject prompts into route handlers:</p> <pre><code>from rapidai import App, LLM\nfrom rapidai.prompts import prompt, PromptManager\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\nprompt_manager = PromptManager(auto_reload=True)\n\n# Using inline template\n@app.route(\"/greet\", methods=[\"POST\"])\n@prompt(template=\"Hello {{ name }}! How can I help?\", manager=prompt_manager)\nasync def greet(name: str, prompt_template: str, prompt):\n    # prompt_template and prompt are automatically injected\n    filled = prompt.render(name=name)\n    response = await llm.complete(filled)\n    return {\"response\": response}\n\n# Using file-based prompt\n@app.route(\"/analyze\", methods=[\"POST\"])\n@prompt(name=\"analyze\", manager=prompt_manager)\nasync def analyze(text: str, prompt_template: str, prompt):\n    filled = prompt.render(text=text)\n    response = await llm.complete(filled)\n    return {\"analysis\": response}\n</code></pre> <p>The decorator automatically injects:</p> <ul> <li><code>prompt_template</code>: The raw template string</li> <li><code>prompt</code>: The Prompt object with <code>render()</code> method and metadata</li> </ul>"},{"location":"advanced/prompts/#working-with-prompts","title":"Working with Prompts","text":""},{"location":"advanced/prompts/#extract-variables","title":"Extract Variables","text":"<pre><code>prompt = prompts.get(\"greeting\")\nprint(prompt.variables)\n# [\"name\", \"count\"]\n</code></pre> <p>Variables are automatically extracted from Jinja2 templates.</p>"},{"location":"advanced/prompts/#render-with-variables","title":"Render with Variables","text":"<pre><code>prompt = prompts.get(\"greeting\")\nresult = prompt.render(name=\"Alice\", count=5)\n</code></pre> <p>Missing required variables will raise a <code>PromptError</code>.</p>"},{"location":"advanced/prompts/#list-available-prompts","title":"List Available Prompts","text":"<pre><code>prompt_names = prompts.list_prompts()\nprint(prompt_names)\n# [\"greeting\", \"analyze\", \"support\"]\n</code></pre>"},{"location":"advanced/prompts/#complete-example-customer-support-bot","title":"Complete Example: Customer Support Bot","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.prompts import prompt, PromptManager\n\napp = App(title=\"Support Bot\")\nllm = LLM(\"claude-3-haiku-20240307\")\nprompts = PromptManager(prompt_dir=\"prompts\", auto_reload=True)\n\n@app.route(\"/support/greeting\", methods=[\"POST\"])\n@prompt(\n    template=\"\"\"Hello {{ customer_name }}!\n\nI'm here to help with your {{ product }} inquiry.\n{% if is_premium %}\nAs a premium customer, you have priority support access.\n{% endif %}\n\nHow can I assist you today?\"\"\",\n    manager=prompts\n)\nasync def support_greeting(\n    customer_name: str,\n    product: str,\n    is_premium: bool = False,\n    prompt_template: str = None,\n    prompt = None\n):\n    filled = prompt.render(\n        customer_name=customer_name,\n        product=product,\n        is_premium=is_premium\n    )\n    response = await llm.complete(filled)\n    return {\"greeting\": response}\n\n@app.route(\"/support/analyze\", methods=[\"POST\"])\n@prompt(\n    template=\"\"\"Analyze this customer support ticket and provide:\n1. Issue category (technical/billing/general)\n2. Urgency level (low/medium/high)\n3. Suggested response approach\n\nTicket: {{ ticket_text }}\n\nAnalysis:\"\"\",\n    manager=prompts\n)\nasync def analyze_ticket(ticket_text: str, prompt_template: str, prompt):\n    filled = prompt.render(ticket_text=ticket_text)\n    analysis = await llm.complete(filled)\n    return {\"analysis\": analysis}\n\nif __name__ == \"__main__\":\n    app.run(port=8000)\n</code></pre> <p>Test the support bot:</p> <pre><code># Greeting\ncurl -X POST http://localhost:8000/support/greeting \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"customer_name\": \"Alice\",\n    \"product\": \"RapidAI Premium\",\n    \"is_premium\": true\n  }'\n\n# Analyze ticket\ncurl -X POST http://localhost:8000/support/analyze \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"ticket_text\": \"I cannot access my account after updating my password\"\n  }'\n</code></pre>"},{"location":"advanced/prompts/#best-practices","title":"Best Practices","text":""},{"location":"advanced/prompts/#1-organize-prompts-by-purpose","title":"1. Organize Prompts by Purpose","text":"<pre><code>prompts/\n  \u251c\u2500\u2500 greetings/\n  \u2502   \u251c\u2500\u2500 welcome.txt\n  \u2502   \u2514\u2500\u2500 farewell.txt\n  \u251c\u2500\u2500 analysis/\n  \u2502   \u251c\u2500\u2500 sentiment.txt\n  \u2502   \u2514\u2500\u2500 topics.txt\n  \u2514\u2500\u2500 support/\n      \u251c\u2500\u2500 ticket_analysis.txt\n      \u2514\u2500\u2500 response_template.txt\n</code></pre>"},{"location":"advanced/prompts/#2-use-metadata-for-tracking","title":"2. Use Metadata for Tracking","text":"<pre><code>---\nversion: \"2.1\"\ncreated: \"2024-01-15\"\nmodified: \"2024-02-01\"\nauthor: \"AI Team\"\nperformance: \"95% satisfaction\"\n---\nYour prompt template here\n</code></pre>"},{"location":"advanced/prompts/#3-version-control-prompts","title":"3. Version Control Prompts","text":"<pre><code># Track changes\nprompt.add_version(\n    version=\"2.0\",\n    template=new_template,\n    metadata={\n        \"changelog\": \"Improved clarity\",\n        \"tested_on\": \"2024-02-01\"\n    }\n)\n</code></pre>"},{"location":"advanced/prompts/#4-use-hot-reload-in-development-only","title":"4. Use Hot Reload in Development Only","text":"<pre><code>import os\n\nprompts = PromptManager(\n    auto_reload=os.getenv(\"ENV\") == \"development\"\n)\n</code></pre>"},{"location":"advanced/prompts/#5-validate-variables","title":"5. Validate Variables","text":"<pre><code>prompt = prompts.get(\"greeting\")\n\n# Check required variables\nrequired_vars = prompt.variables\nprint(f\"Required: {required_vars}\")\n\n# Ensure all are provided\nuser_vars = {\"name\": \"Alice\"}\nmissing = set(required_vars) - set(user_vars.keys())\nif missing:\n    raise ValueError(f\"Missing variables: {missing}\")\n</code></pre>"},{"location":"advanced/prompts/#6-cache-rendered-prompts","title":"6. Cache Rendered Prompts","text":"<pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=100)\ndef get_rendered_prompt(name: str, **kwargs):\n    prompt = prompts.get(name)\n    return prompt.render(**kwargs)\n</code></pre>"},{"location":"advanced/prompts/#7-use-descriptive-names","title":"7. Use Descriptive Names","text":"<pre><code># Good\nprompts.register(\"customer_support_greeting\", template)\nprompts.register(\"sentiment_analysis_system_prompt\", template)\n\n# Avoid\nprompts.register(\"p1\", template)\nprompts.register(\"temp\", template)\n</code></pre>"},{"location":"advanced/prompts/#configuration","title":"Configuration","text":"<p>Configure prompts via environment variables:</p> <pre><code># Prompt directory\nexport RAPIDAI_PROMPT_DIR=\"./my_prompts\"\n\n# Enable auto-reload\nexport DEBUG=true\n</code></pre> <p>Or in code:</p> <pre><code>from rapidai.prompts import PromptManager\n\nprompts = PromptManager(\n    prompt_dir=\"prompts\",\n    auto_reload=True,\n    reload_interval=10\n)\n</code></pre>"},{"location":"advanced/prompts/#error-handling","title":"Error Handling","text":"<pre><code>from rapidai.prompts import PromptError\n\ntry:\n    prompt = prompts.get(\"missing_prompt\")\n    if not prompt:\n        raise PromptError(\"Prompt not found\")\n\n    result = prompt.render(name=\"Alice\")\nexcept PromptError as e:\n    print(f\"Prompt error: {e}\")\n</code></pre>"},{"location":"advanced/prompts/#next-steps","title":"Next Steps","text":"<ul> <li>See API Reference for complete API documentation</li> <li>Check examples/prompt_decorator.py for more examples</li> <li>Learn about RAG for combining prompts with retrieved context</li> </ul>"},{"location":"advanced/rag/","title":"Retrieval-Augmented Generation (RAG)","text":"<p>RapidAI includes a powerful RAG system that allows you to augment your LLM applications with custom knowledge from documents.</p>"},{"location":"advanced/rag/#quick-start","title":"Quick Start","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.rag import RAG\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\nrag = RAG()\n\n# Add documents\nawait rag.add_document(\"docs/manual.pdf\")\n\n# Query with context\nanswer = await rag.query(\"How do I install?\", llm=llm)\n</code></pre>"},{"location":"advanced/rag/#architecture","title":"Architecture","text":"<p>The RAG system consists of four main components:</p> <ol> <li>Document Loaders - Load PDF, DOCX, TXT, HTML, Markdown</li> <li>Chunkers - Split documents into semantic chunks</li> <li>Embeddings - Convert text to vector embeddings</li> <li>Vector Database - Store and search embeddings</li> </ol>"},{"location":"advanced/rag/#configuration","title":"Configuration","text":"<p>Configure RAG via environment variables or YAML:</p> <pre><code>rag:\n  embedding:\n    provider: \"sentence-transformers\"  # or \"openai\"\n    model: \"all-MiniLM-L6-v2\"\n\n  chunking:\n    strategy: \"recursive\"  # or \"sentence\"\n    chunk_size: 512\n    chunk_overlap: 50\n\n  vectordb:\n    backend: \"chromadb\"\n    persist_directory: \"./chroma_data\"\n    collection_name: \"my_docs\"\n\n  top_k: 5\n</code></pre> <p>Or via environment variables:</p> <pre><code>RAPIDAI_RAG_TOP_K=5\nRAPIDAI_EMBEDDING_PROVIDER=openai\nRAPIDAI_EMBEDDING_MODEL=text-embedding-3-small\n</code></pre>"},{"location":"advanced/rag/#document-loading","title":"Document Loading","text":""},{"location":"advanced/rag/#supported-formats","title":"Supported Formats","text":"<ul> <li>PDF (<code>.pdf</code>)</li> <li>Word Documents (<code>.docx</code>)</li> <li>Plain Text (<code>.txt</code>)</li> <li>Markdown (<code>.md</code>)</li> <li>HTML (<code>.html</code>, <code>.htm</code>)</li> </ul>"},{"location":"advanced/rag/#loading-documents","title":"Loading Documents","text":"<pre><code>from rapidai.rag import RAG\n\nrag = RAG()\n\n# Single document\nawait rag.add_document(\"path/to/document.pdf\")\n\n# Multiple documents\nawait rag.add_documents([\n    \"doc1.pdf\",\n    \"doc2.docx\",\n    \"doc3.md\"\n])\n\n# From Document object\nfrom rapidai.types import Document\n\ndoc = Document(\n    content=\"Custom content\",\n    metadata={\"source\": \"manual\", \"version\": \"1.0\"}\n)\nawait rag.add_document(doc)\n</code></pre>"},{"location":"advanced/rag/#embeddings","title":"Embeddings","text":"<p>RapidAI supports multiple embedding providers:</p>"},{"location":"advanced/rag/#sentence-transformers-default","title":"Sentence Transformers (Default)","text":"<pre><code>from rapidai.rag import Embedding\n\nembedding = Embedding(\n    provider=\"sentence-transformers\",\n    model=\"all-MiniLM-L6-v2\"\n)\n</code></pre> <p>Pros:</p> <ul> <li>Free and open source</li> <li>Runs locally (no API costs)</li> <li>Fast for batch processing</li> </ul> <p>Cons:</p> <ul> <li>Requires local compute</li> <li>Lower quality than OpenAI</li> </ul>"},{"location":"advanced/rag/#openai-embeddings","title":"OpenAI Embeddings","text":"<pre><code>embedding = Embedding(\n    provider=\"openai\",\n    model=\"text-embedding-3-small\",\n    api_key=\"your-api-key\"  # or OPENAI_API_KEY env var\n)\n</code></pre> <p>Pros:</p> <ul> <li>High quality embeddings</li> <li>Latest models</li> </ul> <p>Cons:</p> <ul> <li>API costs</li> <li>Requires internet connection</li> </ul>"},{"location":"advanced/rag/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from rapidai.rag import RAG, Embedding\n\nembedding = Embedding(provider=\"openai\", model=\"text-embedding-3-large\")\nrag = RAG(embedding=embedding)\n</code></pre>"},{"location":"advanced/rag/#retrieval","title":"Retrieval","text":""},{"location":"advanced/rag/#basic-retrieval","title":"Basic Retrieval","text":"<pre><code># Retrieve relevant chunks\nresult = await rag.retrieve(\"your query\", top_k=5)\n\nprint(result.text)  # Combined text from chunks\nprint(result.sources)  # List of DocumentChunk objects\n</code></pre>"},{"location":"advanced/rag/#with-metadata-filtering","title":"With Metadata Filtering","text":"<pre><code># Only search in specific document types\nresult = await rag.retrieve(\n    \"query\",\n    top_k=5,\n    filter_metadata={\"type\": \"pdf\"}\n)\n</code></pre>"},{"location":"advanced/rag/#full-rag-query","title":"Full RAG Query","text":"<pre><code>from rapidai import LLM\n\nllm = LLM(\"claude-3-haiku-20240307\")\n\n# Retrieve + Generate in one call\nanswer = await rag.query(\n    \"What is the installation process?\",\n    llm=llm,\n    top_k=5\n)\n</code></pre>"},{"location":"advanced/rag/#rag-decorator","title":"RAG Decorator","text":"<p>The <code>@rag()</code> decorator automatically injects retrieval context into your routes:</p> <pre><code>from rapidai import App, LLM\nfrom rapidai.rag import rag\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/ask\", methods=[\"POST\"])\n@rag(sources=[\"docs/manual.pdf\"], top_k=5)\nasync def ask(query: str, rag_context):\n    \"\"\"rag_context is automatically injected.\"\"\"\n    prompt = f\"Context: {rag_context.text}\\n\\nQuestion: {query}\"\n    answer = await llm.complete(prompt)\n\n    return {\n        \"answer\": answer,\n        \"sources\": [s.metadata for s in rag_context.sources]\n    }\n</code></pre>"},{"location":"advanced/rag/#chunking-strategies","title":"Chunking Strategies","text":""},{"location":"advanced/rag/#recursive-chunking-default","title":"Recursive Chunking (Default)","text":"<p>Splits text using multiple separators hierarchically:</p> <pre><code>from rapidai.rag import Chunker\n\nchunker = Chunker(\n    strategy=\"recursive\",\n    chunk_size=512,\n    chunk_overlap=50\n)\n</code></pre> <p>Best for: General purpose, mixed content</p>"},{"location":"advanced/rag/#sentence-chunking","title":"Sentence Chunking","text":"<p>Splits by sentences with semantic boundaries:</p> <pre><code>chunker = Chunker(\n    strategy=\"sentence\",\n    chunk_size=512,\n    chunk_overlap=2  # Number of sentences to overlap\n)\n</code></pre> <p>Best for: Narrative text, articles</p>"},{"location":"advanced/rag/#best-practices","title":"Best Practices","text":"<ol> <li>Chunk Size: Start with 512 tokens, adjust based on your use case</li> <li>Overlap: Use 10-20% overlap to maintain context</li> <li>top_k: Start with 3-5 chunks, increase if needed</li> <li>Embeddings: Use sentence-transformers for speed, OpenAI for quality</li> <li>Metadata: Add rich metadata for filtering and provenance</li> </ol>"},{"location":"advanced/rag/#complete-example-customer-support-bot","title":"Complete Example: Customer Support Bot","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.rag import rag\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/support\", methods=[\"POST\"])\n@rag(\n    sources=[\n        \"kb/troubleshooting.pdf\",\n        \"kb/faq.md\",\n        \"kb/user_guide.pdf\"\n    ],\n    top_k=5\n)\nasync def support_query(question: str, rag_context):\n    \"\"\"Customer support with RAG.\"\"\"\n    system_prompt = \"\"\"You are a helpful customer support assistant.\n    Answer based on the knowledge base provided. If unsure, say so.\"\"\"\n\n    prompt = f\"\"\"{system_prompt}\n\nKnowledge Base:\n{rag_context.text}\n\nCustomer Question: {question}\n\nAnswer:\"\"\"\n\n    response = await llm.complete(prompt)\n\n    return {\n        \"answer\": response,\n        \"sources\": [\n            {\n                \"file\": s.metadata.get(\"filename\"),\n                \"excerpt\": s.content[:100]\n            }\n            for s in rag_context.sources\n        ]\n    }\n\nif __name__ == \"__main__\":\n    app.run()\n</code></pre>"},{"location":"advanced/rag/#see-also","title":"See Also","text":"<ul> <li>RAG API Reference</li> <li>Configuration Guide</li> <li>Testing RAG Applications</li> </ul>"},{"location":"advanced/testing/","title":"Testing","text":"<p>RapidAI includes comprehensive testing utilities for building reliable AI applications. Test your endpoints, mock LLMs, and verify behavior without making real API calls.</p>"},{"location":"advanced/testing/#quick-start","title":"Quick Start","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.testing import TestClient, MockLLM\n\napp = App()\n\n@app.route(\"/hello\")\nasync def hello():\n    return {\"message\": \"Hello, World!\"}\n\n# Test\ndef test_hello():\n    client = TestClient(app)\n    response = client.get(\"/hello\")\n\n    assert response.status_code == 200\n    assert response.json() == {\"message\": \"Hello, World!\"}\n</code></pre>"},{"location":"advanced/testing/#features","title":"Features","text":"<ul> <li>TestClient - Test HTTP endpoints without running a server</li> <li>MockLLM - Mock language models for testing</li> <li>MockMemory - Mock conversation memory</li> <li>Call Tracking - Verify mock calls and arguments</li> <li>Pytest Fixtures - Ready-to-use pytest fixtures</li> <li>No API Costs - Test without making real API calls</li> </ul>"},{"location":"advanced/testing/#testclient","title":"TestClient","text":""},{"location":"advanced/testing/#basic-usage","title":"Basic Usage","text":"<pre><code>from rapidai import App\nfrom rapidai.testing import TestClient\n\napp = App()\n\n@app.route(\"/echo\", methods=[\"POST\"])\nasync def echo(message: str):\n    return {\"echo\": message}\n\ndef test_echo():\n    client = TestClient(app)\n    response = client.post(\"/echo\", json={\"message\": \"hello\"})\n\n    assert response.status_code == 200\n    assert response.json() == {\"echo\": \"hello\"}\n</code></pre>"},{"location":"advanced/testing/#http-methods","title":"HTTP Methods","text":"<pre><code>from rapidai.testing import TestClient\n\nclient = TestClient(app)\n\n# GET request\nresponse = client.get(\"/users\")\nresponse = client.get(\"/users/123\")\nresponse = client.get(\"/search\", params={\"q\": \"test\"})\n\n# POST request\nresponse = client.post(\"/users\", json={\"name\": \"Alice\"})\nresponse = client.post(\"/upload\", data={\"file\": \"data\"})\n\n# PUT request\nresponse = client.put(\"/users/123\", json={\"name\": \"Bob\"})\n\n# DELETE request\nresponse = client.delete(\"/users/123\")\n</code></pre>"},{"location":"advanced/testing/#request-headers","title":"Request Headers","text":"<pre><code>response = client.get(\n    \"/protected\",\n    headers={\"Authorization\": \"Bearer token\"}\n)\n\nresponse = client.post(\n    \"/api/data\",\n    json={\"key\": \"value\"},\n    headers={\"Content-Type\": \"application/json\"}\n)\n</code></pre>"},{"location":"advanced/testing/#response-object","title":"Response Object","text":"<pre><code>response = client.get(\"/api/data\")\n\n# Status code\nassert response.status_code == 200\n\n# JSON body\ndata = response.json()\nassert data[\"key\"] == \"value\"\n\n# Text body\ntext = response.text\nassert \"success\" in text\n\n# Headers\nassert response.headers.get(\"Content-Type\") == \"application/json\"\n</code></pre>"},{"location":"advanced/testing/#mockllm","title":"MockLLM","text":""},{"location":"advanced/testing/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from rapidai.testing import MockLLM\n\n# Create mock with default response\nllm = MockLLM(response=\"This is a mock response\")\n\n# Use like regular LLM\nresult = await llm.complete(\"test prompt\")\nassert result == \"This is a mock response\"\n</code></pre>"},{"location":"advanced/testing/#with-testclient","title":"With TestClient","text":"<pre><code>from rapidai import App\nfrom rapidai.testing import TestClient, MockLLM\n\n# Create app with mock LLM\nmock_llm = MockLLM(response=\"Mocked answer\")\napp = App()\n\n@app.route(\"/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    response = await mock_llm.complete(message)\n    return {\"response\": response}\n\n# Test\ndef test_chat():\n    client = TestClient(app)\n    response = client.post(\"/chat\", json={\"message\": \"hello\"})\n\n    assert response.status_code == 200\n    assert response.json() == {\"response\": \"Mocked answer\"}\n</code></pre>"},{"location":"advanced/testing/#tracking-calls","title":"Tracking Calls","text":"<pre><code>from rapidai.testing import MockLLM\n\nllm = MockLLM()\n\n# Make calls\nawait llm.complete(\"first prompt\")\nawait llm.complete(\"second prompt\")\nawait llm.chat([{\"role\": \"user\", \"content\": \"hello\"}])\n\n# Verify calls\nassert len(llm.calls) == 3\n\n# Check specific calls\nmethod, prompt, kwargs = llm.calls[0]\nassert method == \"complete\"\nassert prompt == \"first prompt\"\n\n# Reset call history\nllm.reset()\nassert len(llm.calls) == 0\n</code></pre>"},{"location":"advanced/testing/#mock-methods","title":"Mock Methods","text":"<pre><code>from rapidai.testing import MockLLM\n\nllm = MockLLM(response=\"Mock response\")\n\n# Complete\nresult = await llm.complete(\"prompt\")\nassert result == \"Mock response\"\n\n# Chat\nresult = await llm.chat([{\"role\": \"user\", \"content\": \"hi\"}])\nassert result == \"Mock response\"\n\n# Stream\nchunks = []\nasync for chunk in llm.stream(\"prompt\"):\n    chunks.append(chunk)\nassert \"\".join(chunks) == \"Mock response\"\n\n# Embed\nembedding = await llm.embed(\"text\")\nassert len(embedding) == 384  # Fake embedding\n</code></pre>"},{"location":"advanced/testing/#mockmemory","title":"MockMemory","text":""},{"location":"advanced/testing/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from rapidai.testing import MockMemory\n\nmemory = MockMemory()\n\n# Use like regular memory\nmemory.add_message(\"user1\", \"user\", \"Hello\")\nmemory.add_message(\"user1\", \"assistant\", \"Hi!\")\n\nhistory = memory.get_history(\"user1\")\nassert len(history) == 2\n\n# Verify calls\nassert len(memory.calls) == 3  # 2 add + 1 get\n</code></pre>"},{"location":"advanced/testing/#with-testclient_1","title":"With TestClient","text":"<pre><code>from rapidai import App\nfrom rapidai.testing import TestClient, MockLLM, MockMemory\n\nmock_llm = MockLLM(response=\"Mocked response\")\nmock_memory = MockMemory()\napp = App()\n\n@app.route(\"/chat\", methods=[\"POST\"])\nasync def chat(user_id: str, message: str):\n    mock_memory.add_message(user_id, \"user\", message)\n    history = mock_memory.get_history(user_id)\n\n    response = await mock_llm.chat(history)\n    mock_memory.add_message(user_id, \"assistant\", response)\n\n    return {\"response\": response}\n\ndef test_chat_with_memory():\n    client = TestClient(app)\n\n    # First message\n    response = client.post(\"/chat\", json={\"user_id\": \"123\", \"message\": \"Hi\"})\n    assert response.status_code == 200\n\n    # Verify memory was used\n    assert len(mock_memory.calls) &gt; 0\n\n    # Check add_message was called\n    method, user_id, role, content = mock_memory.calls[0]\n    assert method == \"add_message\"\n    assert user_id == \"123\"\n    assert role == \"user\"\n    assert content == \"Hi\"\n</code></pre>"},{"location":"advanced/testing/#testing-patterns","title":"Testing Patterns","text":""},{"location":"advanced/testing/#test-endpoints","title":"Test Endpoints","text":"<pre><code>from rapidai import App\nfrom rapidai.testing import TestClient\n\napp = App()\n\n@app.route(\"/add\", methods=[\"POST\"])\nasync def add(a: int, b: int):\n    return {\"result\": a + b}\n\ndef test_add():\n    client = TestClient(app)\n\n    # Test successful case\n    response = client.post(\"/add\", json={\"a\": 2, \"b\": 3})\n    assert response.status_code == 200\n    assert response.json() == {\"result\": 5}\n\n    # Test with different values\n    response = client.post(\"/add\", json={\"a\": 10, \"b\": 20})\n    assert response.json() == {\"result\": 30}\n</code></pre>"},{"location":"advanced/testing/#test-with-fixtures","title":"Test with Fixtures","text":"<pre><code>import pytest\nfrom rapidai import App\nfrom rapidai.testing import TestClient, MockLLM\n\n@pytest.fixture\ndef app():\n    return App()\n\n@pytest.fixture\ndef client(app):\n    return TestClient(app)\n\n@pytest.fixture\ndef mock_llm():\n    return MockLLM(response=\"Test response\")\n\ndef test_chat_endpoint(client, mock_llm):\n    # Use fixtures in test\n    response = client.post(\"/chat\", json={\"message\": \"hello\"})\n    assert response.status_code == 200\n</code></pre>"},{"location":"advanced/testing/#test-error-handling","title":"Test Error Handling","text":"<pre><code>from rapidai import App\nfrom rapidai.testing import TestClient\n\napp = App()\n\n@app.route(\"/divide\", methods=[\"POST\"])\nasync def divide(a: int, b: int):\n    if b == 0:\n        return {\"error\": \"Division by zero\"}, 400\n    return {\"result\": a / b}\n\ndef test_divide_by_zero():\n    client = TestClient(app)\n\n    response = client.post(\"/divide\", json={\"a\": 10, \"b\": 0})\n\n    assert response.status_code == 400\n    assert response.json() == {\"error\": \"Division by zero\"}\n</code></pre>"},{"location":"advanced/testing/#test-rag-system","title":"Test RAG System","text":"<pre><code>from rapidai.rag import RAG\nfrom rapidai.rag.mocks import MockEmbedding, MockVectorDB\nfrom rapidai.types import Document\n\nasync def test_rag_pipeline():\n    # Create RAG with mocks\n    embedding = MockEmbedding(dimension=384)\n    vectordb = MockVectorDB()\n    rag = RAG(embedding=embedding, vectordb=vectordb)\n\n    # Add document\n    doc = Document(\n        content=\"RapidAI is a Python framework.\",\n        metadata={\"source\": \"test.txt\"}\n    )\n    chunks = await rag.add_document(doc)\n\n    assert len(chunks) &gt; 0\n    assert all(chunk.embedding is not None for chunk in chunks)\n\n    # Query\n    result = await rag.retrieve(\"What is RapidAI?\", top_k=1)\n\n    assert result.text\n    assert len(result.sources) &gt; 0\n</code></pre>"},{"location":"advanced/testing/#test-background-jobs","title":"Test Background Jobs","text":"<pre><code>from rapidai.background import background\n\n@background(max_retries=2)\nasync def process_data(data: str):\n    return {\"processed\": data}\n\nasync def test_background_job():\n    # Enqueue job\n    job_id = await process_data.enqueue(data=\"test\")\n\n    # Wait for completion\n    result = await process_data.get_result(job_id)\n\n    assert result.status == \"completed\"\n    assert result.result == {\"processed\": \"test\"}\n</code></pre>"},{"location":"advanced/testing/#test-monitoring","title":"Test Monitoring","text":"<pre><code>from rapidai import App\nfrom rapidai.monitoring import monitor, get_collector\nfrom rapidai.testing import TestClient, MockLLM\n\napp = App()\nmock_llm = MockLLM()\n\n@app.route(\"/chat\", methods=[\"POST\"])\n@monitor(track_tokens=True, track_cost=True)\nasync def chat(message: str):\n    response = await mock_llm.complete(message)\n    return {\n        \"response\": response,\n        \"tokens_used\": 100,\n        \"model\": \"mock-model\"\n    }\n\ndef test_monitoring():\n    client = TestClient(app)\n    collector = get_collector()\n\n    # Clear previous metrics\n    collector.clear()\n\n    # Make request\n    response = client.post(\"/chat\", json={\"message\": \"hello\"})\n    assert response.status_code == 200\n\n    # Verify metrics\n    summary = collector.get_summary()\n    assert summary[\"total_requests\"] &gt;= 1\n</code></pre>"},{"location":"advanced/testing/#pytest-integration","title":"Pytest Integration","text":""},{"location":"advanced/testing/#setup-fixtures","title":"Setup Fixtures","text":"<p>Create <code>conftest.py</code>:</p> <pre><code>import pytest\nfrom rapidai import App\nfrom rapidai.testing import TestClient, MockLLM, MockMemory\n\n@pytest.fixture\ndef app():\n    \"\"\"Create test app.\"\"\"\n    return App(title=\"Test App\")\n\n@pytest.fixture\ndef client(app):\n    \"\"\"Create test client.\"\"\"\n    return TestClient(app)\n\n@pytest.fixture\ndef mock_llm():\n    \"\"\"Create mock LLM.\"\"\"\n    return MockLLM(response=\"Test response\")\n\n@pytest.fixture\ndef mock_memory():\n    \"\"\"Create mock memory.\"\"\"\n    return MockMemory()\n</code></pre>"},{"location":"advanced/testing/#write-tests","title":"Write Tests","text":"<p>Create <code>test_app.py</code>:</p> <pre><code>def test_health_endpoint(client):\n    \"\"\"Test health check endpoint.\"\"\"\n    response = client.get(\"/health\")\n\n    assert response.status_code == 200\n    assert response.json() == {\"status\": \"healthy\"}\n\ndef test_chat_endpoint(client, mock_llm):\n    \"\"\"Test chat endpoint with mock LLM.\"\"\"\n    response = client.post(\"/chat\", json={\"message\": \"hello\"})\n\n    assert response.status_code == 200\n    assert \"response\" in response.json()\n\ndef test_memory_integration(mock_memory):\n    \"\"\"Test memory integration.\"\"\"\n    mock_memory.add_message(\"user1\", \"user\", \"Hello\")\n\n    history = mock_memory.get_history(\"user1\")\n\n    assert len(history) == 1\n    assert history[0][\"role\"] == \"user\"\n    assert history[0][\"content\"] == \"Hello\"\n</code></pre>"},{"location":"advanced/testing/#run-tests","title":"Run Tests","text":"<pre><code># Run all tests\npytest\n\n# Run specific test\npytest tests/test_app.py::test_health_endpoint\n\n# Run with coverage\npytest --cov=rapidai tests/\n\n# Run with verbose output\npytest -v\n</code></pre>"},{"location":"advanced/testing/#complete-example-testing-chat-app","title":"Complete Example: Testing Chat App","text":"<pre><code># app.py\nfrom rapidai import App, LLM\nfrom rapidai.memory import ConversationMemory\nfrom rapidai.monitoring import monitor\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\nmemory = ConversationMemory()\n\n@app.route(\"/health\")\nasync def health():\n    return {\"status\": \"healthy\"}\n\n@app.route(\"/chat\", methods=[\"POST\"])\n@monitor(track_tokens=True)\nasync def chat(user_id: str, message: str):\n    memory.add_message(user_id, \"user\", message)\n    history = memory.get_history(user_id)\n\n    response = await llm.chat(history)\n\n    memory.add_message(user_id, \"assistant\", response)\n\n    return {\n        \"response\": response,\n        \"tokens_used\": 100,\n        \"model\": llm.model\n    }\n\n@app.route(\"/clear\", methods=[\"POST\"])\nasync def clear(user_id: str):\n    memory.clear(user_id)\n    return {\"message\": \"Cleared\"}\n\n# test_app.py\nimport pytest\nfrom rapidai.testing import TestClient, MockLLM, MockMemory\nfrom rapidai.monitoring import get_collector\nfrom app import app\n\n@pytest.fixture\ndef client():\n    return TestClient(app)\n\n@pytest.fixture\ndef mock_llm():\n    return MockLLM(response=\"Test response\")\n\n@pytest.fixture\ndef mock_memory():\n    return MockMemory()\n\ndef test_health(client):\n    \"\"\"Test health endpoint.\"\"\"\n    response = client.get(\"/health\")\n\n    assert response.status_code == 200\n    assert response.json() == {\"status\": \"healthy\"}\n\ndef test_chat(client):\n    \"\"\"Test chat endpoint.\"\"\"\n    response = client.post(\n        \"/chat\",\n        json={\"user_id\": \"test123\", \"message\": \"Hello\"}\n    )\n\n    assert response.status_code == 200\n    assert \"response\" in response.json()\n    assert \"model\" in response.json()\n\ndef test_chat_with_mocks(client, mock_llm, mock_memory):\n    \"\"\"Test chat with mocks.\"\"\"\n    response = client.post(\n        \"/chat\",\n        json={\"user_id\": \"user1\", \"message\": \"Hi\"}\n    )\n\n    assert response.status_code == 200\n\n    # Verify LLM was called\n    assert len(mock_llm.calls) &gt; 0\n\n    # Verify memory was used\n    assert len(mock_memory.calls) &gt; 0\n\ndef test_clear(client):\n    \"\"\"Test clear endpoint.\"\"\"\n    response = client.post(\"/clear\", json={\"user_id\": \"user1\"})\n\n    assert response.status_code == 200\n    assert response.json() == {\"message\": \"Cleared\"}\n\ndef test_monitoring(client):\n    \"\"\"Test monitoring integration.\"\"\"\n    collector = get_collector()\n    collector.clear()\n\n    # Make request\n    client.post(\"/chat\", json={\"user_id\": \"u1\", \"message\": \"test\"})\n\n    # Check metrics\n    summary = collector.get_summary()\n    assert summary[\"total_requests\"] &gt;= 1\n\ndef test_conversation_flow(client):\n    \"\"\"Test multi-turn conversation.\"\"\"\n    user_id = \"test_user\"\n\n    # First message\n    r1 = client.post(\"/chat\", json={\"user_id\": user_id, \"message\": \"Hello\"})\n    assert r1.status_code == 200\n\n    # Second message\n    r2 = client.post(\"/chat\", json={\"user_id\": user_id, \"message\": \"How are you?\"})\n    assert r2.status_code == 200\n\n    # Clear\n    r3 = client.post(\"/clear\", json={\"user_id\": user_id})\n    assert r3.status_code == 200\n</code></pre>"},{"location":"advanced/testing/#best-practices","title":"Best Practices","text":""},{"location":"advanced/testing/#1-use-mocks-for-external-services","title":"1. Use Mocks for External Services","text":"<pre><code># \u2705 Good - use mocks\nfrom rapidai.testing import MockLLM\n\nmock_llm = MockLLM(response=\"Test\")\nresult = await mock_llm.complete(\"test\")\n\n# \u274c Avoid - real API calls in tests\nllm = LLM(\"claude-3-haiku-20240307\")\nresult = await llm.complete(\"test\")  # Costs money!\n</code></pre>"},{"location":"advanced/testing/#2-test-happy-and-error-paths","title":"2. Test Happy and Error Paths","text":"<pre><code>def test_divide_success(client):\n    \"\"\"Test successful division.\"\"\"\n    response = client.post(\"/divide\", json={\"a\": 10, \"b\": 2})\n    assert response.status_code == 200\n    assert response.json() == {\"result\": 5}\n\ndef test_divide_by_zero(client):\n    \"\"\"Test division by zero error.\"\"\"\n    response = client.post(\"/divide\", json={\"a\": 10, \"b\": 0})\n    assert response.status_code == 400\n    assert \"error\" in response.json()\n</code></pre>"},{"location":"advanced/testing/#3-use-fixtures-for-reusable-components","title":"3. Use Fixtures for Reusable Components","text":"<pre><code>@pytest.fixture\ndef authenticated_client(client):\n    \"\"\"Client with authentication.\"\"\"\n    client.headers = {\"Authorization\": \"Bearer test-token\"}\n    return client\n\ndef test_protected_endpoint(authenticated_client):\n    response = authenticated_client.get(\"/protected\")\n    assert response.status_code == 200\n</code></pre>"},{"location":"advanced/testing/#4-clean-up-after-tests","title":"4. Clean Up After Tests","text":"<pre><code>@pytest.fixture\ndef collector():\n    \"\"\"Metrics collector fixture.\"\"\"\n    from rapidai.monitoring import get_collector\n\n    collector = get_collector()\n    collector.clear()  # Start clean\n\n    yield collector\n\n    collector.clear()  # Clean up after\n</code></pre>"},{"location":"advanced/testing/#5-test-realistic-scenarios","title":"5. Test Realistic Scenarios","text":"<pre><code>def test_complete_chat_flow(client, mock_llm):\n    \"\"\"Test complete user interaction.\"\"\"\n    user_id = \"user123\"\n\n    # Start conversation\n    response = client.post(\"/chat\", json={\n        \"user_id\": user_id,\n        \"message\": \"I need help with Python\"\n    })\n    assert response.status_code == 200\n\n    # Follow-up question\n    response = client.post(\"/chat\", json={\n        \"user_id\": user_id,\n        \"message\": \"Can you explain decorators?\"\n    })\n    assert response.status_code == 200\n\n    # Clear conversation\n    response = client.post(\"/clear\", json={\"user_id\": user_id})\n    assert response.status_code == 200\n</code></pre>"},{"location":"advanced/testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/testing/#tests-not-running","title":"Tests Not Running","text":"<pre><code># Ensure pytest is installed\npip install pytest\n\n# Run from project root\ncd /path/to/project\npytest\n</code></pre>"},{"location":"advanced/testing/#import-errors","title":"Import Errors","text":"<pre><code># Add project to path in conftest.py\nimport sys\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent))\n</code></pre>"},{"location":"advanced/testing/#async-test-errors","title":"Async Test Errors","text":"<pre><code># Use pytest-asyncio for async tests\npip install pytest-asyncio\n\n# Mark async tests\n@pytest.mark.asyncio\nasync def test_async_function():\n    result = await async_func()\n    assert result == expected\n</code></pre>"},{"location":"advanced/testing/#next-steps","title":"Next Steps","text":"<ul> <li>Testing API Reference - Complete testing API</li> <li>Monitoring - Test monitoring integration</li> <li>Deployment Tutorial - Test before deploying</li> </ul>"},{"location":"advanced/ui/","title":"UI Components","text":"<p>RapidAI includes pre-built UI components for rapid prototyping of AI applications. Get a beautiful chat interface running in minutes with zero frontend code.</p>"},{"location":"advanced/ui/#quick-start","title":"Quick Start","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.ui import page, get_chat_template\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return get_chat_template(\n        title=\"My AI Chat\",\n        theme=\"dark\"\n    )\n\n@app.route(\"/api/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    response = await llm.complete(message)\n    return {\"response\": response}\n\nif __name__ == \"__main__\":\n    app.run()\n</code></pre> <p>Visit <code>http://localhost:8000</code> to see your chat interface!</p>"},{"location":"advanced/ui/#features","title":"Features","text":"<ul> <li>Zero Frontend Code - Pure Python, no HTML/CSS/JS required</li> <li>Beautiful Themes - Dark and light themes out of the box</li> <li>Markdown Support - Render formatted responses</li> <li>File Uploads - Handle document uploads</li> <li>Real-time Streaming - Stream LLM responses</li> <li>Mobile Responsive - Works on all devices</li> <li>Customizable - Easy theming and branding</li> </ul>"},{"location":"advanced/ui/#chat-interface","title":"Chat Interface","text":""},{"location":"advanced/ui/#basic-chat","title":"Basic Chat","text":"<pre><code>from rapidai import App\nfrom rapidai.ui import page, get_chat_template\n\napp = App()\n\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return get_chat_template()\n</code></pre> <p>Default features: - Send messages with Enter key - Markdown rendering - Auto-scroll to latest message - Message history - Clear conversation button</p>"},{"location":"advanced/ui/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from rapidai.ui import ChatInterface, get_chat_template\n\ninterface = ChatInterface(\n    title=\"Customer Support Bot\",\n    theme=\"light\",\n    primary_color=\"#007bff\",\n    placeholder=\"How can I help you today?\",\n    enable_file_upload=True,\n    max_file_size_mb=10,\n    allowed_file_types=[\".pdf\", \".txt\", \".docx\"],\n    show_timestamps=True,\n    enable_markdown=True\n)\n\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return get_chat_template(config=interface)\n</code></pre>"},{"location":"advanced/ui/#themes","title":"Themes","text":"<p>Built-in themes:</p> <pre><code># Dark theme (default)\nget_chat_template(theme=\"dark\")\n\n# Light theme\nget_chat_template(theme=\"light\")\n</code></pre> <p>Custom theme:</p> <pre><code>interface = ChatInterface(\n    theme=\"custom\",\n    primary_color=\"#ff6b6b\",\n    background_color=\"#1a1a2e\",\n    text_color=\"#eee\",\n    user_message_color=\"#3a3a5e\",\n    bot_message_color=\"#2d2d4e\"\n)\n</code></pre>"},{"location":"advanced/ui/#page-decorator","title":"@page Decorator","text":"<p>The <code>@page</code> decorator serves HTML content with proper content type headers.</p>"},{"location":"advanced/ui/#basic-usage","title":"Basic Usage","text":"<pre><code>from rapidai.ui import page\n\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return \"&lt;h1&gt;Hello, World!&lt;/h1&gt;\"\n</code></pre>"},{"location":"advanced/ui/#with-templates","title":"With Templates","text":"<pre><code>from rapidai.ui import page, get_chat_template\n\n@app.route(\"/chat\")\n@page(\"/chat\")\nasync def chat_page():\n    return get_chat_template(title=\"AI Chat\")\n\n@app.route(\"/dashboard\")\n@page(\"/dashboard\")\nasync def dashboard():\n    return get_dashboard_template()\n</code></pre>"},{"location":"advanced/ui/#response-format","title":"Response Format","text":"<p>The decorator automatically adds:</p> <pre><code>{\n    \"status\": 200,\n    \"headers\": {\"Content-Type\": \"text/html; charset=utf-8\"},\n    \"body\": html_content\n}\n</code></pre>"},{"location":"advanced/ui/#chat-api-integration","title":"Chat API Integration","text":""},{"location":"advanced/ui/#simple-chat-endpoint","title":"Simple Chat Endpoint","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.ui import page, get_chat_template\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return get_chat_template()\n\n@app.route(\"/api/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    response = await llm.complete(message)\n    return {\"response\": response}\n</code></pre>"},{"location":"advanced/ui/#with-memory","title":"With Memory","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.memory import ConversationMemory\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\nmemory = ConversationMemory()\n\n@app.route(\"/api/chat\", methods=[\"POST\"])\nasync def chat(user_id: str, message: str):\n    # Add user message\n    memory.add_message(user_id, \"user\", message)\n\n    # Get history\n    history = memory.get_history(user_id)\n\n    # Generate response\n    response = await llm.chat(history)\n\n    # Add assistant message\n    memory.add_message(user_id, \"assistant\", response)\n\n    return {\"response\": response}\n</code></pre>"},{"location":"advanced/ui/#with-rag","title":"With RAG","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.rag import RAG\nfrom rapidai.ui import page, get_chat_template\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\nrag = RAG()\n\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return get_chat_template(\n        title=\"Document Q&amp;A\",\n        placeholder=\"Ask a question about your documents...\"\n    )\n\n@app.route(\"/api/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    # Retrieve relevant context\n    retrieval = await rag.retrieve(message, top_k=3)\n\n    # Build prompt with context\n    prompt = f\"\"\"Context:\n{retrieval.text}\n\nQuestion: {message}\n\nAnswer:\"\"\"\n\n    response = await llm.complete(prompt)\n\n    return {\n        \"response\": response,\n        \"sources\": [s[\"source\"] for s in retrieval.sources]\n    }\n</code></pre>"},{"location":"advanced/ui/#streaming-responses","title":"Streaming Responses","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.ui import page, get_chat_template\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return get_chat_template(enable_streaming=True)\n\n@app.route(\"/api/chat/stream\", methods=[\"POST\"])\nasync def chat_stream(message: str):\n    async def generate():\n        async for chunk in llm.stream(message):\n            yield f\"data: {chunk}\\n\\n\"\n\n    return {\n        \"type\": \"stream\",\n        \"generator\": generate(),\n        \"headers\": {\n            \"Content-Type\": \"text/event-stream\",\n            \"Cache-Control\": \"no-cache\"\n        }\n    }\n</code></pre>"},{"location":"advanced/ui/#file-upload-integration","title":"File Upload Integration","text":""},{"location":"advanced/ui/#enable-file-uploads","title":"Enable File Uploads","text":"<pre><code>from rapidai.ui import ChatInterface, get_chat_template\n\ninterface = ChatInterface(\n    enable_file_upload=True,\n    max_file_size_mb=10,\n    allowed_file_types=[\".pdf\", \".txt\", \".docx\"]\n)\n\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return get_chat_template(config=interface)\n</code></pre>"},{"location":"advanced/ui/#handle-uploads","title":"Handle Uploads","text":"<pre><code>from rapidai.rag import RAG\n\nrag = RAG()\n\n@app.route(\"/api/upload\", methods=[\"POST\"])\nasync def upload(file: UploadFile):\n    # Save file\n    filepath = f\"./uploads/{file.filename}\"\n    with open(filepath, \"wb\") as f:\n        f.write(await file.read())\n\n    # Process with RAG\n    chunks = await rag.add_document(filepath)\n\n    return {\n        \"message\": \"File uploaded successfully\",\n        \"chunks\": len(chunks),\n        \"filename\": file.filename\n    }\n\n@app.route(\"/api/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    # Query RAG\n    retrieval = await rag.retrieve(message)\n    prompt = f\"Context: {retrieval.text}\\n\\nQuestion: {message}\"\n    response = await llm.complete(prompt)\n\n    return {\"response\": response}\n</code></pre>"},{"location":"advanced/ui/#custom-templates","title":"Custom Templates","text":""},{"location":"advanced/ui/#create-custom-html","title":"Create Custom HTML","text":"<pre><code>from rapidai.ui import page\n\ndef get_custom_template():\n    return \"\"\"\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;Custom App&lt;/title&gt;\n        &lt;style&gt;\n            body {\n                font-family: system-ui;\n                max-width: 800px;\n                margin: 0 auto;\n                padding: 20px;\n            }\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;h1&gt;My Custom AI App&lt;/h1&gt;\n        &lt;div id=\"chat\"&gt;&lt;/div&gt;\n        &lt;input type=\"text\" id=\"input\" placeholder=\"Type a message...\" /&gt;\n        &lt;button onclick=\"send()\"&gt;Send&lt;/button&gt;\n\n        &lt;script&gt;\n            async function send() {\n                const input = document.getElementById('input');\n                const message = input.value;\n\n                const response = await fetch('/api/chat', {\n                    method: 'POST',\n                    headers: {'Content-Type': 'application/json'},\n                    body: JSON.stringify({message})\n                });\n\n                const data = await response.json();\n                document.getElementById('chat').innerHTML +=\n                    `&lt;p&gt;&lt;strong&gt;You:&lt;/strong&gt; ${message}&lt;/p&gt;` +\n                    `&lt;p&gt;&lt;strong&gt;Bot:&lt;/strong&gt; ${data.response}&lt;/p&gt;`;\n\n                input.value = '';\n            }\n        &lt;/script&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return get_custom_template()\n</code></pre>"},{"location":"advanced/ui/#extend-chat-template","title":"Extend Chat Template","text":"<pre><code>from rapidai.ui import get_chat_template\n\ndef get_enhanced_chat():\n    base = get_chat_template()\n\n    # Add custom CSS\n    custom_css = \"\"\"\n    &lt;style&gt;\n        .custom-header {\n            background: linear-gradient(to right, #667eea, #764ba2);\n            padding: 20px;\n            color: white;\n        }\n    &lt;/style&gt;\n    \"\"\"\n\n    # Inject before &lt;/head&gt;\n    return base.replace(\"&lt;/head&gt;\", f\"{custom_css}&lt;/head&gt;\")\n\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return get_enhanced_chat()\n</code></pre>"},{"location":"advanced/ui/#complete-example-support-bot","title":"Complete Example: Support Bot","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.memory import ConversationMemory\nfrom rapidai.rag import RAG\nfrom rapidai.ui import page, ChatInterface, get_chat_template\n\napp = App(title=\"Customer Support Bot\")\nllm = LLM(\"claude-3-haiku-20240307\")\nmemory = ConversationMemory()\nrag = RAG()\n\n# Load knowledge base\nasync def load_docs():\n    docs = [\"faq.pdf\", \"manual.pdf\", \"policies.txt\"]\n    for doc in docs:\n        await rag.add_document(doc)\n\n@app.on_startup\nasync def startup():\n    await load_docs()\n\n# Serve UI\ninterface = ChatInterface(\n    title=\"Customer Support\",\n    theme=\"light\",\n    primary_color=\"#0066cc\",\n    placeholder=\"How can we help you today?\",\n    show_timestamps=True\n)\n\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return get_chat_template(config=interface)\n\n# Chat endpoint\n@app.route(\"/api/chat\", methods=[\"POST\"])\nasync def chat(user_id: str, message: str):\n    # Add to memory\n    memory.add_message(user_id, \"user\", message)\n\n    # Retrieve relevant docs\n    retrieval = await rag.retrieve(message, top_k=3)\n\n    # Get conversation history\n    history = memory.get_history(user_id, limit=10)\n\n    # Build context-aware prompt\n    system = f\"\"\"You are a helpful customer support agent.\nUse the following knowledge base to answer questions:\n\n{retrieval.text}\n\nBe concise, friendly, and helpful.\"\"\"\n\n    messages = [{\"role\": \"system\", \"content\": system}] + history\n\n    # Generate response\n    response = await llm.chat(messages)\n\n    # Add to memory\n    memory.add_message(user_id, \"assistant\", response)\n\n    return {\n        \"response\": response,\n        \"sources\": [s[\"source\"] for s in retrieval.sources]\n    }\n\n# Clear conversation\n@app.route(\"/api/clear\", methods=[\"POST\"])\nasync def clear(user_id: str):\n    memory.clear(user_id)\n    return {\"message\": \"Conversation cleared\"}\n\nif __name__ == \"__main__\":\n    app.run(port=8000)\n</code></pre>"},{"location":"advanced/ui/#best-practices","title":"Best Practices","text":""},{"location":"advanced/ui/#1-use-semantic-html","title":"1. Use Semantic HTML","text":"<pre><code># \u2705 Good\nreturn get_chat_template(title=\"My App\")\n\n# \u274c Avoid\nreturn \"&lt;h1&gt;My App&lt;/h1&gt;&lt;div&gt;...&lt;/div&gt;\"  # Rebuild entire UI\n</code></pre>"},{"location":"advanced/ui/#2-separate-ui-and-api","title":"2. Separate UI and API","text":"<pre><code># UI routes\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return get_chat_template()\n\n# API routes\n@app.route(\"/api/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    return {\"response\": await llm.complete(message)}\n</code></pre>"},{"location":"advanced/ui/#3-handle-errors-gracefully","title":"3. Handle Errors Gracefully","text":"<pre><code>@app.route(\"/api/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    try:\n        response = await llm.complete(message)\n        return {\"response\": response}\n    except Exception as e:\n        return {\n            \"error\": \"Sorry, something went wrong. Please try again.\",\n            \"details\": str(e)\n        }, 500\n</code></pre>"},{"location":"advanced/ui/#4-add-loading-states","title":"4. Add Loading States","text":"<pre><code># Frontend automatically shows loading spinner\n# Just ensure timely responses\n\n@app.route(\"/api/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    # Long-running task\n    response = await llm.complete(message)  # User sees loading...\n    return {\"response\": response}\n</code></pre>"},{"location":"advanced/ui/#5-validate-input","title":"5. Validate Input","text":"<pre><code>@app.route(\"/api/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    if not message or len(message) &lt; 1:\n        return {\"error\": \"Message cannot be empty\"}, 400\n\n    if len(message) &gt; 1000:\n        return {\"error\": \"Message too long (max 1000 chars)\"}, 400\n\n    response = await llm.complete(message)\n    return {\"response\": response}\n</code></pre>"},{"location":"advanced/ui/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/ui/#template-not-rendering","title":"Template Not Rendering","text":"<pre><code># Ensure @page decorator is applied\n@app.route(\"/\")\n@page(\"/\")  # \u2705 Correct\nasync def index():\n    return get_chat_template()\n\n# Missing decorator\n@app.route(\"/\")  # \u274c Returns HTML as JSON\nasync def index():\n    return get_chat_template()\n</code></pre>"},{"location":"advanced/ui/#file-upload-not-working","title":"File Upload Not Working","text":"<pre><code># Ensure file upload is enabled\ninterface = ChatInterface(\n    enable_file_upload=True,  # Must be True\n    max_file_size_mb=10\n)\n\n# Handle upload endpoint\n@app.route(\"/api/upload\", methods=[\"POST\"])\nasync def upload(file: UploadFile):\n    # Process file\n    return {\"message\": \"Uploaded\"}\n</code></pre>"},{"location":"advanced/ui/#styling-issues","title":"Styling Issues","text":"<pre><code># Use custom CSS for fine-tuning\ninterface = ChatInterface(\n    theme=\"custom\",\n    primary_color=\"#your-color\",\n    background_color=\"#your-bg\"\n)\n\n# Or extend template with custom CSS\n</code></pre>"},{"location":"advanced/ui/#next-steps","title":"Next Steps","text":"<ul> <li>UI API Reference - Complete UI API</li> <li>Testing - Test your UI endpoints</li> <li>Deployment Tutorial - Deploy your UI</li> </ul>"},{"location":"deployment/best-practices/","title":"uest practices","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"deployment/cloud/","title":"cloud","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"deployment/docker/","title":"docker","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"deployment/overview/","title":"overview","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"getting-started/first-steps/","title":"First Steps","text":"<p>Let's create your first RapidAI application in under 5 minutes! \u26a1</p>"},{"location":"getting-started/first-steps/#create-your-first-app","title":"Create Your First App","text":"<p>Create a file called <code>app.py</code>:</p> app.py<pre><code>from rapidai import App, LLM, stream\n\n# Create the app\napp = App()\n\n# Initialize LLM\nllm = LLM(\"claude-3-haiku-20240307\")\n\n# Define a streaming chat endpoint\n@app.route(\"/chat\", methods=[\"POST\"])\n@stream\nasync def chat(message: str):\n    \"\"\"Stream a chat response.\"\"\"\n    response = await llm.chat(message, stream=True)\n    async for chunk in response:\n        yield chunk\n\n# Run the server\nif __name__ == \"__main__\":\n    app.run(port=8000)\n</code></pre>"},{"location":"getting-started/first-steps/#run-your-app","title":"Run Your App","text":"python app.py <p>You should see:</p> <pre><code>INFO:     Started server process [12345]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000\n</code></pre> <p>Server Running!</p> <p>Your AI-powered API is now live at <code>http://localhost:8000</code></p>"},{"location":"getting-started/first-steps/#test-your-app","title":"Test Your App","text":"<p>Open a new terminal and test your endpoint:</p> Test with curl<pre><code>curl -X POST http://localhost:8000/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"message\": \"Hello! Tell me a joke.\"}'\n</code></pre> <p>You'll see streaming output:</p> <pre><code>event: message\ndata: Why don't scientists trust atoms?\n\nevent: message\ndata: Because they make up everything!\n</code></pre>"},{"location":"getting-started/first-steps/#understanding-the-code","title":"Understanding the Code","text":"<p>Let's break down what's happening:</p>"},{"location":"getting-started/first-steps/#1-import-components","title":"1. Import Components","text":"<pre><code>from rapidai import App, LLM, stream\n</code></pre> <ul> <li><code>App</code> - The main application class</li> <li><code>LLM</code> - Unified LLM interface</li> <li><code>stream</code> - Decorator for streaming responses</li> </ul>"},{"location":"getting-started/first-steps/#2-create-app-and-llm","title":"2. Create App and LLM","text":"<pre><code>app = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n</code></pre> <ul> <li><code>App()</code> creates your ASGI application</li> <li><code>LLM()</code> auto-detects provider from model name</li> </ul>"},{"location":"getting-started/first-steps/#3-define-routes","title":"3. Define Routes","text":"<pre><code>@app.route(\"/chat\", methods=[\"POST\"])\n@stream\nasync def chat(message: str):\n    response = await llm.chat(message, stream=True)\n    async for chunk in response:\n        yield chunk\n</code></pre> <ul> <li><code>@app.route()</code> registers an endpoint</li> <li><code>@stream</code> enables Server-Sent Events</li> <li><code>async def</code> makes it asynchronous</li> <li>Parameters auto-extracted from request</li> </ul>"},{"location":"getting-started/first-steps/#4-run-the-server","title":"4. Run the Server","text":"<pre><code>if __name__ == \"__main__\":\n    app.run(port=8000)\n</code></pre> <p>Starts uvicorn ASGI server on port 8000</p>"},{"location":"getting-started/first-steps/#add-a-health-check","title":"Add a Health Check","text":"<p>Let's add a health check endpoint:</p> app.py<pre><code>from rapidai import App, LLM, stream\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/chat\", methods=[\"POST\"])\n@stream\nasync def chat(message: str):\n    \"\"\"Stream a chat response.\"\"\"\n    response = await llm.chat(message, stream=True)\n    async for chunk in response:\n        yield chunk\n\n# Add health check\n@app.route(\"/health\", methods=[\"GET\"])\nasync def health():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\"status\": \"healthy\", \"version\": \"0.1.0\"}\n\nif __name__ == \"__main__\":\n    app.run(port=8000)\n</code></pre> <p>Test it:</p> <pre><code>curl http://localhost:8000/health\n</code></pre> <p>Output: <pre><code>{\"status\": \"healthy\", \"version\": \"0.1.0\"}\n</code></pre></p>"},{"location":"getting-started/first-steps/#add-multiple-routes","title":"Add Multiple Routes","text":"<p>Expand your app with more endpoints:</p> app.py<pre><code>from rapidai import App, LLM, stream\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/chat\", methods=[\"POST\"])\n@stream\nasync def chat(message: str):\n    \"\"\"General chat.\"\"\"\n    response = await llm.chat(message, stream=True)\n    async for chunk in response:\n        yield chunk\n\n@app.route(\"/summarize\", methods=[\"POST\"])\nasync def summarize(text: str):\n    \"\"\"Summarize text.\"\"\"\n    prompt = f\"Summarize this concisely:\\n\\n{text}\"\n    response = await llm.chat(prompt)\n    return {\"summary\": response}\n\n@app.route(\"/translate\", methods=[\"POST\"])\nasync def translate(text: str, target_lang: str = \"Spanish\"):\n    \"\"\"Translate text.\"\"\"\n    prompt = f\"Translate to {target_lang}:\\n\\n{text}\"\n    response = await llm.chat(prompt)\n    return {\"translation\": response, \"language\": target_lang}\n\nif __name__ == \"__main__\":\n    app.run(port=8000)\n</code></pre> <p>Test the new endpoints:</p> <pre><code># Summarize\ncurl -X POST http://localhost:8000/summarize \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\": \"Long article text here...\"}'\n\n# Translate\ncurl -X POST http://localhost:8000/translate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\": \"Hello, how are you?\", \"target_lang\": \"French\"}'\n</code></pre>"},{"location":"getting-started/first-steps/#next-steps","title":"Next Steps","text":"<p>You're Ready!</p> <p>You've created your first RapidAI application!</p> <p>Continue Learning:</p> <ul> <li>Quick Start Guide - Learn more features</li> <li>Tutorial - Step-by-step walkthrough</li> <li>Streaming - Deep dive into streaming</li> <li>Memory - Add conversation memory</li> </ul> <p>Try This:</p> <ul> <li>Switch to a different LLM provider</li> <li>Add error handling</li> <li>Implement rate limiting</li> <li>Deploy to production</li> </ul> Continue to Quick Start \u2192"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<p>Python Version</p> <p>RapidAI requires Python 3.9 or higher</p>"},{"location":"getting-started/installation/#install-rapidai","title":"Install RapidAI","text":"pipWith AnthropicWith OpenAIWith All FeaturesFor Development <pre><code>pip install rapidai\n</code></pre> <pre><code>pip install rapidai[anthropic]\n</code></pre> <pre><code>pip install rapidai[openai]\n</code></pre> <pre><code>pip install rapidai[all]\n</code></pre> <pre><code>git clone https://github.com/shaungehring/rapidai.git\ncd rapidai\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>RapidAI has several optional dependency groups:</p> Extra Description Install Command <code>anthropic</code> Anthropic Claude support <code>pip install rapidai[anthropic]</code> <code>openai</code> OpenAI GPT support <code>pip install rapidai[openai]</code> <code>cohere</code> Cohere support <code>pip install rapidai[cohere]</code> <code>rag</code> RAG features (vector DB, embeddings) <code>pip install rapidai[rag]</code> <code>redis</code> Redis cache/memory backend <code>pip install rapidai[redis]</code> <code>postgres</code> PostgreSQL memory backend <code>pip install rapidai[postgres]</code> <code>all</code> All optional features <code>pip install rapidai[all]</code> <code>dev</code> Development tools <code>pip install rapidai[dev]</code>"},{"location":"getting-started/installation/#setup-api-keys","title":"Setup API Keys","text":"<p>Create a <code>.env</code> file in your project root:</p> .env<pre><code># Anthropic (Claude)\nANTHROPIC_API_KEY=sk-ant-your-key-here\n\n# OpenAI (GPT)\nOPENAI_API_KEY=sk-your-key-here\n\n# Cohere\nCOHERE_API_KEY=your-key-here\n\n# Optional: Default settings\nRAPIDAI_LLM_PROVIDER=anthropic\nRAPIDAI_LLM_MODEL=claude-3-haiku-20240307\nRAPIDAI_LLM_TEMPERATURE=0.7\nRAPIDAI_LLM_MAX_TOKENS=4000\n</code></pre> <p>Keep Your Keys Secret</p> <p>Never commit <code>.env</code> files to version control. Add <code>.env</code> to your <code>.gitignore</code>.</p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"test_install.py<pre><code>import rapidai\n\nprint(f\"RapidAI version: {rapidai.__version__}\")\nprint(\"\u2713 Installation successful!\")\n</code></pre> python test_install.py <p>Expected output: <pre><code>RapidAI version: 0.1.0\n\u2713 Installation successful!\n</code></pre></p>"},{"location":"getting-started/installation/#get-api-keys","title":"Get API Keys","text":""},{"location":"getting-started/installation/#anthropic-claude","title":"Anthropic (Claude)","text":"<ol> <li>Visit console.anthropic.com</li> <li>Sign up or log in</li> <li>Navigate to API Keys</li> <li>Create a new API key</li> <li>Copy and add to your <code>.env</code> file</li> </ol>"},{"location":"getting-started/installation/#openai-gpt","title":"OpenAI (GPT)","text":"<ol> <li>Visit platform.openai.com</li> <li>Sign up or log in</li> <li>Navigate to API Keys</li> <li>Create a new API key</li> <li>Copy and add to your <code>.env</code> file</li> </ol>"},{"location":"getting-started/installation/#cohere","title":"Cohere","text":"<ol> <li>Visit dashboard.cohere.com</li> <li>Sign up or log in</li> <li>Navigate to API Keys</li> <li>Copy your API key</li> <li>Add to your <code>.env</code> file</li> </ol>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#modulenotfounderror","title":"ModuleNotFoundError","text":"<p>If you get <code>ModuleNotFoundError: No module named 'rapidai'</code>:</p> <ol> <li>Check your Python version: <code>python --version</code></li> <li>Verify installation: <code>pip list | grep rapidai</code></li> <li>Try reinstalling: <code>pip install --force-reinstall rapidai</code></li> </ol>"},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>If you get import errors for optional dependencies:</p> <pre><code># \u274c This will fail if anthropic not installed\nfrom rapidai import LLM\nllm = LLM(\"claude-3-haiku-20240307\")\n</code></pre> <p>Solution: <pre><code>pip install rapidai[anthropic]\n</code></pre></p>"},{"location":"getting-started/installation/#api-key-not-found","title":"API Key Not Found","text":"<p>If you get \"API key not found\" errors:</p> <ol> <li>Check <code>.env</code> file exists in project root</li> <li>Verify key names match: <code>ANTHROPIC_API_KEY</code>, <code>OPENAI_API_KEY</code></li> <li>No quotes around keys in <code>.env</code></li> <li>No spaces around <code>=</code> in <code>.env</code></li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>First Steps - Create your first RapidAI app</li> <li>Quick Start - Learn the basics</li> <li>Tutorial - Step-by-step guide</li> </ul> <p>Ready to Go!</p> <p>Installation complete! Let's build something amazing.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide covers the essential features of RapidAI in 10 minutes.</p>"},{"location":"getting-started/quickstart/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Chatbot</li> <li>Streaming Responses</li> <li>Conversation Memory</li> <li>Caching</li> <li>Multiple LLM Providers</li> </ol>"},{"location":"getting-started/quickstart/#basic-chatbot","title":"Basic Chatbot","text":"<p>The simplest possible chatbot:</p> <pre><code>from rapidai import App, LLM\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    return await llm.chat(message)\n\nif __name__ == \"__main__\":\n    app.run()\n</code></pre> <p>Non-Streaming</p> <p>Without <code>@stream</code>, responses are returned all at once.</p>"},{"location":"getting-started/quickstart/#streaming-responses","title":"Streaming Responses","text":"<p>Add real-time streaming with one decorator:</p> <pre><code>from rapidai import App, LLM, stream\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/chat\", methods=[\"POST\"])\n@stream\nasync def chat(message: str):\n    response = await llm.chat(message, stream=True)\n    async for chunk in response:\n        yield chunk\n\nif __name__ == \"__main__\":\n    app.run()\n</code></pre> <p>Streaming Enabled</p> <p>Responses now stream via Server-Sent Events (SSE)</p> <p>Test with curl:</p> <pre><code>curl -N -X POST http://localhost:8000/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"message\": \"Tell me a story\"}'\n</code></pre>"},{"location":"getting-started/quickstart/#conversation-memory","title":"Conversation Memory","text":"<p>Add stateful conversations:</p> <pre><code>from rapidai import App, LLM, stream\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/chat\", methods=[\"POST\"])\n@stream\nasync def chat(user_id: str, message: str):\n    # Get user's conversation memory\n    memory = app.memory(user_id)\n    history = memory.to_dict_list()\n\n    # Chat with context\n    response = await llm.chat(message, history=history, stream=True)\n    async for chunk in response:\n        yield chunk\n\n    # Save to memory\n    memory.add(\"user\", message)\n    memory.add(\"assistant\", \"[full response here]\")\n\nif __name__ == \"__main__\":\n    app.run()\n</code></pre> <p>Test context awareness:</p> <pre><code># First message\ncurl -X POST http://localhost:8000/chat \\\n  -d '{\"user_id\": \"alice\", \"message\": \"My name is Alice\"}'\n\n# Second message - bot remembers!\ncurl -X POST http://localhost:8000/chat \\\n  -d '{\"user_id\": \"alice\", \"message\": \"What is my name?\"}'\n</code></pre> <p>Memory Backends</p> <p>Default is in-memory. Use Redis or PostgreSQL for production.</p>"},{"location":"getting-started/quickstart/#caching","title":"Caching","text":"<p>Save money and time with automatic caching:</p> <pre><code>from rapidai import App, LLM, cache\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/summarize\", methods=[\"POST\"])\n@cache(ttl=3600)  # Cache for 1 hour\nasync def summarize(text: str):\n    prompt = f\"Summarize: {text}\"\n    return await llm.chat(prompt)\n\nif __name__ == \"__main__\":\n    app.run()\n</code></pre> <p>Smart Caching</p> <p>Identical requests return cached results instantly!</p>"},{"location":"getting-started/quickstart/#multiple-llm-providers","title":"Multiple LLM Providers","text":"<p>Use different models for different tasks:</p> <pre><code>from rapidai import App, LLM\n\napp = App()\n\n# Different models for different purposes\nfast_llm = LLM(\"claude-3-haiku-20240307\")  # Fast &amp; cheap\nsmart_llm = LLM(\"claude-3-sonnet-20240229\")  # More capable\n\n@app.route(\"/quick\", methods=[\"POST\"])\nasync def quick_answer(question: str):\n    \"\"\"Fast responses for simple questions.\"\"\"\n    return await fast_llm.chat(question)\n\n@app.route(\"/deep\", methods=[\"POST\"])\nasync def deep_analysis(question: str):\n    \"\"\"Detailed analysis for complex questions.\"\"\"\n    return await smart_llm.chat(question)\n\nif __name__ == \"__main__\":\n    app.run()\n</code></pre>"},{"location":"getting-started/quickstart/#switch-providers-easily","title":"Switch Providers Easily","text":"<pre><code># Anthropic Claude\nllm = LLM(\"claude-3-haiku-20240307\")\n\n# OpenAI GPT\nllm = LLM(\"gpt-4o-mini\")\n\n# Cohere\nllm = LLM(\"command-r\")\n\n# Auto-detection works!\n</code></pre>"},{"location":"getting-started/quickstart/#complete-example","title":"Complete Example","text":"<p>Put it all together:</p> production_app.py<pre><code>from rapidai import App, LLM, stream, cache\n\napp = App(title=\"AI Assistant\", version=\"1.0.0\")\nllm = LLM(\"claude-3-haiku-20240307\")\n\n# Streaming chat with memory\n@app.route(\"/chat\", methods=[\"POST\"])\n@stream\nasync def chat(user_id: str, message: str):\n    memory = app.memory(user_id)\n    history = memory.to_dict_list()\n\n    response = await llm.chat(message, history=history, stream=True)\n    async for chunk in response:\n        yield chunk\n\n    memory.add(\"user\", message)\n    memory.add(\"assistant\", \"[response]\")\n\n# Cached summarization\n@app.route(\"/summarize\", methods=[\"POST\"])\n@cache(ttl=3600)\nasync def summarize(text: str):\n    return await llm.chat(f\"Summarize: {text}\")\n\n# Health check\n@app.route(\"/health\", methods=[\"GET\"])\nasync def health():\n    return {\"status\": \"healthy\"}\n\n# Clear user memory\n@app.route(\"/clear\", methods=[\"POST\"])\nasync def clear(user_id: str):\n    memory = app.memory(user_id)\n    memory.clear()\n    return {\"status\": \"cleared\"}\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"getting-started/quickstart/#configuration","title":"Configuration","text":"<p>Create <code>rapidai.yaml</code> for advanced configuration:</p> rapidai.yaml<pre><code>app:\n  name: my-ai-app\n  debug: false\n\nllm:\n  default_provider: anthropic\n  default_model: claude-3-haiku-20240307\n  temperature: 0.7\n  max_tokens: 4000\n\ncache:\n  enabled: true\n  backend: redis\n  ttl: 3600\n  redis_url: redis://localhost:6379\n\nmemory:\n  backend: redis\n  max_history: 10\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>Ready for More?</p> <p>You now know the core features of RapidAI!</p> <p>Continue Learning:</p> <ul> <li>Tutorial - Comprehensive walkthrough</li> <li>Streaming Deep Dive</li> <li>Memory Management</li> <li>Advanced Features</li> </ul> <p>Deploy:</p> <ul> <li>Docker Deployment</li> <li>Cloud Platforms</li> <li>Production Best Practices</li> </ul> Start Tutorial \u2192"},{"location":"reference/app/","title":"App Reference","text":"<p>The <code>App</code> class is the core of RapidAI, providing routing, middleware, and application lifecycle management.</p>"},{"location":"reference/app/#class-app","title":"Class: <code>App</code>","text":"<pre><code>from rapidai import App\n\napp = App(\n    title=\"My AI App\",\n    version=\"1.0.0\",\n    config=None\n)\n</code></pre>"},{"location":"reference/app/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>title</code> <code>str</code> <code>\"RapidAI Application\"</code> Application title <code>version</code> <code>str</code> <code>\"1.0.0\"</code> Application version <code>config</code> <code>RapidAIConfig</code> <code>None</code> Configuration object (auto-loaded if None)"},{"location":"reference/app/#example","title":"Example","text":"<pre><code>from rapidai import App\n\n# Simple initialization\napp = App()\n\n# With custom title and version\napp = App(title=\"AI Assistant\", version=\"2.0.0\")\n\n# With custom configuration\nfrom rapidai import RapidAIConfig\nconfig = RapidAIConfig.load(\"config.yaml\")\napp = App(config=config)\n</code></pre>"},{"location":"reference/app/#methods","title":"Methods","text":""},{"location":"reference/app/#route","title":"<code>route()</code>","text":"<p>Register a route handler.</p> <pre><code>@app.route(path: str, methods: List[str] = None)\n</code></pre>"},{"location":"reference/app/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>path</code> <code>str</code> Required URL path <code>methods</code> <code>List[str]</code> <code>[\"GET\"]</code> HTTP methods"},{"location":"reference/app/#example_1","title":"Example","text":"<pre><code>@app.route(\"/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    return {\"response\": \"Hello!\"}\n\n@app.route(\"/health\", methods=[\"GET\"])\nasync def health():\n    return {\"status\": \"healthy\"}\n</code></pre>"},{"location":"reference/app/#use","title":"<code>use()</code>","text":"<p>Add middleware to the application.</p> <pre><code>app.use(middleware: Middleware)\n</code></pre>"},{"location":"reference/app/#parameters_2","title":"Parameters","text":"Parameter Type Description <code>middleware</code> <code>Middleware</code> Middleware function"},{"location":"reference/app/#example_2","title":"Example","text":"<pre><code>async def logging_middleware(request, next):\n    print(f\"Request: {request}\")\n    response = await next()\n    print(f\"Response: {response}\")\n    return response\n\napp.use(logging_middleware)\n</code></pre>"},{"location":"reference/app/#memory","title":"<code>memory()</code>","text":"<p>Get conversation memory for a user.</p> <pre><code>app.memory(user_id: str) -&gt; ConversationMemory\n</code></pre>"},{"location":"reference/app/#parameters_3","title":"Parameters","text":"Parameter Type Description <code>user_id</code> <code>str</code> User identifier"},{"location":"reference/app/#returns","title":"Returns","text":"<p><code>ConversationMemory</code> - Memory instance for the user</p>"},{"location":"reference/app/#example_3","title":"Example","text":"<pre><code>@app.route(\"/chat\", methods=[\"POST\"])\nasync def chat(user_id: str, message: str):\n    memory = app.memory(user_id)\n    history = memory.get()\n    # Use history...\n</code></pre>"},{"location":"reference/app/#run","title":"<code>run()</code>","text":"<p>Run the application server.</p> <pre><code>app.run(\n    host: str = None,\n    port: int = None,\n    workers: int = None\n)\n</code></pre>"},{"location":"reference/app/#parameters_4","title":"Parameters","text":"Parameter Type Default Description <code>host</code> <code>str</code> <code>\"0.0.0.0\"</code> Host to bind to <code>port</code> <code>int</code> <code>8000</code> Port to bind to <code>workers</code> <code>int</code> <code>1</code> Number of workers"},{"location":"reference/app/#example_4","title":"Example","text":"<pre><code>if __name__ == \"__main__\":\n    # Default settings\n    app.run()\n\n    # Custom port\n    app.run(port=3000)\n\n    # Production settings\n    app.run(host=\"0.0.0.0\", port=8000, workers=4)\n</code></pre>"},{"location":"reference/app/#attributes","title":"Attributes","text":""},{"location":"reference/app/#title","title":"<code>title</code>","text":"<p>Application title.</p> <pre><code>app.title -&gt; str\n</code></pre>"},{"location":"reference/app/#version","title":"<code>version</code>","text":"<p>Application version.</p> <pre><code>app.version -&gt; str\n</code></pre>"},{"location":"reference/app/#config","title":"<code>config</code>","text":"<p>Application configuration.</p> <pre><code>app.config -&gt; RapidAIConfig\n</code></pre>"},{"location":"reference/app/#full-example","title":"Full Example","text":"<pre><code>from rapidai import App, LLM, stream, cache\n\n# Initialize\napp = App(title=\"AI Assistant\", version=\"1.0.0\")\nllm = LLM(\"claude-3-haiku-20240307\")\n\n# Middleware\nasync def auth_middleware(request, next):\n    # Add authentication logic\n    return await next()\n\napp.use(auth_middleware)\n\n# Routes\n@app.route(\"/chat\", methods=[\"POST\"])\n@stream\nasync def chat(user_id: str, message: str):\n    memory = app.memory(user_id)\n    history = memory.to_dict_list()\n\n    response = await llm.chat(message, history=history, stream=True)\n    async for chunk in response:\n        yield chunk\n\n    memory.add(\"user\", message)\n    memory.add(\"assistant\", \"[response]\")\n\n@app.route(\"/summarize\", methods=[\"POST\"])\n@cache(ttl=3600)\nasync def summarize(text: str):\n    return await llm.chat(f\"Summarize: {text}\")\n\n@app.route(\"/health\")\nasync def health():\n    return {\"status\": \"healthy\"}\n\n# Run\nif __name__ == \"__main__\":\n    app.run(port=8000)\n</code></pre>"},{"location":"reference/app/#see-also","title":"See Also","text":"<ul> <li>LLM Reference</li> <li>Streaming Reference</li> <li>Memory Reference</li> <li>Cache Reference</li> </ul>"},{"location":"reference/background/","title":"Background Jobs API Reference","text":"<p>Complete API reference for RapidAI's background job system.</p>"},{"location":"reference/background/#decorator","title":"Decorator","text":""},{"location":"reference/background/#background","title":"background","text":"<pre><code>def background(\n    max_retries: int = 3,\n    queue: Optional[JobQueue] = None\n) -&gt; Callable\n</code></pre> <p>Decorator to run a function as a background job.</p> <p>Parameters:</p> Parameter Type Default Description <code>max_retries</code> <code>int</code> <code>3</code> Maximum retry attempts on failure <code>queue</code> <code>JobQueue</code> <code>None</code> Custom job queue (uses default if None) <p>Returns: Decorated function with <code>enqueue()</code>, <code>get_result()</code>, and <code>cancel()</code> methods</p> <p>Example:</p> <pre><code>from rapidai.background import background\n\n@background(max_retries=5)\nasync def process_task(data: str):\n    return {\"processed\": data}\n\n# Enqueue\njob_id = await process_task.enqueue(data=\"test\")\n\n# Get result\nresult = await process_task.get_result(job_id)\n\n# Cancel\ncancelled = await process_task.cancel(job_id)\n</code></pre>"},{"location":"reference/background/#classes","title":"Classes","text":""},{"location":"reference/background/#jobresult","title":"JobResult","text":"<pre><code>@dataclass\nclass JobResult:\n    job_id: str\n    status: JobStatus\n    result: Any = None\n    error: Optional[str] = None\n    created_at: datetime\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    attempts: int = 0\n    max_retries: int = 3\n</code></pre> <p>Properties:</p> Property Type Description <code>job_id</code> <code>str</code> Unique job identifier <code>status</code> <code>JobStatus</code> Current job status <code>result</code> <code>Any</code> Job result (if completed) <code>error</code> <code>str</code> Error message (if failed) <code>created_at</code> <code>datetime</code> When job was created <code>started_at</code> <code>datetime</code> When job started running <code>completed_at</code> <code>datetime</code> When job finished <code>attempts</code> <code>int</code> Number of execution attempts <code>max_retries</code> <code>int</code> Maximum retry attempts <p>Methods:</p>"},{"location":"reference/background/#duration","title":"<code>duration</code>","text":"<pre><code>@property\ndef duration(self) -&gt; Optional[float]\n</code></pre> <p>Get job duration in seconds.</p> <p>Returns: <code>float</code> - Duration in seconds, or <code>None</code> if not completed</p>"},{"location":"reference/background/#is_done","title":"<code>is_done</code>","text":"<pre><code>@property\ndef is_done(self) -&gt; bool\n</code></pre> <p>Check if job is in a terminal state.</p> <p>Returns: <code>bool</code> - True if completed, failed, or cancelled</p>"},{"location":"reference/background/#jobstatus","title":"JobStatus","text":"<pre><code>class JobStatus(str, Enum):\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n</code></pre> <p>Values:</p> <ul> <li><code>PENDING</code> - Job queued, waiting to start</li> <li><code>RUNNING</code> - Job currently executing</li> <li><code>COMPLETED</code> - Job finished successfully</li> <li><code>FAILED</code> - Job failed after all retries</li> <li><code>CANCELLED</code> - Job was cancelled</li> </ul>"},{"location":"reference/background/#jobqueue","title":"JobQueue","text":"<pre><code>class JobQueue:\n    async def enqueue(\n        self,\n        job_id: str,\n        func: Callable,\n        args: tuple,\n        kwargs: dict,\n        max_retries: int = 3\n    ) -&gt; str\n\n    async def get_result(self, job_id: str) -&gt; Optional[JobResult]\n\n    async def cancel(self, job_id: str) -&gt; bool\n\n    async def list_jobs(\n        self,\n        status: Optional[JobStatus] = None\n    ) -&gt; List[JobResult]\n</code></pre> <p>Base class for job queue backends.</p>"},{"location":"reference/background/#inmemoryqueue","title":"InMemoryQueue","text":"<pre><code>class InMemoryQueue(JobQueue):\n    def __init__(self) -&gt; None\n</code></pre> <p>In-memory job queue implementation.</p> <p>Features: - Fast execution - No external dependencies - Jobs lost on restart - Single-server only</p> <p>Example:</p> <pre><code>from rapidai.background import InMemoryQueue\n\nqueue = InMemoryQueue()\n</code></pre>"},{"location":"reference/background/#redisqueue","title":"RedisQueue","text":"<pre><code>class RedisQueue(JobQueue):\n    def __init__(\n        self,\n        url: str = \"redis://localhost:6379\",\n        prefix: str = \"rapidai:jobs:\"\n    ) -&gt; None\n</code></pre> <p>Redis-backed job queue implementation.</p> <p>Parameters:</p> Parameter Type Default Description <code>url</code> <code>str</code> <code>\"redis://localhost:6379\"</code> Redis connection URL <code>prefix</code> <code>str</code> <code>\"rapidai:jobs:\"</code> Key prefix for namespacing <p>Features: - Persistent storage - Survives restarts - Multi-server support - Production-ready</p> <p>Example:</p> <pre><code>from rapidai.background import RedisQueue\n\nqueue = RedisQueue(\n    url=\"redis://localhost:6379\",\n    prefix=\"myapp:jobs:\"\n)\n</code></pre>"},{"location":"reference/background/#functions","title":"Functions","text":""},{"location":"reference/background/#get_queue","title":"get_queue","text":"<pre><code>def get_queue(\n    backend: str = \"memory\",\n    **kwargs: Any\n) -&gt; JobQueue\n</code></pre> <p>Get or create job queue.</p> <p>Parameters:</p> Parameter Type Default Description <code>backend</code> <code>str</code> <code>\"memory\"</code> Queue backend (\"memory\" or \"redis\") <code>**kwargs</code> <code>Any</code> - Backend-specific arguments <p>Returns: <code>JobQueue</code> - Queue instance</p> <p>Example:</p> <pre><code>from rapidai.background import get_queue\n\n# In-memory queue\nqueue = get_queue(backend=\"memory\")\n\n# Redis queue\nqueue = get_queue(\n    backend=\"redis\",\n    url=\"redis://localhost:6379\"\n)\n</code></pre>"},{"location":"reference/background/#exceptions","title":"Exceptions","text":""},{"location":"reference/background/#joberror","title":"JobError","text":"<pre><code>class JobError(RapidAIException):\n    \"\"\"Background job errors.\"\"\"\n</code></pre> <p>Raised for job-related errors.</p>"},{"location":"reference/background/#complete-example","title":"Complete Example","text":"<pre><code>from rapidai import App\nfrom rapidai.background import background, get_queue, JobStatus\n\napp = App()\n\n# Configure Redis queue for production\nqueue = get_queue(\n    backend=\"redis\",\n    url=\"redis://localhost:6379\"\n)\n\n@background(max_retries=3, queue=queue)\nasync def process_data(data_id: str):\n    \"\"\"Process data with automatic retries.\"\"\"\n    # Simulate processing\n    result = await external_api.process(data_id)\n    return {\"data_id\": data_id, \"result\": result}\n\n@app.route(\"/process\", methods=[\"POST\"])\nasync def start_processing(data_id: str):\n    \"\"\"Start background processing.\"\"\"\n    job_id = await process_data.enqueue(data_id=data_id)\n    return {\n        \"job_id\": job_id,\n        \"status\": \"queued\",\n        \"check_url\": f\"/jobs/{job_id}\"\n    }\n\n@app.route(\"/jobs/&lt;job_id&gt;\", methods=[\"GET\"])\nasync def get_job_status(job_id: str):\n    \"\"\"Get job status and result.\"\"\"\n    result = await process_data.get_result(job_id)\n\n    if not result:\n        return {\"error\": \"Job not found\"}, 404\n\n    response = {\n        \"job_id\": result.job_id,\n        \"status\": result.status.value,\n        \"created_at\": result.created_at.isoformat(),\n        \"attempts\": result.attempts\n    }\n\n    if result.is_done:\n        response[\"completed_at\"] = result.completed_at.isoformat()\n        response[\"duration\"] = result.duration\n\n        if result.status == JobStatus.COMPLETED:\n            response[\"result\"] = result.result\n        elif result.status == JobStatus.FAILED:\n            response[\"error\"] = result.error\n\n    return response\n\n@app.route(\"/jobs/&lt;job_id&gt;\", methods=[\"DELETE\"])\nasync def cancel_job(job_id: str):\n    \"\"\"Cancel a running job.\"\"\"\n    cancelled = await process_data.cancel(job_id)\n\n    if cancelled:\n        return {\"status\": \"cancelled\"}\n    else:\n        return {\"error\": \"Cannot cancel job\"}, 400\n\n@app.route(\"/jobs\", methods=[\"GET\"])\nasync def list_all_jobs(status: str = None):\n    \"\"\"List all jobs, optionally filtered by status.\"\"\"\n    job_status = JobStatus(status) if status else None\n    jobs = await queue.list_jobs(status=job_status)\n\n    return {\n        \"jobs\": [\n            {\n                \"job_id\": job.job_id,\n                \"status\": job.status.value,\n                \"created_at\": job.created_at.isoformat()\n            }\n            for job in jobs\n        ]\n    }\n</code></pre>"},{"location":"reference/background/#see-also","title":"See Also","text":"<ul> <li>Background Jobs Guide - Complete usage guide</li> <li>Monitoring - Track job performance</li> <li>Testing - Test background jobs</li> </ul>"},{"location":"reference/cache/","title":"cache","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"reference/cli/","title":"CLI API Reference","text":"<p>Complete API reference for the RapidAI CLI tool.</p>"},{"location":"reference/cli/#commands","title":"Commands","text":""},{"location":"reference/cli/#rapidai","title":"rapidai","text":"<p>Main CLI entry point.</p> <pre><code>rapidai [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--version</code> - Show the version and exit</li> <li><code>--help</code> - Show help message and exit</li> </ul> <p>Commands:</p> <ul> <li><code>new</code> - Create a new RapidAI project</li> <li><code>dev</code> - Run development server</li> <li><code>deploy</code> - Deploy to cloud platforms</li> <li><code>test</code> - Run tests</li> <li><code>docs</code> - Generate and serve documentation</li> </ul>"},{"location":"reference/cli/#new","title":"new","text":"<p>Create a new RapidAI project from a template.</p> <pre><code>rapidai new PROJECT_NAME [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Type Required Description <code>PROJECT_NAME</code> string Yes Name of the project to create <p>Options:</p> Option Short Type Default Description <code>--template</code> <code>-t</code> choice <code>chatbot</code> Template to use (chatbot|rag|agent|api) <code>--directory</code> <code>-d</code> path <code>.</code> Directory to create project in <p>Return Codes:</p> <ul> <li><code>0</code> - Success</li> <li><code>1</code> - Error (invalid name, directory exists, etc.)</li> </ul> <p>Examples:</p> <pre><code># Create chatbot project\nrapidai new my-bot\n\n# Create RAG project in specific directory\nrapidai new doc-qa -t rag -d ~/projects\n\n# Create API project\nrapidai new my-api --template api\n</code></pre> <p>Template Details:</p>"},{"location":"reference/cli/#chatbot","title":"chatbot","text":"<p>Creates a chatbot application with conversation memory.</p> <p>Generated Files:</p> <ul> <li><code>app.py</code> - Main application with chat endpoints</li> <li><code>.env</code> - Environment variable template</li> <li><code>requirements.txt</code> - Dependencies (rapidai, anthropic/openai)</li> <li><code>README.md</code> - Project documentation</li> <li><code>tests/</code> - Empty test directory</li> </ul> <p>Endpoints:</p> <ul> <li><code>POST /chat</code> - Chat with memory</li> <li><code>POST /clear</code> - Clear conversation history</li> </ul>"},{"location":"reference/cli/#rag","title":"rag","text":"<p>Creates a RAG application with document upload and Q&amp;A.</p> <p>Generated Files:</p> <ul> <li><code>app.py</code> - Main application with RAG endpoints</li> <li><code>.env</code> - Environment variable template</li> <li><code>requirements.txt</code> - Dependencies (rapidai[rag], anthropic/openai)</li> <li><code>README.md</code> - Project documentation</li> <li><code>tests/</code> - Empty test directory</li> <li><code>docs/</code> - Document storage directory</li> </ul> <p>Endpoints:</p> <ul> <li><code>POST /upload</code> - Upload documents</li> <li><code>POST /ask</code> - Ask questions</li> <li><code>POST /search</code> - Search documents</li> </ul>"},{"location":"reference/cli/#agent","title":"agent","text":"<p>Creates an AI agent with analysis and generation capabilities.</p> <p>Generated Files:</p> <ul> <li><code>app.py</code> - Main application with agent endpoints</li> <li><code>.env</code> - Environment variable template</li> <li><code>requirements.txt</code> - Dependencies (rapidai, anthropic/openai)</li> <li><code>README.md</code> - Project documentation</li> <li><code>tests/</code> - Empty test directory</li> </ul> <p>Endpoints:</p> <ul> <li><code>POST /analyze</code> - Analyze text</li> <li><code>POST /generate</code> - Generate content</li> <li><code>POST /chat</code> - Agent chat</li> </ul>"},{"location":"reference/cli/#api","title":"api","text":"<p>Creates a REST API with authentication and CORS.</p> <p>Generated Files:</p> <ul> <li><code>app.py</code> - Main application with API endpoints</li> <li><code>.env</code> - Environment variable template</li> <li><code>requirements.txt</code> - Dependencies (rapidai, anthropic/openai)</li> <li><code>README.md</code> - Project documentation</li> <li><code>tests/</code> - Empty test directory</li> </ul> <p>Endpoints:</p> <ul> <li><code>GET /</code> - API information</li> <li><code>POST /complete</code> - Complete prompts</li> <li><code>POST /chat</code> - Chat endpoint</li> <li><code>GET /health</code> - Health check</li> </ul>"},{"location":"reference/cli/#dev","title":"dev","text":"<p>Run the development server with hot reload.</p> <pre><code>rapidai dev [OPTIONS]\n</code></pre> <p>Options:</p> Option Short Type Default Description <code>--port</code> <code>-p</code> integer <code>8000</code> Port to run server on <code>--host</code> <code>-h</code> string <code>127.0.0.1</code> Host to bind server to <code>--reload</code> - flag <code>True</code> Enable auto-reload on file changes <code>--no-reload</code> - flag <code>False</code> Disable auto-reload <code>--app</code> <code>-a</code> string <code>app:app</code> Application module path <p>Return Codes:</p> <ul> <li><code>0</code> - Server stopped normally (Ctrl+C)</li> <li><code>1</code> - Error (app file not found, uvicorn not installed, etc.)</li> </ul> <p>Examples:</p> <pre><code># Run with defaults\nrapidai dev\n\n# Custom port\nrapidai dev -p 3000\n\n# Custom host (allow external connections)\nrapidai dev -h 0.0.0.0\n\n# Disable auto-reload\nrapidai dev --no-reload\n\n# Custom app module\nrapidai dev --app myapp:application\n</code></pre> <p>Behavior:</p> <ol> <li>Checks if app file exists</li> <li>Displays startup banner with URL and settings</li> <li>Starts Uvicorn server with specified options</li> <li>Watches files for changes if <code>--reload</code> is enabled</li> <li>Stops on Ctrl+C</li> </ol> <p>File Watching:</p> <p>When <code>--reload</code> is enabled, watches:</p> <ul> <li><code>*.py</code> files</li> <li>Excludes: <code>*.pyc</code>, <code>__pycache__</code>, <code>.git</code></li> </ul>"},{"location":"reference/cli/#deploy","title":"deploy","text":"<p>Deploy application to a cloud platform.</p> <pre><code>rapidai deploy PLATFORM [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Type Required Description <code>PLATFORM</code> choice Yes Cloud platform (fly|heroku|vercel|aws) <p>Options:</p> Option Short Type Default Description <code>--app-name</code> <code>-n</code> string - Application name for deployment <code>--region</code> <code>-r</code> string - Region to deploy to <p>Return Codes:</p> <ul> <li><code>0</code> - Success</li> <li><code>1</code> - Error (CLI not installed, deployment failed, etc.)</li> </ul> <p>Examples:</p> <pre><code># Deploy to Fly.io\nrapidai deploy fly\n\n# Deploy to Heroku with app name\nrapidai deploy heroku -n my-app\n\n# Deploy to Vercel in specific region\nrapidai deploy vercel -r us-east-1\n\n# AWS deployment instructions\nrapidai deploy aws\n</code></pre> <p>Platform Details:</p>"},{"location":"reference/cli/#fly","title":"fly","text":"<p>Deploy to Fly.io.</p> <p>Requirements:</p> <ul> <li><code>flyctl</code> CLI installed</li> <li>Fly.io account</li> </ul> <p>Behavior:</p> <ol> <li>Checks for <code>flyctl</code> installation</li> <li>Prompts for app name if not provided</li> <li>Generates <code>fly.toml</code> if not present</li> <li>Runs <code>flyctl deploy</code></li> <li>Displays deployment URL</li> </ol> <p>Generated Files:</p> <ul> <li><code>fly.toml</code> - Fly.io configuration</li> </ul> <p>Configuration:</p> <pre><code>app = \"app-name\"\nprimary_region = \"iad\"  # if --region specified\n\n[build]\n  builder = \"paketobuildpacks/builder:base\"\n\n[env]\n  PORT = \"8080\"\n\n[[services]]\n  internal_port = 8080\n  protocol = \"tcp\"\n</code></pre>"},{"location":"reference/cli/#heroku","title":"heroku","text":"<p>Deploy to Heroku.</p> <p>Requirements:</p> <ul> <li><code>heroku</code> CLI installed</li> <li>Heroku account</li> <li>Git repository</li> </ul> <p>Behavior:</p> <ol> <li>Checks for <code>heroku</code> CLI installation</li> <li>Prompts for app name if not provided</li> <li>Generates <code>Procfile</code> if not present</li> <li>Creates Heroku app</li> <li>Sets Python buildpack</li> <li>Displays git push instructions</li> </ol> <p>Generated Files:</p> <ul> <li><code>Procfile</code> - Heroku process configuration</li> </ul> <p>Configuration:</p> <pre><code>web: uvicorn app:app --host 0.0.0.0 --port $PORT\n</code></pre>"},{"location":"reference/cli/#vercel","title":"vercel","text":"<p>Deploy to Vercel.</p> <p>Requirements:</p> <ul> <li><code>vercel</code> CLI installed (npm)</li> <li>Vercel account</li> </ul> <p>Behavior:</p> <ol> <li>Checks for <code>vercel</code> CLI installation</li> <li>Generates <code>vercel.json</code> if not present</li> <li>Runs <code>vercel --prod</code></li> <li>Displays deployment URL</li> </ol> <p>Generated Files:</p> <ul> <li><code>vercel.json</code> - Vercel configuration</li> </ul> <p>Configuration:</p> <pre><code>{\n  \"builds\": [{\"src\": \"app.py\", \"use\": \"@vercel/python\"}],\n  \"routes\": [{\"src\": \"/(.*)\", \"dest\": \"app.py\"}]\n}\n</code></pre>"},{"location":"reference/cli/#aws","title":"aws","text":"<p>AWS deployment instructions.</p> <p>Behavior:</p> <p>Displays manual deployment guide for:</p> <ul> <li>AWS Lambda + API Gateway (serverless)</li> <li>AWS ECS/Fargate (containers)</li> <li>AWS Elastic Beanstalk (platform)</li> </ul> <p>No automatic deployment.</p>"},{"location":"reference/cli/#test","title":"test","text":"<p>Run tests for the application.</p> <pre><code>rapidai test [OPTIONS]\n</code></pre> <p>Options:</p> Option Short Type Default Description <code>--coverage</code> - flag <code>True</code> Run with coverage reporting <code>--no-coverage</code> - flag <code>False</code> Disable coverage reporting <code>--verbose</code> <code>-v</code> flag <code>False</code> Verbose output <p>Return Codes:</p> <ul> <li><code>0</code> - All tests passed</li> <li><code>1</code> - Tests failed or pytest not found</li> </ul> <p>Examples:</p> <pre><code># Run with coverage\nrapidai test\n\n# Run without coverage\nrapidai test --no-coverage\n\n# Verbose output\nrapidai test -v\n\n# Combine options\nrapidai test --no-coverage --verbose\n</code></pre> <p>Behavior:</p> <ol> <li>Checks for <code>pytest</code> installation</li> <li>Builds pytest command with options</li> <li>Runs tests</li> <li>Returns pytest exit code</li> </ol> <p>Command Generated:</p> <pre><code># With coverage (default)\npytest --cov=. --cov-report=term-missing\n\n# Without coverage\npytest\n\n# Verbose\npytest -v\n</code></pre>"},{"location":"reference/cli/#docs","title":"docs","text":"<p>Generate and serve project documentation.</p> <pre><code>rapidai docs [OPTIONS]\n</code></pre> <p>Options:</p> Option Short Type Default Description <code>--serve</code> - flag <code>True</code> Serve documentation locally <code>--build</code> - flag <code>False</code> Build for production <code>--port</code> <code>-p</code> integer <code>8001</code> Port to serve docs on <p>Return Codes:</p> <ul> <li><code>0</code> - Success</li> <li><code>1</code> - Error (mkdocs not installed, build failed, etc.)</li> </ul> <p>Examples:</p> <pre><code># Serve locally\nrapidai docs\n\n# Build for production\nrapidai docs --build\n\n# Custom port\nrapidai docs -p 3000\n</code></pre> <p>Behavior:</p> <ol> <li>Checks for <code>mkdocs</code> installation</li> <li>Creates basic docs structure if missing</li> <li>Either serves or builds documentation</li> <li>Displays URL or build location</li> </ol> <p>Serve Mode:</p> <ul> <li>Runs <code>mkdocs serve</code></li> <li>Hot reloads on changes</li> <li>Accessible at <code>http://localhost:{port}</code></li> </ul> <p>Build Mode:</p> <ul> <li>Runs <code>mkdocs build</code></li> <li>Generates static site in <code>site/</code></li> <li>Ready for deployment</li> </ul>"},{"location":"reference/cli/#configuration","title":"Configuration","text":""},{"location":"reference/cli/#environment-variables","title":"Environment Variables","text":"<p>The CLI respects these environment variables:</p> Variable Description Default <code>DEBUG</code> Enable debug mode <code>false</code> <code>RAPIDAI_*</code> Any RapidAI configuration -"},{"location":"reference/cli/#config-files","title":"Config Files","text":"<p>Generated config files:</p> <ul> <li><code>fly.toml</code> - Fly.io configuration</li> <li><code>Procfile</code> - Heroku configuration</li> <li><code>vercel.json</code> - Vercel configuration</li> </ul>"},{"location":"reference/cli/#error-handling","title":"Error Handling","text":""},{"location":"reference/cli/#common-errors","title":"Common Errors","text":"<p>\"Application file not found\"</p> <pre><code># Solution: Make sure you're in the project directory\ncd my-project\nrapidai dev\n</code></pre> <p>\"pytest not found\"</p> <pre><code># Solution: Install pytest\npip install pytest pytest-cov\n</code></pre> <p>\"mkdocs not found\"</p> <pre><code># Solution: Install mkdocs\npip install mkdocs mkdocs-material\n</code></pre> <p>\"Directory already exists\"</p> <pre><code># Solution: Choose a different name or directory\nrapidai new my-project-v2\n# or\nrm -rf my-project  # if you want to overwrite\n</code></pre>"},{"location":"reference/cli/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 Error 2 Invalid usage"},{"location":"reference/cli/#see-also","title":"See Also","text":"<ul> <li>CLI Guide - Complete usage guide with examples</li> <li>Configuration - Environment variable configuration</li> <li>Deployment - Deployment best practices</li> </ul>"},{"location":"reference/configuration/","title":"Configuration API Reference","text":"<p>Complete API reference for RapidAI's configuration system.</p>"},{"location":"reference/configuration/#configuration-classes","title":"Configuration Classes","text":""},{"location":"reference/configuration/#rapidaiconfig","title":"RapidAIConfig","text":"<pre><code>class RapidAIConfig(BaseSettings):\n    app: AppConfig\n    llm: LLMConfig\n    cache: CacheConfig\n    memory: MemoryConfig\n    rag: RAGConfig\n    prompts: PromptsConfig\n</code></pre> <p>Root configuration object that contains all subsystem configurations.</p> <p>Attributes:</p> Attribute Type Description <code>app</code> <code>AppConfig</code> Application configuration <code>llm</code> <code>LLMConfig</code> LLM provider configuration <code>cache</code> <code>CacheConfig</code> Caching configuration <code>memory</code> <code>MemoryConfig</code> Memory storage configuration <code>rag</code> <code>RAGConfig</code> RAG system configuration <code>prompts</code> <code>PromptsConfig</code> Prompt management configuration <p>Example:</p> <pre><code>from rapidai.config import get_config\n\nconfig = get_config()\nprint(config.app.title)\nprint(config.llm.provider)\n</code></pre>"},{"location":"reference/configuration/#appconfig","title":"AppConfig","text":"<pre><code>class AppConfig(BaseSettings):\n    model_config = SettingsConfigDict(env_prefix=\"RAPIDAI_APP_\", extra=\"ignore\")\n\n    title: str = \"RapidAI Application\"\n    version: str = \"0.1.0\"\n    debug: bool = False\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n</code></pre> <p>Application-level configuration.</p> <p>Environment Variables:</p> Variable Type Default Description <code>RAPIDAI_APP_TITLE</code> <code>str</code> <code>\"RapidAI Application\"</code> Application title <code>RAPIDAI_APP_VERSION</code> <code>str</code> <code>\"0.1.0\"</code> Application version <code>RAPIDAI_APP_DEBUG</code> <code>bool</code> <code>False</code> Enable debug mode <code>RAPIDAI_APP_HOST</code> <code>str</code> <code>\"0.0.0.0\"</code> Server host <code>RAPIDAI_APP_PORT</code> <code>int</code> <code>8000</code> Server port <p>Example:</p> <pre><code>export RAPIDAI_APP_TITLE=\"My AI App\"\nexport RAPIDAI_APP_DEBUG=true\nexport RAPIDAI_APP_PORT=3000\n</code></pre>"},{"location":"reference/configuration/#llmconfig","title":"LLMConfig","text":"<pre><code>class LLMConfig(BaseSettings):\n    model_config = SettingsConfigDict(env_prefix=\"RAPIDAI_LLM_\", extra=\"ignore\")\n\n    provider: str = \"anthropic\"\n    model: str = \"claude-3-haiku-20240307\"\n    api_key: Optional[str] = None\n    temperature: float = 0.7\n    max_tokens: int = 1024\n</code></pre> <p>LLM provider configuration.</p> <p>Environment Variables:</p> Variable Type Default Description <code>RAPIDAI_LLM_PROVIDER</code> <code>str</code> <code>\"anthropic\"</code> LLM provider (anthropic/openai) <code>RAPIDAI_LLM_MODEL</code> <code>str</code> <code>\"claude-3-haiku-20240307\"</code> Model name <code>RAPIDAI_LLM_API_KEY</code> <code>str</code> <code>None</code> API key <code>RAPIDAI_LLM_TEMPERATURE</code> <code>float</code> <code>0.7</code> Sampling temperature <code>RAPIDAI_LLM_MAX_TOKENS</code> <code>int</code> <code>1024</code> Maximum tokens <p>Example:</p> <pre><code>export RAPIDAI_LLM_PROVIDER=openai\nexport RAPIDAI_LLM_MODEL=gpt-4o-mini\nexport RAPIDAI_LLM_API_KEY=sk-...\nexport RAPIDAI_LLM_TEMPERATURE=0.5\n</code></pre>"},{"location":"reference/configuration/#cacheconfig","title":"CacheConfig","text":"<pre><code>class CacheConfig(BaseSettings):\n    model_config = SettingsConfigDict(env_prefix=\"RAPIDAI_CACHE_\", extra=\"ignore\")\n\n    backend: str = \"memory\"\n    ttl: int = 3600\n    redis_url: Optional[str] = None\n</code></pre> <p>Caching configuration.</p> <p>Environment Variables:</p> Variable Type Default Description <code>RAPIDAI_CACHE_BACKEND</code> <code>str</code> <code>\"memory\"</code> Cache backend (memory/redis) <code>RAPIDAI_CACHE_TTL</code> <code>int</code> <code>3600</code> Default TTL in seconds <code>RAPIDAI_CACHE_REDIS_URL</code> <code>str</code> <code>None</code> Redis connection URL <p>Example:</p> <pre><code>export RAPIDAI_CACHE_BACKEND=redis\nexport RAPIDAI_CACHE_TTL=7200\nexport RAPIDAI_CACHE_REDIS_URL=redis://localhost:6379\n</code></pre>"},{"location":"reference/configuration/#memoryconfig","title":"MemoryConfig","text":"<pre><code>class MemoryConfig(BaseSettings):\n    model_config = SettingsConfigDict(env_prefix=\"RAPIDAI_MEMORY_\", extra=\"ignore\")\n\n    backend: str = \"memory\"\n    max_messages: int = 100\n    redis_url: Optional[str] = None\n</code></pre> <p>Memory storage configuration.</p> <p>Environment Variables:</p> Variable Type Default Description <code>RAPIDAI_MEMORY_BACKEND</code> <code>str</code> <code>\"memory\"</code> Memory backend (memory/redis) <code>RAPIDAI_MEMORY_MAX_MESSAGES</code> <code>int</code> <code>100</code> Max messages per conversation <code>RAPIDAI_MEMORY_REDIS_URL</code> <code>str</code> <code>None</code> Redis connection URL <p>Example:</p> <pre><code>export RAPIDAI_MEMORY_BACKEND=redis\nexport RAPIDAI_MEMORY_MAX_MESSAGES=500\nexport RAPIDAI_MEMORY_REDIS_URL=redis://localhost:6379\n</code></pre>"},{"location":"reference/configuration/#ragconfig","title":"RAGConfig","text":"<pre><code>class RAGConfig(BaseSettings):\n    model_config = SettingsConfigDict(env_prefix=\"RAPIDAI_RAG_\", extra=\"ignore\")\n\n    embedding: EmbeddingConfig\n    chunking: ChunkingConfig\n    vectordb: VectorDBConfig\n    top_k: int = 5\n</code></pre> <p>RAG system configuration.</p> <p>Environment Variables:</p> Variable Type Default Description <code>RAPIDAI_RAG_TOP_K</code> <code>int</code> <code>5</code> Default number of results"},{"location":"reference/configuration/#embeddingconfig","title":"EmbeddingConfig","text":"<pre><code>class EmbeddingConfig(BaseSettings):\n    model_config = SettingsConfigDict(env_prefix=\"RAPIDAI_EMBEDDING_\", extra=\"ignore\")\n\n    provider: str = \"sentence-transformers\"\n    model: str = \"all-MiniLM-L6-v2\"\n    batch_size: int = 32\n    api_key: Optional[str] = None\n</code></pre> <p>Environment Variables:</p> Variable Type Default Description <code>RAPIDAI_EMBEDDING_PROVIDER</code> <code>str</code> <code>\"sentence-transformers\"</code> Embedding provider <code>RAPIDAI_EMBEDDING_MODEL</code> <code>str</code> <code>\"all-MiniLM-L6-v2\"</code> Embedding model <code>RAPIDAI_EMBEDDING_BATCH_SIZE</code> <code>int</code> <code>32</code> Batch size <code>RAPIDAI_EMBEDDING_API_KEY</code> <code>str</code> <code>None</code> API key for OpenAI embeddings"},{"location":"reference/configuration/#chunkingconfig","title":"ChunkingConfig","text":"<pre><code>class ChunkingConfig(BaseSettings):\n    model_config = SettingsConfigDict(env_prefix=\"RAPIDAI_CHUNKING_\", extra=\"ignore\")\n\n    strategy: str = \"recursive\"\n    chunk_size: int = 512\n    chunk_overlap: int = 50\n</code></pre> <p>Environment Variables:</p> Variable Type Default Description <code>RAPIDAI_CHUNKING_STRATEGY</code> <code>str</code> <code>\"recursive\"</code> Chunking strategy <code>RAPIDAI_CHUNKING_CHUNK_SIZE</code> <code>int</code> <code>512</code> Chunk size in characters <code>RAPIDAI_CHUNKING_CHUNK_OVERLAP</code> <code>int</code> <code>50</code> Overlap between chunks"},{"location":"reference/configuration/#vectordbconfig","title":"VectorDBConfig","text":"<pre><code>class VectorDBConfig(BaseSettings):\n    model_config = SettingsConfigDict(env_prefix=\"RAPIDAI_VECTORDB_\", extra=\"ignore\")\n\n    backend: str = \"chromadb\"\n    persist_directory: str = \"./chroma_data\"\n    collection_name: str = \"rapidai_docs\"\n</code></pre> <p>Environment Variables:</p> Variable Type Default Description <code>RAPIDAI_VECTORDB_BACKEND</code> <code>str</code> <code>\"chromadb\"</code> Vector DB backend <code>RAPIDAI_VECTORDB_PERSIST_DIRECTORY</code> <code>str</code> <code>\"./chroma_data\"</code> Persistence directory <code>RAPIDAI_VECTORDB_COLLECTION_NAME</code> <code>str</code> <code>\"rapidai_docs\"</code> Collection name"},{"location":"reference/configuration/#promptsconfig","title":"PromptsConfig","text":"<pre><code>class PromptsConfig(BaseSettings):\n    model_config = SettingsConfigDict(env_prefix=\"RAPIDAI_PROMPTS_\", extra=\"ignore\")\n\n    directory: str = \"./prompts\"\n    use_jinja: bool = True\n</code></pre> <p>Prompt management configuration.</p> <p>Environment Variables:</p> Variable Type Default Description <code>RAPIDAI_PROMPTS_DIRECTORY</code> <code>str</code> <code>\"./prompts\"</code> Prompts directory <code>RAPIDAI_PROMPTS_USE_JINJA</code> <code>bool</code> <code>True</code> Enable Jinja2 templates <p>Example:</p> <pre><code>export RAPIDAI_PROMPTS_DIRECTORY=./templates\nexport RAPIDAI_PROMPTS_USE_JINJA=true\n</code></pre>"},{"location":"reference/configuration/#functions","title":"Functions","text":""},{"location":"reference/configuration/#get_config","title":"get_config","text":"<pre><code>def get_config() -&gt; RapidAIConfig\n</code></pre> <p>Get or create global configuration instance.</p> <p>Returns: <code>RapidAIConfig</code> - Global configuration singleton</p> <p>Example:</p> <pre><code>from rapidai.config import get_config\n\nconfig = get_config()\nprint(config.app.title)\nprint(config.llm.model)\n</code></pre>"},{"location":"reference/configuration/#load_config","title":"load_config","text":"<pre><code>def load_config(path: str) -&gt; RapidAIConfig\n</code></pre> <p>Load configuration from file.</p> <p>Parameters:</p> <ul> <li><code>path</code> (<code>str</code>) - Path to configuration file (YAML, JSON, or TOML)</li> </ul> <p>Returns: <code>RapidAIConfig</code> - Loaded configuration</p> <p>Example:</p> <pre><code>from rapidai.config import load_config\n\nconfig = load_config(\"config.yaml\")\n</code></pre> <p>Supported formats:</p> <p>YAML: <pre><code>app:\n  title: My AI App\n  port: 3000\n\nllm:\n  provider: openai\n  model: gpt-4o-mini\n  api_key: sk-...\n</code></pre></p> <p>JSON: <pre><code>{\n  \"app\": {\n    \"title\": \"My AI App\",\n    \"port\": 3000\n  },\n  \"llm\": {\n    \"provider\": \"openai\",\n    \"model\": \"gpt-4o-mini\"\n  }\n}\n</code></pre></p>"},{"location":"reference/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>Configuration values are loaded in the following priority order (highest to lowest):</p> <ol> <li>Environment variables - <code>RAPIDAI_*</code> variables</li> <li>Configuration file - Loaded via <code>load_config()</code></li> <li>Default values - Built-in defaults</li> </ol> <p>Example:</p> <pre><code># Default\nconfig.app.port  # 8000\n\n# Override with environment variable\nos.environ[\"RAPIDAI_APP_PORT\"] = \"3000\"\nconfig = get_config()\nconfig.app.port  # 3000\n\n# Override with config file\nconfig = load_config(\"config.yaml\")  # port: 9000\nconfig.app.port  # 9000\n</code></pre>"},{"location":"reference/configuration/#complete-example","title":"Complete Example","text":"<p>config.yaml:</p> <pre><code>app:\n  title: Production AI App\n  debug: false\n  port: 8080\n\nllm:\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  temperature: 0.7\n  max_tokens: 2048\n\ncache:\n  backend: redis\n  ttl: 7200\n  redis_url: redis://localhost:6379\n\nmemory:\n  backend: redis\n  max_messages: 500\n  redis_url: redis://localhost:6379\n\nrag:\n  top_k: 10\n  embedding:\n    provider: openai\n    model: text-embedding-3-small\n  chunking:\n    chunk_size: 1024\n    chunk_overlap: 100\n  vectordb:\n    backend: chromadb\n    persist_directory: ./vector_data\n\nprompts:\n  directory: ./prompt_templates\n  use_jinja: true\n</code></pre> <p>app.py:</p> <pre><code>from rapidai import App\nfrom rapidai.config import load_config\n\n# Load configuration\nconfig = load_config(\"config.yaml\")\n\n# Create app with config\napp = App(\n    title=config.app.title,\n    debug=config.app.debug\n)\n\n@app.route(\"/config\")\nasync def get_config_info():\n    return {\n        \"app\": {\n            \"title\": config.app.title,\n            \"version\": config.app.version,\n            \"port\": config.app.port\n        },\n        \"llm\": {\n            \"provider\": config.llm.provider,\n            \"model\": config.llm.model\n        },\n        \"cache\": {\n            \"backend\": config.cache.backend,\n            \"ttl\": config.cache.ttl\n        }\n    }\n\nif __name__ == \"__main__\":\n    app.run(\n        host=config.app.host,\n        port=config.app.port\n    )\n</code></pre>"},{"location":"reference/configuration/#see-also","title":"See Also","text":"<ul> <li>CLI Reference - CLI configuration options</li> <li>Deployment Tutorial - Production configuration</li> <li>RAG Reference - RAG configuration details</li> </ul>"},{"location":"reference/llm/","title":"llm","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"reference/memory/","title":"memory","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"reference/monitoring/","title":"Monitoring API Reference","text":"<p>Complete API reference for RapidAI's monitoring and observability system.</p>"},{"location":"reference/monitoring/#decorator","title":"Decorator","text":""},{"location":"reference/monitoring/#monitor","title":"monitor","text":"<pre><code>def monitor(\n    track_tokens: bool = False,\n    track_cost: bool = False\n) -&gt; Callable\n</code></pre> <p>Decorator to monitor endpoint performance.</p> <p>Parameters:</p> Parameter Type Default Description <code>track_tokens</code> <code>bool</code> <code>False</code> Track token usage (requires LLM integration) <code>track_cost</code> <code>bool</code> <code>False</code> Calculate and track costs <p>Returns: Decorator function</p> <p>Example:</p> <pre><code>from rapidai.monitoring import monitor\n\n@app.route(\"/chat\", methods=[\"POST\"])\n@monitor(track_tokens=True, track_cost=True)\nasync def chat(message: str):\n    response = await llm.complete(message)\n    return {\n        \"response\": response,\n        \"tokens_used\": 150,\n        \"model\": \"claude-3-haiku-20240307\"\n    }\n</code></pre>"},{"location":"reference/monitoring/#classes","title":"Classes","text":""},{"location":"reference/monitoring/#metricscollector","title":"MetricsCollector","text":"<pre><code>class MetricsCollector:\n    def __init__(self) -&gt; None\n</code></pre> <p>Collects and stores metrics.</p> <p>Methods:</p>"},{"location":"reference/monitoring/#record_metric","title":"<code>record_metric</code>","text":"<pre><code>def record_metric(\n    self,\n    name: str,\n    value: float,\n    tags: Optional[Dict[str, str]] = None\n) -&gt; None\n</code></pre> <p>Record a metric.</p> <p>Parameters:</p> <ul> <li><code>name</code> (<code>str</code>) - Metric name</li> <li><code>value</code> (<code>float</code>) - Metric value</li> <li><code>tags</code> (<code>Dict[str, str]</code>, optional) - Optional tags</li> </ul> <p>Example:</p> <pre><code>collector.record_metric(\n    name=\"api.latency\",\n    value=0.523,\n    tags={\"endpoint\": \"/chat\"}\n)\n</code></pre>"},{"location":"reference/monitoring/#record_request","title":"<code>record_request</code>","text":"<pre><code>def record_request(\n    self,\n    endpoint: str,\n    method: str,\n    duration: float,\n    status_code: int,\n    tokens_used: Optional[int] = None,\n    model: Optional[str] = None,\n    error: Optional[str] = None\n) -&gt; None\n</code></pre> <p>Record request metrics.</p> <p>Parameters:</p> <ul> <li><code>endpoint</code> (<code>str</code>) - Request endpoint</li> <li><code>method</code> (<code>str</code>) - HTTP method</li> <li><code>duration</code> (<code>float</code>) - Duration in seconds</li> <li><code>status_code</code> (<code>int</code>) - HTTP status code</li> <li><code>tokens_used</code> (<code>int</code>, optional) - Total tokens used</li> <li><code>model</code> (<code>str</code>, optional) - Model name</li> <li><code>error</code> (<code>str</code>, optional) - Error message if any</li> </ul>"},{"location":"reference/monitoring/#get_metrics","title":"<code>get_metrics</code>","text":"<pre><code>def get_metrics(\n    self,\n    name: Optional[str] = None,\n    since: Optional[datetime] = None\n) -&gt; List[Metric]\n</code></pre> <p>Get recorded metrics.</p> <p>Parameters:</p> <ul> <li><code>name</code> (<code>str</code>, optional) - Optional metric name filter</li> <li><code>since</code> (<code>datetime</code>, optional) - Optional time filter</li> </ul> <p>Returns: <code>List[Metric]</code> - List of metrics</p>"},{"location":"reference/monitoring/#get_requests","title":"<code>get_requests</code>","text":"<pre><code>def get_requests(\n    self,\n    endpoint: Optional[str] = None,\n    since: Optional[datetime] = None\n) -&gt; List[RequestMetrics]\n</code></pre> <p>Get request metrics.</p> <p>Parameters:</p> <ul> <li><code>endpoint</code> (<code>str</code>, optional) - Optional endpoint filter</li> <li><code>since</code> (<code>datetime</code>, optional) - Optional time filter</li> </ul> <p>Returns: <code>List[RequestMetrics]</code> - List of request metrics</p>"},{"location":"reference/monitoring/#get_model_usage","title":"<code>get_model_usage</code>","text":"<pre><code>def get_model_usage(\n    self,\n    model: Optional[str] = None\n) -&gt; Dict[str, ModelUsage]\n</code></pre> <p>Get model usage statistics.</p> <p>Parameters:</p> <ul> <li><code>model</code> (<code>str</code>, optional) - Optional model filter</li> </ul> <p>Returns: <code>Dict[str, ModelUsage]</code> - Dictionary of model usage</p>"},{"location":"reference/monitoring/#get_summary","title":"<code>get_summary</code>","text":"<pre><code>def get_summary(self) -&gt; Dict[str, Any]\n</code></pre> <p>Get summary statistics.</p> <p>Returns: Dictionary with:</p> <pre><code>{\n    \"uptime_seconds\": float,\n    \"total_requests\": int,\n    \"successful_requests\": int,\n    \"failed_requests\": int,\n    \"success_rate\": float,\n    \"average_duration\": float,\n    \"total_tokens\": int,\n    \"total_cost\": float,\n    \"models_used\": int\n}\n</code></pre>"},{"location":"reference/monitoring/#clear","title":"<code>clear</code>","text":"<pre><code>def clear(self) -&gt; None\n</code></pre> <p>Clear all metrics.</p>"},{"location":"reference/monitoring/#metric","title":"Metric","text":"<pre><code>@dataclass\nclass Metric:\n    name: str\n    value: float\n    timestamp: datetime\n    tags: Dict[str, str]\n</code></pre> <p>Represents a metric measurement.</p> <p>Attributes:</p> Attribute Type Description <code>name</code> <code>str</code> Metric name <code>value</code> <code>float</code> Metric value <code>timestamp</code> <code>datetime</code> When metric was recorded <code>tags</code> <code>Dict[str, str]</code> Metric tags"},{"location":"reference/monitoring/#requestmetrics","title":"RequestMetrics","text":"<pre><code>@dataclass\nclass RequestMetrics:\n    endpoint: str\n    method: str\n    duration: float\n    status_code: int\n    timestamp: datetime\n    tokens_used: Optional[int] = None\n    model: Optional[str] = None\n    cost: Optional[float] = None\n    error: Optional[str] = None\n</code></pre> <p>Metrics for a single request.</p> <p>Attributes:</p> Attribute Type Description <code>endpoint</code> <code>str</code> Request endpoint <code>method</code> <code>str</code> HTTP method <code>duration</code> <code>float</code> Request duration in seconds <code>status_code</code> <code>int</code> HTTP status code <code>timestamp</code> <code>datetime</code> Request timestamp <code>tokens_used</code> <code>int</code> Total tokens used <code>model</code> <code>str</code> Model name <code>cost</code> <code>float</code> Request cost in USD <code>error</code> <code>str</code> Error message if any"},{"location":"reference/monitoring/#modelusage","title":"ModelUsage","text":"<pre><code>@dataclass\nclass ModelUsage:\n    model: str\n    total_tokens: int = 0\n    prompt_tokens: int = 0\n    completion_tokens: int = 0\n    total_cost: float = 0.0\n    request_count: int = 0\n</code></pre> <p>Token usage and cost for a model.</p> <p>Attributes:</p> Attribute Type Description <code>model</code> <code>str</code> Model name <code>total_tokens</code> <code>int</code> Total tokens used <code>prompt_tokens</code> <code>int</code> Prompt tokens <code>completion_tokens</code> <code>int</code> Completion tokens <code>total_cost</code> <code>float</code> Total cost in USD <code>request_count</code> <code>int</code> Number of requests <p>Methods:</p>"},{"location":"reference/monitoring/#add_tokens","title":"<code>add_tokens</code>","text":"<pre><code>def add_tokens(\n    self,\n    prompt_tokens: int = 0,\n    completion_tokens: int = 0,\n    cost: float = 0.0\n) -&gt; None\n</code></pre> <p>Add token usage.</p>"},{"location":"reference/monitoring/#functions","title":"Functions","text":""},{"location":"reference/monitoring/#get_collector","title":"get_collector","text":"<pre><code>def get_collector() -&gt; MetricsCollector\n</code></pre> <p>Get or create global metrics collector.</p> <p>Returns: <code>MetricsCollector</code> - Global collector instance</p> <p>Example:</p> <pre><code>from rapidai.monitoring import get_collector\n\ncollector = get_collector()\nsummary = collector.get_summary()\n</code></pre>"},{"location":"reference/monitoring/#calculate_cost","title":"calculate_cost","text":"<pre><code>def calculate_cost(\n    model: str,\n    prompt_tokens: int = 0,\n    completion_tokens: int = 0\n) -&gt; float\n</code></pre> <p>Calculate cost for model usage.</p> <p>Parameters:</p> Parameter Type Default Description <code>model</code> <code>str</code> - Model name <code>prompt_tokens</code> <code>int</code> <code>0</code> Number of prompt tokens <code>completion_tokens</code> <code>int</code> <code>0</code> Number of completion tokens <p>Returns: <code>float</code> - Cost in USD</p> <p>Example:</p> <pre><code>from rapidai.monitoring import calculate_cost\n\ncost = calculate_cost(\n    model=\"claude-3-haiku-20240307\",\n    prompt_tokens=100,\n    completion_tokens=50\n)\n# Returns: 0.0000875\n</code></pre>"},{"location":"reference/monitoring/#get_dashboard_html","title":"get_dashboard_html","text":"<pre><code>def get_dashboard_html() -&gt; str\n</code></pre> <p>Generate HTML dashboard for metrics.</p> <p>Returns: <code>str</code> - HTML string</p> <p>Example:</p> <pre><code>from rapidai.monitoring import get_dashboard_html\n\n@app.route(\"/metrics\")\nasync def dashboard():\n    return get_dashboard_html()\n</code></pre>"},{"location":"reference/monitoring/#model-pricing","title":"Model Pricing","text":"<p>Pricing per 1M tokens (prompt/completion):</p> <p>Anthropic Claude:</p> Model Prompt Completion claude-3-opus-20240229 $15.00 $75.00 claude-3-sonnet-20240229 $3.00 $15.00 claude-3-haiku-20240307 $0.25 $1.25 claude-3-5-sonnet-20241022 $3.00 $15.00 <p>OpenAI:</p> Model Prompt Completion gpt-4o $5.00 $15.00 gpt-4o-mini $0.15 $0.60 gpt-4-turbo $10.00 $30.00 gpt-3.5-turbo $0.50 $1.50"},{"location":"reference/monitoring/#complete-example","title":"Complete Example","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.monitoring import (\n    monitor,\n    get_collector,\n    get_dashboard_html,\n    calculate_cost\n)\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/chat\", methods=[\"POST\"])\n@monitor(track_tokens=True, track_cost=True)\nasync def chat(message: str):\n    \"\"\"Monitored chat endpoint.\"\"\"\n    response = await llm.complete(message)\n    return {\n        \"response\": response,\n        \"tokens_used\": 150,\n        \"model\": \"claude-3-haiku-20240307\"\n    }\n\n@app.route(\"/metrics/dashboard\")\nasync def dashboard():\n    \"\"\"Serve metrics dashboard.\"\"\"\n    return get_dashboard_html()\n\n@app.route(\"/metrics/api\")\nasync def metrics_api():\n    \"\"\"Get metrics as JSON.\"\"\"\n    collector = get_collector()\n    return {\n        \"summary\": collector.get_summary(),\n        \"models\": {\n            model: {\n                \"tokens\": usage.total_tokens,\n                \"cost\": usage.total_cost,\n                \"requests\": usage.request_count\n            }\n            for model, usage in collector.get_model_usage().items()\n        }\n    }\n\n@app.route(\"/metrics/cost\")\nasync def cost_estimate(\n    model: str,\n    prompt_tokens: int,\n    completion_tokens: int\n):\n    \"\"\"Estimate cost for token usage.\"\"\"\n    cost = calculate_cost(model, prompt_tokens, completion_tokens)\n    return {\n        \"model\": model,\n        \"prompt_tokens\": prompt_tokens,\n        \"completion_tokens\": completion_tokens,\n        \"cost\": cost,\n        \"cost_formatted\": f\"${cost:.6f}\"\n    }\n\n@app.route(\"/metrics/custom\")\nasync def custom_metric():\n    \"\"\"Record custom metric.\"\"\"\n    collector = get_collector()\n    collector.record_metric(\n        name=\"custom.event\",\n        value=1.0,\n        tags={\"type\": \"important\"}\n    )\n    return {\"recorded\": True}\n</code></pre>"},{"location":"reference/monitoring/#see-also","title":"See Also","text":"<ul> <li>Monitoring Guide - Complete usage guide</li> <li>Background Jobs - Monitor async tasks</li> <li>Testing - Test monitored endpoints</li> </ul>"},{"location":"reference/prompts/","title":"Prompts API Reference","text":"<p>Complete API reference for RapidAI's prompt management system.</p>"},{"location":"reference/prompts/#classes","title":"Classes","text":""},{"location":"reference/prompts/#promptmanager","title":"PromptManager","text":"<p>Manages prompt templates with loading, caching, and hot reloading.</p> <pre><code>class PromptManager:\n    def __init__(\n        self,\n        prompt_dir: Optional[Union[str, Path]] = None,\n        auto_reload: bool = False,\n        reload_interval: int = 5,\n    )\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>prompt_dir</code> <code>str \\| Path</code> <code>\"prompts\"</code> Directory containing prompt template files <code>auto_reload</code> <code>bool</code> <code>False</code> Enable hot reloading in development <code>reload_interval</code> <code>int</code> <code>5</code> Seconds between reload checks <p>Methods:</p>"},{"location":"reference/prompts/#load_all","title":"<code>load_all()</code>","text":"<p>Load all prompt templates from the prompt directory.</p> <pre><code>prompts.load_all()\n</code></pre> <p>Loads all <code>.txt</code> and <code>.md</code> files from <code>prompt_dir</code>.</p>"},{"location":"reference/prompts/#load_from_filefile_path","title":"<code>load_from_file(file_path)</code>","text":"<p>Load a prompt template from a file.</p> <pre><code>prompt = prompts.load_from_file(\"prompts/greeting.txt\")\n</code></pre> <p>Parameters:</p> <ul> <li><code>file_path</code> (<code>str | Path</code>) - Path to the prompt file</li> </ul> <p>Returns: <code>Prompt</code> - Loaded Prompt object</p> <p>File Format:</p> <p>Supports YAML frontmatter for metadata:</p> <pre><code>---\nversion: \"1.0\"\ndescription: \"Customer support prompt\"\n---\nHello {{ name }}, how can I help?\n</code></pre>"},{"location":"reference/prompts/#registername-template-metadata","title":"<code>register(name, template, metadata)</code>","text":"<p>Register a prompt template programmatically.</p> <pre><code>prompt = prompts.register(\n    name=\"greeting\",\n    template=\"Hello {{ name }}!\",\n    metadata={\"version\": \"1.0\"}\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>name</code> (<code>str</code>) - Prompt name</li> <li><code>template</code> (<code>str</code>) - Template string</li> <li><code>metadata</code> (<code>Dict[str, Any]</code>, optional) - Optional metadata</li> </ul> <p>Returns: <code>Prompt</code> - Created Prompt object</p>"},{"location":"reference/prompts/#getname-version","title":"<code>get(name, version)</code>","text":"<p>Get a prompt by name.</p> <pre><code>prompt = prompts.get(\"greeting\")\nversioned_prompt = prompts.get(\"greeting\", version=\"2.0\")\n</code></pre> <p>Parameters:</p> <ul> <li><code>name</code> (<code>str</code>) - Prompt name</li> <li><code>version</code> (<code>str</code>, optional) - Optional specific version to retrieve</li> </ul> <p>Returns: <code>Prompt | None</code> - Prompt object, or None if not found</p> <p>If <code>auto_reload=True</code>, checks for file changes before retrieving.</p>"},{"location":"reference/prompts/#rendername-kwargs","title":"<code>render(name, **kwargs)</code>","text":"<p>Render a prompt template by name.</p> <pre><code>result = prompts.render(\"greeting\", name=\"Alice\", count=5)\n</code></pre> <p>Parameters:</p> <ul> <li><code>name</code> (<code>str</code>) - Prompt name</li> <li><code>**kwargs</code> - Variables to substitute in the template</li> </ul> <p>Returns: <code>str</code> - Rendered prompt string</p> <p>Raises: <code>PromptError</code> - If prompt not found or rendering fails</p>"},{"location":"reference/prompts/#list_prompts","title":"<code>list_prompts()</code>","text":"<p>List all registered prompt names.</p> <pre><code>names = prompts.list_prompts()\n# [\"greeting\", \"analyze\", \"support\"]\n</code></pre> <p>Returns: <code>List[str]</code> - List of prompt names</p>"},{"location":"reference/prompts/#prompt","title":"Prompt","text":"<p>Represents a prompt template with version history.</p> <pre><code>@dataclass\nclass Prompt:\n    name: str\n    template: str\n    variables: List[str]\n    versions: List[PromptVersion]\n    metadata: Dict[str, Any]\n</code></pre> <p>Attributes:</p> Attribute Type Description <code>name</code> <code>str</code> Prompt name <code>template</code> <code>str</code> Template string <code>variables</code> <code>List[str]</code> Extracted variable names from template <code>versions</code> <code>List[PromptVersion]</code> Version history <code>metadata</code> <code>Dict[str, Any]</code> Metadata dictionary <p>Methods:</p>"},{"location":"reference/prompts/#renderkwargs","title":"<code>render(**kwargs)</code>","text":"<p>Render the template with provided variables.</p> <pre><code>prompt = prompts.get(\"greeting\")\nresult = prompt.render(name=\"Alice\", count=5)\n</code></pre> <p>Parameters:</p> <ul> <li><code>**kwargs</code> - Variables to substitute in the template</li> </ul> <p>Returns: <code>str</code> - Rendered prompt string</p> <p>Raises: <code>PromptError</code> - If required variables are missing or rendering fails</p>"},{"location":"reference/prompts/#add_versionversion-template-metadata","title":"<code>add_version(version, template, metadata)</code>","text":"<p>Add a new version of the prompt.</p> <pre><code>prompt.add_version(\n    version=\"2.0\",\n    template=\"Hi {{ name }}! What's up?\",\n    metadata={\"changelog\": \"More casual\"}\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>version</code> (<code>str</code>) - Version identifier</li> <li><code>template</code> (<code>str</code>) - New template string</li> <li><code>metadata</code> (<code>Dict[str, Any]</code>, optional) - Optional metadata for this version</li> </ul>"},{"location":"reference/prompts/#get_versionversion","title":"<code>get_version(version)</code>","text":"<p>Get a specific version of the prompt template.</p> <pre><code>template = prompt.get_version(\"2.0\")\n</code></pre> <p>Parameters:</p> <ul> <li><code>version</code> (<code>str</code>) - Version identifier</li> </ul> <p>Returns: <code>str | None</code> - Template string for that version, or None if not found</p>"},{"location":"reference/prompts/#promptversion","title":"PromptVersion","text":"<p>Represents a version of a prompt template.</p> <pre><code>@dataclass\nclass PromptVersion:\n    version: str\n    template: str\n    created_at: datetime\n    metadata: Dict[str, Any]\n</code></pre> <p>Attributes:</p> Attribute Type Description <code>version</code> <code>str</code> Version identifier <code>template</code> <code>str</code> Template string for this version <code>created_at</code> <code>datetime</code> Creation timestamp <code>metadata</code> <code>Dict[str, Any]</code> Version-specific metadata"},{"location":"reference/prompts/#functions","title":"Functions","text":""},{"location":"reference/prompts/#get_prompt_managerprompt_dir-auto_reload","title":"get_prompt_manager(prompt_dir, auto_reload)","text":"<p>Get or create the global prompt manager.</p> <pre><code>from rapidai.prompts import get_prompt_manager\n\nprompts = get_prompt_manager()\n</code></pre> <p>Parameters:</p> <ul> <li><code>prompt_dir</code> (<code>str | Path</code>, optional) - Directory containing prompt templates</li> <li><code>auto_reload</code> (<code>bool</code>, optional) - Enable hot reloading (default: based on <code>DEBUG</code> env var)</li> </ul> <p>Returns: <code>PromptManager</code> - PromptManager instance</p> <p>Auto-detects reload based on environment:</p> <pre><code># Auto-enables if DEBUG=true, DEBUG=1, or DEBUG=yes\nprompts = get_prompt_manager()\n</code></pre>"},{"location":"reference/prompts/#decorators","title":"Decorators","text":""},{"location":"reference/prompts/#promptname-template-manager","title":"@prompt(name, template, manager)","text":"<p>Decorator to load and inject prompt templates into functions.</p> <pre><code>from rapidai.prompts import prompt\n\n@prompt(name=\"greeting\")\nasync def greet(user_name: str, prompt_template: str, prompt):\n    # prompt_template and prompt are automatically injected\n    filled = prompt.render(name=user_name)\n    return await llm.complete(filled)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>name</code> <code>str</code> Function name Prompt name (defaults to function name) <code>template</code> <code>str</code> <code>None</code> Optional inline template string <code>manager</code> <code>PromptManager</code> Global manager Custom PromptManager instance <p>Injected Parameters:</p> <p>The decorator injects two parameters into the wrapped function:</p> <ul> <li><code>prompt_template</code> (<code>str</code>) - The raw template string</li> <li><code>prompt</code> (<code>Prompt</code>) - The Prompt object with <code>render()</code> method and metadata</li> </ul> <p>Usage Examples:</p> <p>With inline template:</p> <pre><code>@prompt(template=\"Hello {{ name }}!\")\nasync def greet(name: str, prompt_template: str, prompt):\n    return prompt.render(name=name)\n</code></pre> <p>With file-based prompt:</p> <pre><code>@prompt(name=\"analyze\")  # Loads from prompts/analyze.txt\nasync def analyze(text: str, prompt_template: str, prompt):\n    return prompt.render(text=text)\n</code></pre> <p>With custom manager:</p> <pre><code>custom_manager = PromptManager(prompt_dir=\"my_prompts\")\n\n@prompt(manager=custom_manager)\nasync def process(data: str, prompt_template: str, prompt):\n    return prompt.render(data=data)\n</code></pre>"},{"location":"reference/prompts/#exceptions","title":"Exceptions","text":""},{"location":"reference/prompts/#prompterror","title":"PromptError","text":"<p>Base exception for prompt-related errors.</p> <pre><code>from rapidai.prompts import PromptError\n\ntry:\n    result = prompts.render(\"missing\", name=\"test\")\nexcept PromptError as e:\n    print(f\"Error: {e}\")\n</code></pre> <p>Raised in these scenarios:</p> <ul> <li>Prompt not found</li> <li>Template rendering error</li> <li>Required variables missing</li> <li>Jinja2 not installed</li> </ul>"},{"location":"reference/prompts/#configuration","title":"Configuration","text":""},{"location":"reference/prompts/#environment-variables","title":"Environment Variables","text":"Variable Type Default Description <code>RAPIDAI_PROMPT_DIR</code> <code>str</code> <code>\"./prompts\"</code> Prompt directory path <code>DEBUG</code> <code>bool</code> <code>False</code> Enable auto-reload (accepts: <code>true</code>, <code>1</code>, <code>yes</code>) <p>Example:</p> <pre><code>export RAPIDAI_PROMPT_DIR=\"./my_prompts\"\nexport DEBUG=true\n</code></pre>"},{"location":"reference/prompts/#type-definitions","title":"Type Definitions","text":""},{"location":"reference/prompts/#template-variables","title":"Template Variables","text":"<p>Variables in Jinja2 templates are automatically extracted:</p> <pre><code>template = \"Hello {{ name }}, you have {{ count }} messages\"\nprompt = Prompt(name=\"greeting\", template=template)\n\nprint(prompt.variables)\n# [\"count\", \"name\"]  # Alphabetically sorted\n</code></pre>"},{"location":"reference/prompts/#metadata-structure","title":"Metadata Structure","text":"<pre><code>metadata = {\n    \"version\": \"1.0\",\n    \"description\": \"Customer greeting\",\n    \"author\": \"Support Team\",\n    \"tags\": [\"greeting\", \"support\"],\n    \"created\": \"2024-01-15\",\n    \"performance\": {\"satisfaction\": 0.95}\n}\n</code></pre>"},{"location":"reference/prompts/#complete-example","title":"Complete Example","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.prompts import PromptManager, prompt, PromptError\n\n# Initialize\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\nprompts = PromptManager(prompt_dir=\"prompts\", auto_reload=True)\n\n# Register programmatic prompt\nprompts.register(\n    name=\"analyze\",\n    template=\"\"\"Analyze: {{ text }}\nProvide:\n1. Summary\n2. Key points\n3. Sentiment\"\"\",\n    metadata={\"type\": \"analysis\"}\n)\n\n# File-based prompt with decorator\n@app.route(\"/greet\", methods=[\"POST\"])\n@prompt(template=\"Hello {{ name }}! How can I help?\", manager=prompts)\nasync def greet(name: str, prompt_template: str, prompt):\n    try:\n        filled = prompt.render(name=name)\n        response = await llm.complete(filled)\n        return {\n            \"response\": response,\n            \"variables\": prompt.variables,\n            \"metadata\": prompt.metadata\n        }\n    except PromptError as e:\n        return {\"error\": str(e)}, 400\n\n# Programmatic usage\n@app.route(\"/analyze\", methods=[\"POST\"])\nasync def analyze(text: str):\n    try:\n        prompt_obj = prompts.get(\"analyze\")\n        if not prompt_obj:\n            return {\"error\": \"Prompt not found\"}, 404\n\n        filled = prompt_obj.render(text=text)\n        response = await llm.complete(filled)\n\n        return {\n            \"analysis\": response,\n            \"prompt_vars\": prompt_obj.variables\n        }\n    except PromptError as e:\n        return {\"error\": str(e)}, 400\n\n# Version management\n@app.route(\"/prompt/version\", methods=[\"POST\"])\nasync def add_prompt_version(name: str, version: str, template: str):\n    prompt_obj = prompts.get(name)\n    if not prompt_obj:\n        return {\"error\": \"Prompt not found\"}, 404\n\n    prompt_obj.add_version(version, template)\n    return {\"success\": True, \"versions\": len(prompt_obj.versions)}\n\nif __name__ == \"__main__\":\n    app.run(port=8000)\n</code></pre>"},{"location":"reference/prompts/#see-also","title":"See Also","text":"<ul> <li>Prompt Management Guide - Comprehensive usage guide</li> <li>RAG API Reference - Combine prompts with retrieval</li> <li>LLM API Reference - LLM integration</li> </ul>"},{"location":"reference/rag/","title":"RAG API Reference","text":"<p>Complete API reference for RapidAI's RAG system.</p>"},{"location":"reference/rag/#classes","title":"Classes","text":""},{"location":"reference/rag/#rag","title":"RAG","text":"<p>Main orchestrator for retrieval-augmented generation.</p> <pre><code>class RAG:\n    def __init__(\n        self,\n        embedding: Optional[BaseEmbedding] = None,\n        vectordb: Optional[BaseVectorDB] = None,\n        chunker: Optional[BaseChunker] = None,\n        config: Optional[RAGConfig] = None,\n    )\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>embedding</code> <code>BaseEmbedding</code> Auto-created Embedding provider <code>vectordb</code> <code>BaseVectorDB</code> Auto-created Vector database <code>chunker</code> <code>BaseChunker</code> Auto-created Text chunker <code>config</code> <code>RAGConfig</code> <code>None</code> Configuration (loads from env if None) <p>Methods:</p>"},{"location":"reference/rag/#async-initialize","title":"<code>async initialize()</code>","text":"<p>Initialize vector database collection.</p> <pre><code>await rag.initialize()\n</code></pre>"},{"location":"reference/rag/#async-add_documentsource","title":"<code>async add_document(source)</code>","text":"<p>Add a document to the RAG system.</p> <pre><code>chunks = await rag.add_document(\"path/to/doc.pdf\")\n</code></pre> <p>Parameters:</p> <ul> <li><code>source</code> (<code>str | Path | Document</code>) - File path or Document object</li> </ul> <p>Returns: <code>List[DocumentChunk]</code> - Created chunks</p>"},{"location":"reference/rag/#async-add_documentssources","title":"<code>async add_documents(sources)</code>","text":"<p>Add multiple documents.</p> <pre><code>all_chunks = await rag.add_documents([\"doc1.pdf\", \"doc2.md\"])\n</code></pre> <p>Parameters:</p> <ul> <li><code>sources</code> (<code>List[str | Path | Document]</code>) - List of file paths or Document objects</li> </ul> <p>Returns: <code>List[List[DocumentChunk]]</code> - List of chunk lists</p>"},{"location":"reference/rag/#async-retrievequery-top_k-filter_metadata","title":"<code>async retrieve(query, top_k, filter_metadata)</code>","text":"<p>Retrieve relevant chunks for a query.</p> <pre><code>result = await rag.retrieve(\"query\", top_k=5)\n</code></pre> <p>Parameters:</p> <ul> <li><code>query</code> (<code>str</code>) - Search query</li> <li><code>top_k</code> (<code>Optional[int]</code>) - Number of results (default from config)</li> <li><code>filter_metadata</code> (<code>Optional[Dict[str, Any]]</code>) - Metadata filters</li> </ul> <p>Returns: <code>RetrievalResult</code> - Result with text and sources</p>"},{"location":"reference/rag/#async-queryquery-llm-system_prompt-top_k","title":"<code>async query(query, llm, system_prompt, top_k)</code>","text":"<p>Full RAG query: retrieve + generate.</p> <pre><code>answer = await rag.query(\"question\", llm=llm)\n</code></pre> <p>Parameters:</p> <ul> <li><code>query</code> (<code>str</code>) - User query</li> <li><code>llm</code> (<code>BaseLLM</code>) - LLM instance for generation</li> <li><code>system_prompt</code> (<code>Optional[str]</code>) - System prompt</li> <li><code>top_k</code> (<code>Optional[int]</code>) - Number of chunks to retrieve</li> </ul> <p>Returns: <code>str</code> - Generated response</p>"},{"location":"reference/rag/#embedding","title":"Embedding","text":"<p>Factory function for creating embedding providers.</p> <pre><code>def Embedding(\n    provider: str = \"sentence-transformers\",\n    **kwargs\n) -&gt; BaseEmbedding\n</code></pre> <p>Parameters:</p> <ul> <li><code>provider</code> (<code>str</code>) - Provider name (\"sentence-transformers\", \"openai\", \"mock\")</li> <li><code>**kwargs</code> - Additional parameters for provider</li> </ul> <p>Returns: <code>BaseEmbedding</code> - Embedding instance</p> <p>Example:</p> <pre><code># Sentence transformers\nembedding = Embedding()\n\n# OpenAI\nembedding = Embedding(provider=\"openai\", model=\"text-embedding-3-large\")\n</code></pre>"},{"location":"reference/rag/#documentloader","title":"DocumentLoader","text":"<p>Factory function for creating document loaders.</p> <pre><code>def DocumentLoader(source: Union[str, Path]) -&gt; BaseDocumentLoader\n</code></pre> <p>Auto-detects file type from extension and returns appropriate loader.</p> <p>Supported extensions: <code>.pdf</code>, <code>.docx</code>, <code>.txt</code>, <code>.md</code>, <code>.markdown</code>, <code>.html</code>, <code>.htm</code></p> <p>Example:</p> <pre><code>loader = DocumentLoader(\"document.pdf\")\ndoc = await loader.load(\"document.pdf\")\n</code></pre>"},{"location":"reference/rag/#chunker","title":"Chunker","text":"<p>Factory function for creating chunkers.</p> <pre><code>def Chunker(\n    strategy: str = \"recursive\",\n    **kwargs\n) -&gt; BaseChunker\n</code></pre> <p>Parameters:</p> <ul> <li><code>strategy</code> (<code>str</code>) - Chunking strategy (\"recursive\", \"sentence\")</li> <li><code>**kwargs</code> - Additional parameters for chunker</li> </ul> <p>Strategies:</p> <ul> <li><code>recursive</code> - Hierarchical splitting with multiple separators</li> <li><code>sentence</code> - Sentence-based splitting</li> </ul> <p>Example:</p> <pre><code># Recursive\nchunker = Chunker()\n\n# Sentence-based\nchunker = Chunker(strategy=\"sentence\", chunk_size=512, chunk_overlap=2)\n</code></pre>"},{"location":"reference/rag/#vectordb","title":"VectorDB","text":"<p>Factory function for creating vector databases.</p> <pre><code>def VectorDB(\n    backend: str = \"chromadb\",\n    **kwargs\n) -&gt; BaseVectorDB\n</code></pre> <p>Parameters:</p> <ul> <li><code>backend</code> (<code>str</code>) - Backend name (\"chromadb\", \"mock\")</li> <li><code>**kwargs</code> - Additional parameters for backend</li> </ul> <p>Example:</p> <pre><code>vectordb = VectorDB(backend=\"chromadb\", persist_directory=\"./my_data\")\n</code></pre>"},{"location":"reference/rag/#decorators","title":"Decorators","text":""},{"location":"reference/rag/#rag_1","title":"@rag()","text":"<p>Decorator for adding RAG to routes.</p> <pre><code>def rag(\n    sources: Optional[List[Union[str, Path]]] = None,\n    top_k: int = 5,\n    collection_name: Optional[str] = None,\n    auto_initialize: bool = True,\n) -&gt; Callable\n</code></pre> <p>Parameters:</p> <ul> <li><code>sources</code> - Documents to load at startup</li> <li><code>top_k</code> - Number of chunks to retrieve</li> <li><code>collection_name</code> - Vector DB collection name</li> <li><code>auto_initialize</code> - Auto-initialize RAG system</li> </ul> <p>Example:</p> <pre><code>@app.route(\"/ask\", methods=[\"POST\"])\n@rag(sources=[\"docs/manual.pdf\"], top_k=5)\nasync def ask(query: str, rag_context):\n    # rag_context is automatically injected\n    pass\n</code></pre>"},{"location":"reference/rag/#configuration","title":"Configuration","text":""},{"location":"reference/rag/#ragconfig","title":"RAGConfig","text":"<pre><code>class RAGConfig(BaseSettings):\n    embedding: EmbeddingConfig\n    chunking: ChunkingConfig\n    vectordb: VectorDBConfig\n    top_k: int = 5\n    enable_caching: bool = True\n    cache_ttl: int = 3600\n</code></pre> <p>Environment Variables:</p> <ul> <li><code>RAPIDAI_RAG_TOP_K</code> - Default top_k value</li> </ul>"},{"location":"reference/rag/#embeddingconfig","title":"EmbeddingConfig","text":"<pre><code>class EmbeddingConfig(BaseSettings):\n    provider: str = \"sentence-transformers\"\n    model: str = \"all-MiniLM-L6-v2\"\n    batch_size: int = 32\n    api_key: Optional[str] = None\n</code></pre> <p>Environment Variables:</p> <ul> <li><code>RAPIDAI_EMBEDDING_PROVIDER</code> - Embedding provider</li> <li><code>RAPIDAI_EMBEDDING_MODEL</code> - Model name</li> <li><code>RAPIDAI_EMBEDDING_BATCH_SIZE</code> - Batch size</li> <li><code>RAPIDAI_EMBEDDING_API_KEY</code> - API key (for OpenAI)</li> </ul>"},{"location":"reference/rag/#chunkingconfig","title":"ChunkingConfig","text":"<pre><code>class ChunkingConfig(BaseSettings):\n    strategy: str = \"recursive\"\n    chunk_size: int = 512\n    chunk_overlap: int = 50\n    separator: str = \"\\n\\n\"\n</code></pre> <p>Environment Variables:</p> <ul> <li><code>RAPIDAI_CHUNKING_STRATEGY</code> - Chunking strategy</li> <li><code>RAPIDAI_CHUNKING_CHUNK_SIZE</code> - Chunk size</li> <li><code>RAPIDAI_CHUNKING_CHUNK_OVERLAP</code> - Chunk overlap</li> <li><code>RAPIDAI_CHUNKING_SEPARATOR</code> - Separator</li> </ul>"},{"location":"reference/rag/#vectordbconfig","title":"VectorDBConfig","text":"<pre><code>class VectorDBConfig(BaseSettings):\n    backend: str = \"chromadb\"\n    persist_directory: str = \"./chroma_data\"\n    collection_name: str = \"rapidai_docs\"\n</code></pre> <p>Environment Variables:</p> <ul> <li><code>RAPIDAI_VECTORDB_BACKEND</code> - Vector DB backend</li> <li><code>RAPIDAI_VECTORDB_PERSIST_DIRECTORY</code> - Persist directory</li> <li><code>RAPIDAI_VECTORDB_COLLECTION_NAME</code> - Collection name</li> </ul>"},{"location":"reference/rag/#types","title":"Types","text":"<p>All RAG-related types are defined in <code>rapidai.types</code>:</p>"},{"location":"reference/rag/#document","title":"Document","text":"<pre><code>@dataclass\nclass Document:\n    content: str\n    metadata: Dict[str, Any]\n    chunks: Optional[List[DocumentChunk]] = None\n</code></pre>"},{"location":"reference/rag/#documentchunk","title":"DocumentChunk","text":"<pre><code>@dataclass\nclass DocumentChunk:\n    content: str\n    metadata: Dict[str, Any]\n    embedding: Optional[List[float]] = None\n</code></pre>"},{"location":"reference/rag/#retrievalresult","title":"RetrievalResult","text":"<pre><code>@dataclass\nclass RetrievalResult:\n    text: str\n    sources: List[DocumentChunk]\n    score: Optional[float] = None\n</code></pre>"},{"location":"reference/rag/#base-classes","title":"Base Classes","text":"<p>For implementing custom components:</p>"},{"location":"reference/rag/#baseembedding","title":"BaseEmbedding","text":"<pre><code>class BaseEmbedding(ABC):\n    @abstractmethod\n    async def embed_text(self, text: str) -&gt; List[float]:\n        pass\n\n    @abstractmethod\n    async def embed_batch(self, texts: List[str]) -&gt; List[List[float]]:\n        pass\n\n    @property\n    @abstractmethod\n    def dimension(self) -&gt; int:\n        pass\n</code></pre>"},{"location":"reference/rag/#basedocumentloader","title":"BaseDocumentLoader","text":"<pre><code>class BaseDocumentLoader(ABC):\n    @abstractmethod\n    async def load(self, source: Union[str, Path]) -&gt; Document:\n        pass\n\n    async def load_batch(self, sources: List[Union[str, Path]]) -&gt; List[Document]:\n        pass\n</code></pre>"},{"location":"reference/rag/#basechunker","title":"BaseChunker","text":"<pre><code>class BaseChunker(ABC):\n    @abstractmethod\n    def chunk(self, document: Document) -&gt; List[DocumentChunk]:\n        pass\n</code></pre>"},{"location":"reference/rag/#basevectordb","title":"BaseVectorDB","text":"<pre><code>class BaseVectorDB(ABC):\n    @abstractmethod\n    async def create_collection(self, name: str, dimension: int, **kwargs) -&gt; None:\n        pass\n\n    @abstractmethod\n    async def add_chunks(self, collection: str, chunks: List[DocumentChunk]) -&gt; None:\n        pass\n\n    @abstractmethod\n    async def search(\n        self,\n        collection: str,\n        query_embedding: List[float],\n        top_k: int = 5,\n        filter_metadata: Optional[Dict[str, Any]] = None,\n    ) -&gt; List[DocumentChunk]:\n        pass\n\n    @abstractmethod\n    async def delete_collection(self, name: str) -&gt; None:\n        pass\n</code></pre>"},{"location":"reference/rag/#see-also","title":"See Also","text":"<ul> <li>RAG Guide</li> <li>App Reference</li> <li>LLM Reference</li> </ul>"},{"location":"reference/streaming/","title":"streaming","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"reference/testing/","title":"Testing API Reference","text":"<p>Complete API reference for RapidAI's testing utilities.</p>"},{"location":"reference/testing/#classes","title":"Classes","text":""},{"location":"reference/testing/#testclient","title":"TestClient","text":"<pre><code>class TestClient:\n    def __init__(self, app: App) -&gt; None\n</code></pre> <p>Test client for making HTTP requests to RapidAI applications without running a server.</p> <p>Parameters:</p> <ul> <li><code>app</code> (<code>App</code>) - RapidAI application instance to test</li> </ul> <p>Example:</p> <pre><code>from rapidai import App\nfrom rapidai.testing import TestClient\n\napp = App()\n\n@app.route(\"/hello\")\nasync def hello():\n    return {\"message\": \"Hello!\"}\n\nclient = TestClient(app)\nresponse = client.get(\"/hello\")\n</code></pre>"},{"location":"reference/testing/#methods","title":"Methods","text":""},{"location":"reference/testing/#get","title":"get","text":"<pre><code>def get(\n    self,\n    path: str,\n    params: Optional[Dict[str, Any]] = None,\n    headers: Optional[Dict[str, str]] = None,\n) -&gt; TestResponse\n</code></pre> <p>Send GET request to application.</p> <p>Parameters:</p> <ul> <li><code>path</code> (<code>str</code>) - Request path</li> <li><code>params</code> (<code>Dict[str, Any]</code>, optional) - Query parameters</li> <li><code>headers</code> (<code>Dict[str, str]</code>, optional) - Request headers</li> </ul> <p>Returns: <code>TestResponse</code> - Response object</p> <p>Example:</p> <pre><code># Simple GET\nresponse = client.get(\"/users\")\n\n# With query params\nresponse = client.get(\"/search\", params={\"q\": \"test\", \"limit\": 10})\n\n# With headers\nresponse = client.get(\"/protected\", headers={\"Authorization\": \"Bearer token\"})\n</code></pre>"},{"location":"reference/testing/#post","title":"post","text":"<pre><code>def post(\n    self,\n    path: str,\n    json: Optional[Dict[str, Any]] = None,\n    data: Optional[Dict[str, Any]] = None,\n    headers: Optional[Dict[str, str]] = None,\n) -&gt; TestResponse\n</code></pre> <p>Send POST request to application.</p> <p>Parameters:</p> <ul> <li><code>path</code> (<code>str</code>) - Request path</li> <li><code>json</code> (<code>Dict[str, Any]</code>, optional) - JSON body</li> <li><code>data</code> (<code>Dict[str, Any]</code>, optional) - Form data</li> <li><code>headers</code> (<code>Dict[str, str]</code>, optional) - Request headers</li> </ul> <p>Returns: <code>TestResponse</code> - Response object</p> <p>Example:</p> <pre><code># JSON body\nresponse = client.post(\"/users\", json={\"name\": \"Alice\", \"email\": \"alice@example.com\"})\n\n# Form data\nresponse = client.post(\"/upload\", data={\"file\": \"data\"})\n\n# With headers\nresponse = client.post(\"/api/data\", json={\"key\": \"value\"}, headers={\"Content-Type\": \"application/json\"})\n</code></pre>"},{"location":"reference/testing/#put","title":"put","text":"<pre><code>def put(\n    self,\n    path: str,\n    json: Optional[Dict[str, Any]] = None,\n    headers: Optional[Dict[str, str]] = None,\n) -&gt; TestResponse\n</code></pre> <p>Send PUT request to application.</p> <p>Parameters:</p> <ul> <li><code>path</code> (<code>str</code>) - Request path</li> <li><code>json</code> (<code>Dict[str, Any]</code>, optional) - JSON body</li> <li><code>headers</code> (<code>Dict[str, str]</code>, optional) - Request headers</li> </ul> <p>Returns: <code>TestResponse</code> - Response object</p> <p>Example:</p> <pre><code>response = client.put(\"/users/123\", json={\"name\": \"Bob\"})\n</code></pre>"},{"location":"reference/testing/#delete","title":"delete","text":"<pre><code>def delete(\n    self,\n    path: str,\n    headers: Optional[Dict[str, str]] = None,\n) -&gt; TestResponse\n</code></pre> <p>Send DELETE request to application.</p> <p>Parameters:</p> <ul> <li><code>path</code> (<code>str</code>) - Request path</li> <li><code>headers</code> (<code>Dict[str, str]</code>, optional) - Request headers</li> </ul> <p>Returns: <code>TestResponse</code> - Response object</p> <p>Example:</p> <pre><code>response = client.delete(\"/users/123\")\nresponse = client.delete(\"/sessions/abc\", headers={\"Authorization\": \"Bearer token\"})\n</code></pre>"},{"location":"reference/testing/#testresponse","title":"TestResponse","text":"<pre><code>class TestResponse:\n    status_code: int\n    headers: Dict[str, str]\n</code></pre> <p>Response object from test client requests.</p> <p>Attributes:</p> Attribute Type Description <code>status_code</code> <code>int</code> HTTP status code <code>headers</code> <code>Dict[str, str]</code> Response headers <p>Methods:</p>"},{"location":"reference/testing/#json","title":"json","text":"<pre><code>def json(self) -&gt; Any\n</code></pre> <p>Get JSON response body.</p> <p>Returns: Parsed JSON data</p> <p>Example:</p> <pre><code>response = client.get(\"/api/data\")\ndata = response.json()\nassert data[\"key\"] == \"value\"\n</code></pre>"},{"location":"reference/testing/#text","title":"text","text":"<pre><code>@property\ndef text(self) -&gt; str\n</code></pre> <p>Get text response body.</p> <p>Returns: <code>str</code> - Response as string</p> <p>Example:</p> <pre><code>response = client.get(\"/html\")\nhtml = response.text\nassert \"&lt;h1&gt;\" in html\n</code></pre>"},{"location":"reference/testing/#mockllm","title":"MockLLM","text":"<pre><code>class MockLLM(BaseLLM):\n    def __init__(\n        self,\n        response: str = \"Mock response\",\n        model: str = \"mock-model\"\n    ) -&gt; None\n</code></pre> <p>Mock LLM for testing without API calls.</p> <p>Parameters:</p> Parameter Type Default Description <code>response</code> <code>str</code> <code>\"Mock response\"</code> Default response to return <code>model</code> <code>str</code> <code>\"mock-model\"</code> Model name <p>Attributes:</p> <ul> <li><code>calls</code> (<code>list</code>) - List of all method calls: <code>[(method, args, kwargs), ...]</code></li> <li><code>default_response</code> (<code>str</code>) - Default response text</li> </ul> <p>Example:</p> <pre><code>from rapidai.testing import MockLLM\n\nllm = MockLLM(response=\"Test response\", model=\"test-model\")\nresult = await llm.complete(\"prompt\")\nassert result == \"Test response\"\n\n# Verify calls\nassert len(llm.calls) == 1\nmethod, prompt, kwargs = llm.calls[0]\nassert method == \"complete\"\nassert prompt == \"prompt\"\n</code></pre>"},{"location":"reference/testing/#methods_1","title":"Methods","text":""},{"location":"reference/testing/#chat","title":"chat","text":"<pre><code>async def chat(self, messages: list, **kwargs: Any) -&gt; str\n</code></pre> <p>Mock chat method.</p> <p>Parameters:</p> <ul> <li><code>messages</code> (<code>list</code>) - Chat messages</li> <li><code>**kwargs</code> - Additional arguments</li> </ul> <p>Returns: <code>str</code> - Default response</p> <p>Tracking: Adds <code>(\"chat\", messages, kwargs)</code> to <code>calls</code></p> <p>Example:</p> <pre><code>llm = MockLLM(response=\"Hello!\")\nresult = await llm.chat([{\"role\": \"user\", \"content\": \"Hi\"}])\nassert result == \"Hello!\"\nassert llm.calls[0][0] == \"chat\"\n</code></pre>"},{"location":"reference/testing/#complete","title":"complete","text":"<pre><code>async def complete(self, prompt: str, **kwargs: Any) -&gt; str\n</code></pre> <p>Mock complete method.</p> <p>Parameters:</p> <ul> <li><code>prompt</code> (<code>str</code>) - Prompt text</li> <li><code>**kwargs</code> - Additional arguments</li> </ul> <p>Returns: <code>str</code> - Default response</p> <p>Tracking: Adds <code>(\"complete\", prompt, kwargs)</code> to <code>calls</code></p> <p>Example:</p> <pre><code>llm = MockLLM(response=\"Completed\")\nresult = await llm.complete(\"test prompt\")\nassert result == \"Completed\"\n</code></pre>"},{"location":"reference/testing/#stream","title":"stream","text":"<pre><code>async def stream(self, prompt: str, **kwargs: Any)\n</code></pre> <p>Mock stream method.</p> <p>Parameters:</p> <ul> <li><code>prompt</code> (<code>str</code>) - Prompt text</li> <li><code>**kwargs</code> - Additional arguments</li> </ul> <p>Yields: Individual characters from default response</p> <p>Tracking: Adds <code>(\"stream\", prompt, kwargs)</code> to <code>calls</code></p> <p>Example:</p> <pre><code>llm = MockLLM(response=\"ABC\")\nchunks = []\nasync for chunk in llm.stream(\"prompt\"):\n    chunks.append(chunk)\nassert \"\".join(chunks) == \"ABC\"\n</code></pre>"},{"location":"reference/testing/#embed","title":"embed","text":"<pre><code>async def embed(self, text: str) -&gt; list\n</code></pre> <p>Mock embed method.</p> <p>Parameters:</p> <ul> <li><code>text</code> (<code>str</code>) - Text to embed</li> </ul> <p>Returns: <code>list</code> - Fake embedding vector (384 dimensions, all 0.1)</p> <p>Tracking: Adds <code>(\"embed\", text)</code> to <code>calls</code></p> <p>Example:</p> <pre><code>llm = MockLLM()\nembedding = await llm.embed(\"text\")\nassert len(embedding) == 384\nassert all(v == 0.1 for v in embedding)\n</code></pre>"},{"location":"reference/testing/#reset","title":"reset","text":"<pre><code>def reset(self) -&gt; None\n</code></pre> <p>Reset call history.</p> <p>Example:</p> <pre><code>llm = MockLLM()\nawait llm.complete(\"test\")\nassert len(llm.calls) == 1\n\nllm.reset()\nassert len(llm.calls) == 0\n</code></pre>"},{"location":"reference/testing/#mockmemory","title":"MockMemory","text":"<pre><code>class MockMemory(ConversationMemory):\n    def __init__(self) -&gt; None\n</code></pre> <p>Mock conversation memory for testing.</p> <p>Attributes:</p> <ul> <li><code>calls</code> (<code>list</code>) - List of all method calls: <code>[(method, args...), ...]</code></li> </ul> <p>Example:</p> <pre><code>from rapidai.testing import MockMemory\n\nmemory = MockMemory()\nmemory.add_message(\"user1\", \"user\", \"Hello\")\n\nassert len(memory.calls) == 1\nmethod, user_id, role, content = memory.calls[0]\nassert method == \"add_message\"\n</code></pre>"},{"location":"reference/testing/#methods_2","title":"Methods","text":""},{"location":"reference/testing/#add_message","title":"add_message","text":"<pre><code>def add_message(self, user_id: str, role: str, content: str) -&gt; None\n</code></pre> <p>Add message to memory.</p> <p>Parameters:</p> <ul> <li><code>user_id</code> (<code>str</code>) - User identifier</li> <li><code>role</code> (<code>str</code>) - Message role (user/assistant)</li> <li><code>content</code> (<code>str</code>) - Message content</li> </ul> <p>Tracking: Adds <code>(\"add_message\", user_id, role, content)</code> to <code>calls</code></p> <p>Example:</p> <pre><code>memory = MockMemory()\nmemory.add_message(\"user1\", \"user\", \"Hello\")\n\n# Verify call\nassert memory.calls[0] == (\"add_message\", \"user1\", \"user\", \"Hello\")\n\n# Check storage\nhistory = memory.get_history(\"user1\")\nassert len(history) == 1\n</code></pre>"},{"location":"reference/testing/#get_history","title":"get_history","text":"<pre><code>def get_history(self, user_id: str, limit: Optional[int] = None) -&gt; list\n</code></pre> <p>Get conversation history.</p> <p>Parameters:</p> <ul> <li><code>user_id</code> (<code>str</code>) - User identifier</li> <li><code>limit</code> (<code>int</code>, optional) - Maximum messages to return</li> </ul> <p>Returns: <code>list</code> - Message history</p> <p>Tracking: Adds <code>(\"get_history\", user_id, limit)</code> to <code>calls</code></p> <p>Example:</p> <pre><code>memory = MockMemory()\nmemory.add_message(\"user1\", \"user\", \"Hi\")\nmemory.add_message(\"user1\", \"assistant\", \"Hello\")\n\nhistory = memory.get_history(\"user1\", limit=10)\nassert len(history) == 2\nassert memory.calls[-1] == (\"get_history\", \"user1\", 10)\n</code></pre>"},{"location":"reference/testing/#clear","title":"clear","text":"<pre><code>def clear(self, user_id: str) -&gt; None\n</code></pre> <p>Clear conversation history.</p> <p>Parameters:</p> <ul> <li><code>user_id</code> (<code>str</code>) - User identifier</li> </ul> <p>Tracking: Adds <code>(\"clear\", user_id)</code> to <code>calls</code></p> <p>Example:</p> <pre><code>memory = MockMemory()\nmemory.add_message(\"user1\", \"user\", \"Hi\")\nmemory.clear(\"user1\")\n\nassert memory.calls[-1] == (\"clear\", \"user1\")\nhistory = memory.get_history(\"user1\")\nassert len(history) == 0\n</code></pre>"},{"location":"reference/testing/#reset_1","title":"reset","text":"<pre><code>def reset(self) -&gt; None\n</code></pre> <p>Reset call history (does not clear stored messages).</p> <p>Example:</p> <pre><code>memory = MockMemory()\nmemory.add_message(\"user1\", \"user\", \"Hi\")\n\nmemory.reset()\nassert len(memory.calls) == 0  # Calls cleared\n\n# Messages still exist\nhistory = memory.get_history(\"user1\")\nassert len(history) == 1\n</code></pre>"},{"location":"reference/testing/#functions","title":"Functions","text":""},{"location":"reference/testing/#create_mock_app","title":"create_mock_app","text":"<pre><code>def create_mock_app() -&gt; App\n</code></pre> <p>Create a mock application with basic routes for testing.</p> <p>Returns: <code>App</code> - Mock application with <code>/health</code> and <code>/echo</code> endpoints</p> <p>Routes:</p> <ul> <li><code>GET /health</code> - Returns <code>{\"status\": \"healthy\"}</code></li> <li><code>POST /echo</code> - Returns <code>{\"echo\": message}</code></li> </ul> <p>Example:</p> <pre><code>from rapidai.testing import create_mock_app, TestClient\n\napp = create_mock_app()\nclient = TestClient(app)\n\n# Test health endpoint\nresponse = client.get(\"/health\")\nassert response.json() == {\"status\": \"healthy\"}\n\n# Test echo endpoint\nresponse = client.post(\"/echo\", json={\"message\": \"test\"})\nassert response.json() == {\"echo\": \"test\"}\n</code></pre>"},{"location":"reference/testing/#pytest_fixtures","title":"pytest_fixtures","text":"<pre><code>def pytest_fixtures() -&gt; dict\n</code></pre> <p>Get pytest fixtures for RapidAI testing.</p> <p>Returns: <code>dict</code> - Dictionary of pytest fixtures</p> <p>Fixtures:</p> <ul> <li><code>mock_llm</code> - MockLLM instance</li> <li><code>mock_memory</code> - MockMemory instance</li> <li><code>test_app</code> - Mock app instance</li> <li><code>test_client</code> - TestClient instance</li> </ul> <p>Usage:</p> <p>Add to <code>conftest.py</code>:</p> <pre><code>from rapidai.testing import pytest_fixtures\n\n# Register fixtures\nfixtures = pytest_fixtures()\n</code></pre> <p>Or define manually:</p> <pre><code>import pytest\nfrom rapidai.testing import MockLLM, MockMemory, create_mock_app, TestClient\n\n@pytest.fixture\ndef mock_llm():\n    return MockLLM()\n\n@pytest.fixture\ndef mock_memory():\n    return MockMemory()\n\n@pytest.fixture\ndef test_app():\n    return create_mock_app()\n\n@pytest.fixture\ndef test_client(test_app):\n    return TestClient(test_app)\n</code></pre>"},{"location":"reference/testing/#complete-testing-example","title":"Complete Testing Example","text":"<pre><code># app.py\nfrom rapidai import App, LLM\nfrom rapidai.memory import ConversationMemory\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\nmemory = ConversationMemory()\n\n@app.route(\"/health\")\nasync def health():\n    return {\"status\": \"healthy\"}\n\n@app.route(\"/chat\", methods=[\"POST\"])\nasync def chat(user_id: str, message: str):\n    memory.add_message(user_id, \"user\", message)\n    history = memory.get_history(user_id)\n    response = await llm.chat(history)\n    memory.add_message(user_id, \"assistant\", response)\n    return {\"response\": response}\n\n# test_app.py\nimport pytest\nfrom rapidai.testing import TestClient, MockLLM, MockMemory\nfrom app import app\n\n@pytest.fixture\ndef client():\n    \"\"\"Test client fixture.\"\"\"\n    return TestClient(app)\n\n@pytest.fixture\ndef mock_llm():\n    \"\"\"Mock LLM fixture.\"\"\"\n    return MockLLM(response=\"Test response\")\n\n@pytest.fixture\ndef mock_memory():\n    \"\"\"Mock memory fixture.\"\"\"\n    return MockMemory()\n\ndef test_health_endpoint(client):\n    \"\"\"Test health check.\"\"\"\n    response = client.get(\"/health\")\n\n    assert response.status_code == 200\n    assert response.json() == {\"status\": \"healthy\"}\n\ndef test_chat_endpoint(client):\n    \"\"\"Test chat endpoint.\"\"\"\n    response = client.post(\n        \"/chat\",\n        json={\"user_id\": \"test123\", \"message\": \"Hello\"}\n    )\n\n    assert response.status_code == 200\n    assert \"response\" in response.json()\n\ndef test_mock_llm_tracking(mock_llm):\n    \"\"\"Test mock LLM call tracking.\"\"\"\n    # Make calls\n    await mock_llm.complete(\"first\")\n    await mock_llm.complete(\"second\")\n\n    # Verify tracking\n    assert len(mock_llm.calls) == 2\n    assert mock_llm.calls[0][1] == \"first\"\n    assert mock_llm.calls[1][1] == \"second\"\n\n    # Reset\n    mock_llm.reset()\n    assert len(mock_llm.calls) == 0\n\ndef test_mock_memory_tracking(mock_memory):\n    \"\"\"Test mock memory call tracking.\"\"\"\n    # Use memory\n    mock_memory.add_message(\"u1\", \"user\", \"Hi\")\n    mock_memory.get_history(\"u1\")\n    mock_memory.clear(\"u1\")\n\n    # Verify all calls tracked\n    assert len(mock_memory.calls) == 3\n    assert mock_memory.calls[0][0] == \"add_message\"\n    assert mock_memory.calls[1][0] == \"get_history\"\n    assert mock_memory.calls[2][0] == \"clear\"\n\n@pytest.mark.asyncio\nasync def test_async_operations(mock_llm):\n    \"\"\"Test async mock operations.\"\"\"\n    # Complete\n    result = await mock_llm.complete(\"test\")\n    assert result == mock_llm.default_response\n\n    # Stream\n    chunks = []\n    async for chunk in mock_llm.stream(\"test\"):\n        chunks.append(chunk)\n    assert \"\".join(chunks) == mock_llm.default_response\n\n    # Embed\n    embedding = await mock_llm.embed(\"text\")\n    assert len(embedding) == 384\n</code></pre>"},{"location":"reference/testing/#best-practices","title":"Best Practices","text":""},{"location":"reference/testing/#1-always-use-mocks","title":"1. Always Use Mocks","text":"<pre><code># \u2705 Good\nfrom rapidai.testing import MockLLM\n\nllm = MockLLM(response=\"Test\")\nresult = await llm.complete(\"prompt\")\n\n# \u274c Avoid\nfrom rapidai import LLM\n\nllm = LLM(\"claude-3-haiku-20240307\")\nresult = await llm.complete(\"prompt\")  # Real API call!\n</code></pre>"},{"location":"reference/testing/#2-track-and-verify-calls","title":"2. Track and Verify Calls","text":"<pre><code>llm = MockLLM()\nawait llm.complete(\"test\")\n\n# Verify the call was made\nassert len(llm.calls) == 1\nmethod, prompt, kwargs = llm.calls[0]\nassert method == \"complete\"\nassert prompt == \"test\"\n</code></pre>"},{"location":"reference/testing/#3-reset-between-tests","title":"3. Reset Between Tests","text":"<pre><code>@pytest.fixture\ndef mock_llm():\n    llm = MockLLM()\n    yield llm\n    llm.reset()  # Clean up after each test\n</code></pre>"},{"location":"reference/testing/#4-test-error-cases","title":"4. Test Error Cases","text":"<pre><code>@app.route(\"/divide\", methods=[\"POST\"])\nasync def divide(a: int, b: int):\n    if b == 0:\n        return {\"error\": \"Division by zero\"}, 400\n    return {\"result\": a / b}\n\ndef test_error_handling(client):\n    response = client.post(\"/divide\", json={\"a\": 10, \"b\": 0})\n    assert response.status_code == 400\n    assert \"error\" in response.json()\n</code></pre>"},{"location":"reference/testing/#5-use-fixtures-for-dry","title":"5. Use Fixtures for DRY","text":"<pre><code>@pytest.fixture\ndef authenticated_client(client):\n    \"\"\"Pre-configured authenticated client.\"\"\"\n    client.headers = {\"Authorization\": \"Bearer test-token\"}\n    return client\n\ndef test_protected_route(authenticated_client):\n    response = authenticated_client.get(\"/protected\")\n    assert response.status_code == 200\n</code></pre>"},{"location":"reference/testing/#see-also","title":"See Also","text":"<ul> <li>Testing Guide - Complete testing guide</li> <li>Monitoring Reference - Test monitoring integration</li> <li>Background Jobs Reference - Test background jobs</li> </ul>"},{"location":"reference/types/","title":"types","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"reference/ui/","title":"UI Components API Reference","text":"<p>Complete API reference for RapidAI's UI components.</p>"},{"location":"reference/ui/#classes","title":"Classes","text":""},{"location":"reference/ui/#chatinterface","title":"ChatInterface","text":"<pre><code>@dataclass\nclass ChatInterface:\n    title: str = \"RapidAI Chat\"\n    theme: str = \"dark\"\n    show_timestamps: bool = True\n    enable_markdown: bool = True\n    enable_file_upload: bool = False\n    placeholder: str = \"Type your message...\"\n    max_height: str = \"600px\"\n    avatar_user: str = \"\ud83d\udc64\"\n    avatar_assistant: str = \"\ud83e\udd16\"\n</code></pre> <p>Configuration for chat interface.</p> <p>Attributes:</p> Attribute Type Default Description <code>title</code> <code>str</code> <code>\"RapidAI Chat\"</code> Chat interface title <code>theme</code> <code>str</code> <code>\"dark\"</code> Theme name (dark/light) <code>show_timestamps</code> <code>bool</code> <code>True</code> Show message timestamps <code>enable_markdown</code> <code>bool</code> <code>True</code> Enable markdown rendering <code>enable_file_upload</code> <code>bool</code> <code>False</code> Enable file upload button <code>placeholder</code> <code>str</code> <code>\"Type your message...\"</code> Input placeholder text <code>max_height</code> <code>str</code> <code>\"600px\"</code> Maximum chat height <code>avatar_user</code> <code>str</code> <code>\"\ud83d\udc64\"</code> User avatar emoji <code>avatar_assistant</code> <code>str</code> <code>\"\ud83e\udd16\"</code> Assistant avatar emoji <p>Example:</p> <pre><code>from rapidai.ui import ChatInterface\n\nconfig = ChatInterface(\n    title=\"Customer Support\",\n    theme=\"light\",\n    show_timestamps=True,\n    enable_markdown=True,\n    enable_file_upload=True,\n    placeholder=\"How can we help?\",\n    max_height=\"800px\",\n    avatar_user=\"\ud83d\ude0a\",\n    avatar_assistant=\"\ud83e\udd1d\"\n)\n</code></pre>"},{"location":"reference/ui/#functions","title":"Functions","text":""},{"location":"reference/ui/#get_chat_template","title":"get_chat_template","text":"<pre><code>def get_chat_template(\n    config: Optional[ChatInterface] = None,\n    title: Optional[str] = None,\n    theme: Optional[str] = None,\n    **kwargs\n) -&gt; str\n</code></pre> <p>Generate HTML template for chat interface.</p> <p>Parameters:</p> Parameter Type Default Description <code>config</code> <code>ChatInterface</code> <code>None</code> Chat interface configuration <code>title</code> <code>str</code> <code>None</code> Chat title (shorthand) <code>theme</code> <code>str</code> <code>None</code> Theme name (shorthand) <code>**kwargs</code> - - Additional ChatInterface parameters <p>Returns: <code>str</code> - Complete HTML page as string</p> <p>Usage patterns:</p> <pre><code># Default configuration\nhtml = get_chat_template()\n\n# With custom title and theme\nhtml = get_chat_template(title=\"My Chat\", theme=\"light\")\n\n# With full configuration\nconfig = ChatInterface(\n    title=\"Support Bot\",\n    theme=\"dark\",\n    enable_file_upload=True\n)\nhtml = get_chat_template(config=config)\n\n# With kwargs\nhtml = get_chat_template(\n    title=\"AI Assistant\",\n    theme=\"light\",\n    placeholder=\"Ask me anything...\",\n    enable_markdown=True\n)\n</code></pre> <p>Template structure:</p> <p>The generated HTML includes: - Responsive layout with flexbox - Auto-resizing textarea - Keyboard shortcuts (Enter to send, Shift+Enter for new line) - Loading indicator - Auto-scroll to latest message - Markdown rendering (optional) - File upload (optional) - Custom theme colors - Smooth animations</p> <p>Default API endpoint:</p> <p>The template expects a POST endpoint at <code>/chat</code> that accepts:</p> <pre><code>{\n  \"message\": \"user message\"\n}\n</code></pre> <p>And returns:</p> <pre><code>{\n  \"response\": \"assistant response\"\n}\n</code></pre> <p>Example:</p> <pre><code>from rapidai import App\nfrom rapidai.ui import get_chat_template\n\napp = App()\n\n@app.route(\"/\")\nasync def index():\n    return get_chat_template(\n        title=\"My AI Chat\",\n        theme=\"dark\"\n    )\n\n@app.route(\"/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    # Process message\n    return {\"response\": \"Hello!\"}\n</code></pre>"},{"location":"reference/ui/#decorators","title":"Decorators","text":""},{"location":"reference/ui/#page","title":"@page","text":"<pre><code>def page(route: str) -&gt; Callable\n</code></pre> <p>Decorator to serve HTML content with proper content type.</p> <p>Parameters:</p> <ul> <li><code>route</code> (<code>str</code>) - Route path (informational, not used for routing)</li> </ul> <p>Returns: Decorator function that wraps async handlers</p> <p>Behavior:</p> <p>The decorator: 1. Calls the wrapped function to get HTML content 2. Wraps the HTML in a proper HTTP response:    <pre><code>{\n    \"status\": 200,\n    \"headers\": {\"Content-Type\": \"text/html; charset=utf-8\"},\n    \"body\": html_content\n}\n</code></pre></p> <p>Example:</p> <pre><code>from rapidai import App\nfrom rapidai.ui import page, get_chat_template\n\napp = App()\n\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return get_chat_template()\n\n@app.route(\"/dashboard\")\n@page(\"/dashboard\")\nasync def dashboard():\n    return \"&lt;h1&gt;Dashboard&lt;/h1&gt;\"\n</code></pre> <p>Without decorator:</p> <pre><code># Manual approach (not recommended)\n@app.route(\"/\")\nasync def index():\n    html = get_chat_template()\n    return {\n        \"status\": 200,\n        \"headers\": {\"Content-Type\": \"text/html; charset=utf-8\"},\n        \"body\": html\n    }\n</code></pre>"},{"location":"reference/ui/#themes","title":"Themes","text":""},{"location":"reference/ui/#built-in-themes","title":"Built-in Themes","text":""},{"location":"reference/ui/#dark-theme","title":"Dark Theme","text":"<pre><code>{\n    \"bg\": \"#0a0a0a\",\n    \"bg_secondary\": \"#1a1a1a\",\n    \"text\": \"#ffffff\",\n    \"text_secondary\": \"#999999\",\n    \"border\": \"#333333\",\n    \"user_bg\": \"#00ffff\",\n    \"user_text\": \"#000000\",\n    \"assistant_bg\": \"#2a2a2a\",\n    \"assistant_text\": \"#ffffff\",\n    \"input_bg\": \"#1a1a1a\"\n}\n</code></pre> <p>Usage:</p> <pre><code>get_chat_template(theme=\"dark\")\n</code></pre>"},{"location":"reference/ui/#light-theme","title":"Light Theme","text":"<pre><code>{\n    \"bg\": \"#ffffff\",\n    \"bg_secondary\": \"#f5f5f5\",\n    \"text\": \"#000000\",\n    \"text_secondary\": \"#666666\",\n    \"border\": \"#dddddd\",\n    \"user_bg\": \"#007bff\",\n    \"user_text\": \"#ffffff\",\n    \"assistant_bg\": \"#f0f0f0\",\n    \"assistant_text\": \"#000000\",\n    \"input_bg\": \"#ffffff\"\n}\n</code></pre> <p>Usage:</p> <pre><code>get_chat_template(theme=\"light\")\n</code></pre>"},{"location":"reference/ui/#custom-themes","title":"Custom Themes","text":"<p>To create a custom theme, modify the generated HTML:</p> <pre><code>from rapidai.ui import get_chat_template\n\ndef get_custom_template():\n    html = get_chat_template(theme=\"dark\")\n\n    # Replace colors\n    html = html.replace(\"#00ffff\", \"#ff6b6b\")  # Custom primary color\n    html = html.replace(\"#0a0a0a\", \"#1a1a2e\")  # Custom background\n\n    return html\n\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return get_custom_template()\n</code></pre>"},{"location":"reference/ui/#frontend-api","title":"Frontend API","text":""},{"location":"reference/ui/#javascript-interface","title":"JavaScript Interface","text":"<p>The chat template exposes these JavaScript functions:</p>"},{"location":"reference/ui/#addmessage","title":"addMessage","text":"<pre><code>function addMessage(role, content, timestamp = new Date())\n</code></pre> <p>Add a message to the chat interface.</p> <p>Parameters: - <code>role</code> (<code>string</code>) - Either \"user\" or \"assistant\" - <code>content</code> (<code>string</code>) - Message content (supports markdown if enabled) - <code>timestamp</code> (<code>Date</code>) - Message timestamp (optional)</p> <p>Example:</p> <pre><code>addMessage('user', 'Hello!');\naddMessage('assistant', 'Hi there!');\n</code></pre>"},{"location":"reference/ui/#sendmessage","title":"sendMessage","text":"<pre><code>async function sendMessage()\n</code></pre> <p>Send the current input value to the backend.</p> <p>Behavior: 1. Gets message from textarea 2. Adds user message to chat 3. Disables input and shows loading 4. POSTs to <code>/chat</code> endpoint 5. Adds assistant response 6. Re-enables input</p> <p>Endpoint requirements:</p> <pre><code>// POST /chat\n{\n  \"message\": \"user message\"\n}\n\n// Response\n{\n  \"response\": \"assistant response\"\n}\n</code></pre>"},{"location":"reference/ui/#rendermarkdown","title":"renderMarkdown","text":"<pre><code>function renderMarkdown(text)\n</code></pre> <p>Render markdown text to HTML (if markdown enabled).</p> <p>Parameters: - <code>text</code> (<code>string</code>) - Markdown text</p> <p>Returns: HTML string</p> <p>Uses: - <code>marked.js</code> library if <code>enable_markdown=True</code> - Simple newline to <code>&lt;br&gt;</code> if <code>enable_markdown=False</code></p>"},{"location":"reference/ui/#dom-elements","title":"DOM Elements","text":"<p>IDs: - <code>#messages</code> - Messages container - <code>#message-input</code> - Input textarea - <code>#send-button</code> - Send button - <code>#loading</code> - Loading indicator - <code>#file-input</code> - File input (if enabled)</p> <p>Classes: - <code>.chat-container</code> - Main container - <code>.chat-header</code> - Header section - <code>.chat-messages</code> - Messages area - <code>.chat-input</code> - Input area - <code>.message</code> - Individual message - <code>.message.user</code> - User message - <code>.message.assistant</code> - Assistant message - <code>.message-avatar</code> - Message avatar - <code>.message-content</code> - Message content - <code>.message-text</code> - Message text - <code>.loading</code> - Loading indicator - <code>.loading.active</code> - Active loading state</p>"},{"location":"reference/ui/#customization-examples","title":"Customization Examples","text":""},{"location":"reference/ui/#custom-colors","title":"Custom Colors","text":"<pre><code>from rapidai.ui import get_chat_template\n\ndef get_branded_chat():\n    html = get_chat_template(theme=\"dark\")\n\n    # Replace brand colors\n    replacements = {\n        \"#00ffff\": \"#ff6b6b\",  # Primary color\n        \"#0a0a0a\": \"#1a1a2e\",  # Background\n        \"#1a1a1a\": \"#16213e\",  # Secondary background\n    }\n\n    for old, new in replacements.items():\n        html = html.replace(old, new)\n\n    return html\n</code></pre>"},{"location":"reference/ui/#custom-javascript","title":"Custom JavaScript","text":"<pre><code>from rapidai.ui import get_chat_template\n\ndef get_enhanced_chat():\n    base = get_chat_template()\n\n    # Add custom script\n    custom_script = \"\"\"\n    &lt;script&gt;\n        // Track analytics\n        window.addEventListener('load', () =&gt; {\n            console.log('Chat loaded');\n        });\n\n        // Custom send handler\n        const originalSend = sendMessage;\n        sendMessage = async function() {\n            console.log('Sending message...');\n            await originalSend();\n            console.log('Message sent');\n        };\n    &lt;/script&gt;\n    \"\"\"\n\n    # Insert before &lt;/body&gt;\n    return base.replace(\"&lt;/body&gt;\", f\"{custom_script}&lt;/body&gt;\")\n</code></pre>"},{"location":"reference/ui/#custom-styles","title":"Custom Styles","text":"<pre><code>from rapidai.ui import get_chat_template\n\ndef get_styled_chat():\n    base = get_chat_template()\n\n    custom_css = \"\"\"\n    &lt;style&gt;\n        .chat-header {\n            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n        }\n\n        .message-text {\n            border-radius: 20px;\n        }\n\n        .message.user .message-text {\n            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);\n        }\n    &lt;/style&gt;\n    \"\"\"\n\n    return base.replace(\"&lt;/head&gt;\", f\"{custom_css}&lt;/head&gt;\")\n</code></pre>"},{"location":"reference/ui/#add-features","title":"Add Features","text":"<pre><code>from rapidai.ui import get_chat_template\n\ndef get_chat_with_clear():\n    base = get_chat_template()\n\n    # Add clear button\n    clear_button = \"\"\"\n    &lt;button id=\"clear-button\" style=\"margin-top: 10px; padding: 0.5rem 1rem; background: #dc3545; color: white; border: none; border-radius: 8px; cursor: pointer;\"&gt;\n        Clear Chat\n    &lt;/button&gt;\n    \"\"\"\n\n    # Add clear script\n    clear_script = \"\"\"\n    &lt;script&gt;\n        document.getElementById('clear-button').addEventListener('click', () =&gt; {\n            messagesContainer.innerHTML = '';\n            addMessage('assistant', 'Hello! How can I help you today?');\n        });\n    &lt;/script&gt;\n    \"\"\"\n\n    html = base.replace(\"&lt;/div&gt;\\n        &lt;/div&gt;\\n\\n        &lt;div class=\\\"chat-input\\\"&gt;\",\n                       f\"{clear_button}&lt;/div&gt;\\n        &lt;/div&gt;\\n\\n        &lt;div class=\\\"chat-input\\\"&gt;\")\n\n    return html.replace(\"&lt;/body&gt;\", f\"{clear_script}&lt;/body&gt;\")\n</code></pre>"},{"location":"reference/ui/#complete-integration-example","title":"Complete Integration Example","text":"<pre><code>from rapidai import App, LLM\nfrom rapidai.memory import ConversationMemory\nfrom rapidai.ui import page, ChatInterface, get_chat_template\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\nmemory = ConversationMemory()\n\n# Configure interface\ninterface = ChatInterface(\n    title=\"AI Assistant\",\n    theme=\"light\",\n    show_timestamps=True,\n    enable_markdown=True,\n    enable_file_upload=False,\n    placeholder=\"Ask me anything...\",\n    max_height=\"700px\",\n    avatar_user=\"\ud83d\udc64\",\n    avatar_assistant=\"\ud83e\udd16\"\n)\n\n# Serve UI\n@app.route(\"/\")\n@page(\"/\")\nasync def index():\n    return get_chat_template(config=interface)\n\n# Chat endpoint\n@app.route(\"/chat\", methods=[\"POST\"])\nasync def chat(message: str, user_id: str = \"default\"):\n    # Add to memory\n    memory.add_message(user_id, \"user\", message)\n\n    # Get history\n    history = memory.get_history(user_id, limit=10)\n\n    # Generate response\n    response = await llm.chat(history)\n\n    # Add to memory\n    memory.add_message(user_id, \"assistant\", response)\n\n    return {\"response\": response}\n\n# Clear endpoint\n@app.route(\"/clear\", methods=[\"POST\"])\nasync def clear(user_id: str = \"default\"):\n    memory.clear(user_id)\n    return {\"message\": \"Cleared\"}\n\nif __name__ == \"__main__\":\n    app.run()\n</code></pre>"},{"location":"reference/ui/#see-also","title":"See Also","text":"<ul> <li>UI Guide - Complete UI usage guide</li> <li>Testing - Test UI components</li> <li>Deployment - Deploy UI applications</li> </ul>"},{"location":"tutorial/caching/","title":"caching","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"tutorial/deployment/","title":"Deployment Tutorial","text":"<p>Learn how to deploy your RapidAI applications to production environments.</p>"},{"location":"tutorial/deployment/#overview","title":"Overview","text":"<p>RapidAI applications can be deployed to various platforms:</p> <ul> <li>Vercel - Serverless deployment with zero config</li> <li>Railway - Container deployment with automatic HTTPS</li> <li>Render - Web services with persistent storage</li> <li>AWS Lambda - Serverless with Mangum adapter</li> </ul> <p>This tutorial covers deployment strategies, configuration, and best practices.</p>"},{"location":"tutorial/deployment/#prerequisites","title":"Prerequisites","text":"<p>Before deploying, ensure:</p> <ul> <li>Your application works locally</li> <li>Dependencies are specified in <code>requirements.txt</code></li> <li>Environment variables are documented</li> <li>Tests pass successfully</li> </ul>"},{"location":"tutorial/deployment/#quick-deploy-with-cli","title":"Quick Deploy with CLI","text":"<p>The RapidAI CLI provides one-command deployment:</p> <pre><code># Deploy to Vercel\nrapidai deploy --platform vercel\n\n# Deploy to Railway\nrapidai deploy --platform railway\n\n# Deploy to Render\nrapidai deploy --platform render\n\n# Deploy to AWS Lambda\nrapidai deploy --platform lambda\n</code></pre>"},{"location":"tutorial/deployment/#platform-specific-guides","title":"Platform-Specific Guides","text":""},{"location":"tutorial/deployment/#vercel-deployment","title":"Vercel Deployment","text":"<p>Step 1: Install Vercel CLI</p> <pre><code>npm install -g vercel\n</code></pre> <p>Step 2: Create vercel.json</p> <pre><code>{\n  \"builds\": [\n    {\n      \"src\": \"app.py\",\n      \"use\": \"@vercel/python\"\n    }\n  ],\n  \"routes\": [\n    {\n      \"src\": \"/(.*)\",\n      \"dest\": \"app.py\"\n    }\n  ]\n}\n</code></pre> <p>Step 3: Deploy</p> <pre><code>vercel --prod\n</code></pre> <p>Or use CLI:</p> <pre><code>rapidai deploy --platform vercel\n</code></pre> <p>Environment Variables:</p> <p>Set in Vercel dashboard or via CLI:</p> <pre><code>vercel env add RAPIDAI_LLM_API_KEY\nvercel env add RAPIDAI_CACHE_REDIS_URL\n</code></pre>"},{"location":"tutorial/deployment/#railway-deployment","title":"Railway Deployment","text":"<p>Step 1: Install Railway CLI</p> <pre><code>npm install -g @railway/cli\n</code></pre> <p>Step 2: Initialize Project</p> <pre><code>railway login\nrailway init\n</code></pre> <p>Step 3: Configure Start Command</p> <p>Create <code>Procfile</code>:</p> <pre><code>web: python app.py\n</code></pre> <p>Or set in railway.toml:</p> <pre><code>[build]\nbuilder = \"NIXPACKS\"\n\n[deploy]\nstartCommand = \"python app.py\"\nnumReplicas = 1\nrestartPolicyType = \"ON_FAILURE\"\nrestartPolicyMaxRetries = 10\n</code></pre> <p>Step 4: Deploy</p> <pre><code>railway up\n</code></pre> <p>Or use CLI:</p> <pre><code>rapidai deploy --platform railway\n</code></pre> <p>Environment Variables:</p> <pre><code>railway variables set RAPIDAI_LLM_API_KEY=sk-...\nrailway variables set RAPIDAI_APP_PORT=8000\n</code></pre>"},{"location":"tutorial/deployment/#render-deployment","title":"Render Deployment","text":"<p>Step 1: Create render.yaml</p> <pre><code>services:\n  - type: web\n    name: rapidai-app\n    env: python\n    buildCommand: pip install -r requirements.txt\n    startCommand: python app.py\n    envVars:\n      - key: RAPIDAI_LLM_API_KEY\n        sync: false\n      - key: RAPIDAI_APP_PORT\n        value: 8000\n</code></pre> <p>Step 2: Connect Git Repository</p> <ol> <li>Push code to GitHub</li> <li>Go to Render dashboard</li> <li>Create new Web Service</li> <li>Connect repository</li> </ol> <p>Step 3: Deploy</p> <p>Render automatically deploys on push.</p> <p>Or use CLI:</p> <pre><code>rapidai deploy --platform render\n</code></pre> <p>Environment Variables:</p> <p>Set in Render dashboard under Environment tab.</p>"},{"location":"tutorial/deployment/#aws-lambda-deployment","title":"AWS Lambda Deployment","text":"<p>Step 1: Install AWS CLI</p> <pre><code>pip install awscli\naws configure\n</code></pre> <p>Step 2: Create Lambda Handler</p> <pre><code># lambda_handler.py\nfrom mangum import Mangum\nfrom app import app\n\nhandler = Mangum(app.asgi_app)\n</code></pre> <p>Step 3: Package Dependencies</p> <pre><code>pip install -r requirements.txt -t package/\ncd package\nzip -r ../deployment.zip .\ncd ..\nzip -g deployment.zip lambda_handler.py app.py\n</code></pre> <p>Step 4: Create Lambda Function</p> <pre><code>aws lambda create-function \\\n  --function-name rapidai-app \\\n  --runtime python3.11 \\\n  --role arn:aws:iam::ACCOUNT:role/lambda-role \\\n  --handler lambda_handler.handler \\\n  --zip-file fileb://deployment.zip \\\n  --timeout 30 \\\n  --memory-size 512\n</code></pre> <p>Or use CLI:</p> <pre><code>rapidai deploy --platform lambda\n</code></pre> <p>Environment Variables:</p> <pre><code>aws lambda update-function-configuration \\\n  --function-name rapidai-app \\\n  --environment Variables={RAPIDAI_LLM_API_KEY=sk-...}\n</code></pre>"},{"location":"tutorial/deployment/#production-configuration","title":"Production Configuration","text":""},{"location":"tutorial/deployment/#environment-variables","title":"Environment Variables","text":"<p>Required:</p> <pre><code># LLM Configuration\nexport RAPIDAI_LLM_PROVIDER=anthropic\nexport RAPIDAI_LLM_MODEL=claude-3-haiku-20240307\nexport RAPIDAI_LLM_API_KEY=sk-ant-...\n\n# App Configuration\nexport RAPIDAI_APP_DEBUG=false\nexport RAPIDAI_APP_HOST=0.0.0.0\nexport RAPIDAI_APP_PORT=8000\n</code></pre> <p>Optional (Production):</p> <pre><code># Redis for caching\nexport RAPIDAI_CACHE_BACKEND=redis\nexport RAPIDAI_CACHE_REDIS_URL=redis://redis:6379\n\n# Redis for memory\nexport RAPIDAI_MEMORY_BACKEND=redis\nexport RAPIDAI_MEMORY_REDIS_URL=redis://redis:6379\n\n# Redis for background jobs\nexport RAPIDAI_BACKGROUND_BACKEND=redis\nexport RAPIDAI_BACKGROUND_REDIS_URL=redis://redis:6379\n</code></pre>"},{"location":"tutorial/deployment/#configuration-file","title":"Configuration File","text":"<p>Create <code>config.production.yaml</code>:</p> <pre><code>app:\n  title: Production App\n  debug: false\n  host: 0.0.0.0\n  port: 8000\n\nllm:\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  temperature: 0.7\n  max_tokens: 2048\n\ncache:\n  backend: redis\n  ttl: 7200\n  redis_url: ${REDIS_URL}\n\nmemory:\n  backend: redis\n  max_messages: 500\n  redis_url: ${REDIS_URL}\n\nrag:\n  top_k: 10\n  vectordb:\n    persist_directory: /data/chroma\n</code></pre> <p>Load in app:</p> <pre><code>from rapidai import App\nfrom rapidai.config import load_config\nimport os\n\nenv = os.getenv(\"ENVIRONMENT\", \"development\")\nconfig = load_config(f\"config.{env}.yaml\")\n\napp = App(title=config.app.title, debug=config.app.debug)\n</code></pre>"},{"location":"tutorial/deployment/#docker-deployment","title":"Docker Deployment","text":""},{"location":"tutorial/deployment/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application\nCOPY . .\n\n# Expose port\nEXPOSE 8000\n\n# Run application\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"tutorial/deployment/#docker-composeyml","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  app:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - RAPIDAI_LLM_API_KEY=${RAPIDAI_LLM_API_KEY}\n      - RAPIDAI_CACHE_BACKEND=redis\n      - RAPIDAI_CACHE_REDIS_URL=redis://redis:6379\n      - RAPIDAI_MEMORY_BACKEND=redis\n      - RAPIDAI_MEMORY_REDIS_URL=redis://redis:6379\n    depends_on:\n      - redis\n    restart: unless-stopped\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_data:/data\n    restart: unless-stopped\n\nvolumes:\n  redis_data:\n</code></pre>"},{"location":"tutorial/deployment/#build-and-run","title":"Build and Run","text":"<pre><code># Build image\ndocker build -t rapidai-app .\n\n# Run container\ndocker run -p 8000:8000 \\\n  -e RAPIDAI_LLM_API_KEY=sk-... \\\n  rapidai-app\n\n# Or use docker-compose\ndocker-compose up -d\n</code></pre>"},{"location":"tutorial/deployment/#health-checks","title":"Health Checks","text":"<p>Add health check endpoint:</p> <pre><code>from rapidai import App\n\napp = App()\n\n@app.route(\"/health\")\nasync def health():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"version\": \"1.0.0\"\n    }\n\n@app.route(\"/ready\")\nasync def ready():\n    \"\"\"Readiness check endpoint.\"\"\"\n    # Check dependencies\n    try:\n        # Test Redis connection\n        cache = get_cache()\n        await cache.get(\"health_check\")\n        return {\"status\": \"ready\"}\n    except Exception as e:\n        return {\"status\": \"not_ready\", \"error\": str(e)}, 503\n</code></pre> <p>Configure in platforms:</p> <p>Vercel: <pre><code>{\n  \"healthCheck\": {\n    \"path\": \"/health\"\n  }\n}\n</code></pre></p> <p>Railway: <pre><code>[deploy]\nhealthcheckPath = \"/health\"\nhealthcheckTimeout = 10\n</code></pre></p> <p>Render: <pre><code>services:\n  - type: web\n    healthCheckPath: /health\n</code></pre></p>"},{"location":"tutorial/deployment/#monitoring","title":"Monitoring","text":""},{"location":"tutorial/deployment/#add-monitoring-endpoint","title":"Add Monitoring Endpoint","text":"<pre><code>from rapidai.monitoring import get_collector, get_dashboard_html\n\n@app.route(\"/metrics\")\nasync def metrics():\n    \"\"\"Metrics dashboard.\"\"\"\n    return get_dashboard_html()\n\n@app.route(\"/metrics/json\")\nasync def metrics_json():\n    \"\"\"Metrics as JSON.\"\"\"\n    collector = get_collector()\n    return collector.get_summary()\n</code></pre>"},{"location":"tutorial/deployment/#external-monitoring","title":"External Monitoring","text":"<p>Prometheus:</p> <pre><code>@app.route(\"/metrics/prometheus\")\nasync def prometheus():\n    \"\"\"Prometheus metrics.\"\"\"\n    collector = get_collector()\n    summary = collector.get_summary()\n\n    metrics = []\n    metrics.append(\"# HELP rapidai_requests_total Total requests\")\n    metrics.append(\"# TYPE rapidai_requests_total counter\")\n    metrics.append(f\"rapidai_requests_total {summary['total_requests']}\")\n\n    return {\"body\": \"\\n\".join(metrics), \"headers\": {\"Content-Type\": \"text/plain\"}}\n</code></pre> <p>CloudWatch:</p> <pre><code>import boto3\n\ncloudwatch = boto3.client('cloudwatch')\n\n@app.on_interval(seconds=60)\nasync def push_metrics():\n    \"\"\"Push metrics to CloudWatch.\"\"\"\n    collector = get_collector()\n    summary = collector.get_summary()\n\n    cloudwatch.put_metric_data(\n        Namespace='RapidAI',\n        MetricData=[\n            {\n                'MetricName': 'Requests',\n                'Value': summary['total_requests'],\n                'Unit': 'Count'\n            },\n            {\n                'MetricName': 'ErrorRate',\n                'Value': 1 - summary['success_rate'],\n                'Unit': 'Percent'\n            }\n        ]\n    )\n</code></pre>"},{"location":"tutorial/deployment/#scaling-strategies","title":"Scaling Strategies","text":""},{"location":"tutorial/deployment/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Multiple Instances:</p> <p>Railway: <pre><code>[deploy]\nnumReplicas = 3\n</code></pre></p> <p>Render: <pre><code>services:\n  - type: web\n    numInstances: 3\n</code></pre></p> <p>Load Balancing:</p> <p>All platforms provide automatic load balancing across replicas.</p>"},{"location":"tutorial/deployment/#vertical-scaling","title":"Vertical Scaling","text":"<p>Memory Limits:</p> <p>Railway: <pre><code>railway service settings --memory 2048\n</code></pre></p> <p>Render: <pre><code>services:\n  - type: web\n    plan: standard  # 512 MB RAM\n</code></pre></p> <p>Lambda: <pre><code>aws lambda update-function-configuration \\\n  --function-name rapidai-app \\\n  --memory-size 1024\n</code></pre></p>"},{"location":"tutorial/deployment/#caching-strategy","title":"Caching Strategy","text":"<p>Use Redis for shared caching:</p> <pre><code>from rapidai.cache import cache\n\n@app.route(\"/chat\")\n@cache(ttl=3600, backend=\"redis\")\nasync def chat(message: str):\n    # Shared cache across instances\n    return await llm.complete(message)\n</code></pre>"},{"location":"tutorial/deployment/#security-best-practices","title":"Security Best Practices","text":""},{"location":"tutorial/deployment/#api-keys","title":"API Keys","text":"<p>Never commit API keys:</p> <pre><code># \u274c Don't do this\nllm = LLM(api_key=\"sk-ant-...\")\n\n# \u2705 Use environment variables\nllm = LLM()  # Reads from RAPIDAI_LLM_API_KEY\n</code></pre> <p>Use secrets management:</p> <p>Vercel: <pre><code>vercel secrets add rapidai-api-key sk-...\n</code></pre></p> <p>Railway: <pre><code>railway variables set RAPIDAI_LLM_API_KEY=sk-...\n</code></pre></p>"},{"location":"tutorial/deployment/#https","title":"HTTPS","text":"<p>All platforms provide automatic HTTPS:</p> <ul> <li>Vercel: Automatic with *.vercel.app</li> <li>Railway: Automatic with *.railway.app</li> <li>Render: Automatic with *.onrender.com</li> <li>Lambda: Use API Gateway with custom domain</li> </ul>"},{"location":"tutorial/deployment/#rate-limiting","title":"Rate Limiting","text":"<pre><code>from rapidai.middleware import RateLimiter\n\napp.add_middleware(\n    RateLimiter,\n    requests_per_minute=60,\n    burst_size=10\n)\n</code></pre>"},{"location":"tutorial/deployment/#cors","title":"CORS","text":"<pre><code>from rapidai.middleware import CORS\n\napp.add_middleware(\n    CORS,\n    allow_origins=[\"https://myapp.com\"],\n    allow_methods=[\"GET\", \"POST\"],\n    allow_headers=[\"Content-Type\", \"Authorization\"]\n)\n</code></pre>"},{"location":"tutorial/deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial/deployment/#build-failures","title":"Build Failures","text":"<p>Check Python version:</p> <pre><code># railway.toml\n[build]\nnixpacksPython = \"3.11\"\n</code></pre> <pre><code># render.yaml\nservices:\n  - type: web\n    runtime: python3.11\n</code></pre> <p>Missing dependencies:</p> <p>Ensure all dependencies in <code>requirements.txt</code>:</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre>"},{"location":"tutorial/deployment/#timeout-issues","title":"Timeout Issues","text":"<p>Increase timeouts:</p> <p>Railway: <pre><code>[deploy]\nhealthcheckTimeout = 30\n</code></pre></p> <p>Lambda: <pre><code>aws lambda update-function-configuration \\\n  --timeout 60\n</code></pre></p>"},{"location":"tutorial/deployment/#memory-issues","title":"Memory Issues","text":"<p>Monitor memory usage:</p> <pre><code>import psutil\n\n@app.route(\"/stats\")\nasync def stats():\n    memory = psutil.Process().memory_info()\n    return {\n        \"memory_mb\": memory.rss / 1024 / 1024,\n        \"percent\": psutil.virtual_memory().percent\n    }\n</code></pre> <p>Increase memory:</p> <p>See Vertical Scaling section above.</p>"},{"location":"tutorial/deployment/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Reference - CLI deployment commands</li> <li>Configuration Reference - All configuration options</li> <li>Monitoring - Production monitoring setup</li> <li>Testing - Test before deploying</li> </ul>"},{"location":"tutorial/error-handling/","title":"error handling","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"tutorial/intro/","title":"Tutorial Introduction","text":"<p>Welcome to the RapidAI tutorial! \ud83d\ude80</p> <p>This tutorial will guide you through building increasingly sophisticated AI applications, from a simple chatbot to a production-ready system with RAG, caching, and deployment.</p>"},{"location":"tutorial/intro/#what-youll-build","title":"What You'll Build","text":"<p>By the end of this tutorial, you'll have built:</p> <ol> <li>Simple Chatbot - Basic streaming chat endpoint</li> <li>Stateful Assistant - Conversation memory and context</li> <li>Document Q&amp;A System - RAG-powered knowledge base</li> <li>Production API - Complete with caching, monitoring, and deployment</li> </ol>"},{"location":"tutorial/intro/#prerequisites","title":"Prerequisites","text":"<p>Before You Start</p> <ul> <li>Python 3.9 or higher installed</li> <li>Basic Python knowledge (functions, async/await)</li> <li>API key for Anthropic or OpenAI</li> <li>30-60 minutes of time</li> </ul>"},{"location":"tutorial/intro/#tutorial-structure","title":"Tutorial Structure","text":""},{"location":"tutorial/intro/#part-1-basics","title":"Part 1: Basics","text":"<ul> <li>Simple Chatbot - Your first AI endpoint</li> <li>Streaming Responses - Real-time output</li> <li>Conversation Memory - Stateful conversations</li> </ul>"},{"location":"tutorial/intro/#part-2-advanced-features","title":"Part 2: Advanced Features","text":"<ul> <li>Caching - Save money and time</li> <li>Multiple Providers - Provider flexibility</li> <li>Error Handling - Production readiness</li> </ul>"},{"location":"tutorial/intro/#part-3-production","title":"Part 3: Production","text":"<ul> <li>Configuration Management</li> <li>Testing Your App</li> <li>Performance Optimization</li> <li>Deployment Strategies</li> </ul>"},{"location":"tutorial/intro/#learning-path","title":"Learning Path","text":"<pre><code>graph LR\n    A[Installation] --&gt; B[Simple Chatbot]\n    B --&gt; C[Streaming]\n    C --&gt; D[Memory]\n    D --&gt; E[Caching]\n    E --&gt; F[Multi-Provider]\n    F --&gt; G[Production]</code></pre>"},{"location":"tutorial/intro/#tutorial-conventions","title":"Tutorial Conventions","text":"<p>Throughout this tutorial:</p> <p>Tips</p> <p>Helpful hints and best practices</p> <p>Warnings</p> <p>Common pitfalls to avoid</p> <p>Information</p> <p>Additional context and explanations</p> <p>Success</p> <p>Milestones and achievements</p>"},{"location":"tutorial/intro/#code-examples","title":"Code Examples","text":"<p>All code examples are:</p> <ul> <li>\u2705 Tested - Every example works out of the box</li> <li>\u2705 Complete - No missing imports or setup</li> <li>\u2705 Progressive - Each builds on the previous</li> <li>\u2705 Practical - Real-world use cases</li> </ul>"},{"location":"tutorial/intro/#getting-help","title":"Getting Help","text":"<p>Stuck? Here's how to get help:</p> <ol> <li>Check the docs - Search the reference documentation</li> <li>GitHub Issues - Report bugs or ask questions</li> <li>Discussions - Community Q&amp;A</li> <li>Discord - Join our community (coming soon)</li> </ol>"},{"location":"tutorial/intro/#ready-to-start","title":"Ready to Start?","text":"<p>Let's build your first AI application!</p> Start with Simple Chatbot \u2192 <p>Remember</p> <p>Every expert was once a beginner. Take your time, experiment, and have fun! \ud83c\udf89</p>"},{"location":"tutorial/memory/","title":"Conversation Memory","text":"<p>Learn how to implement stateful conversations with memory.</p> <p>Coming Soon</p> <p>This page is under construction. Check back soon!</p> <p>For now, see the Quick Start for a basic example.</p>"},{"location":"tutorial/multi-provider/","title":"multi provider","text":"<p>Coming Soon</p> <p>This page is under construction. Check back soon!</p>"},{"location":"tutorial/simple-chatbot/","title":"Simple Chatbot","text":"<p>Let's build your first AI chatbot with RapidAI!</p>"},{"location":"tutorial/simple-chatbot/#goal","title":"Goal","text":"<p>Create a basic chatbot that:</p> <ul> <li>\u2705 Accepts messages via HTTP POST</li> <li>\u2705 Responds using Claude or GPT</li> <li>\u2705 Returns complete responses (non-streaming)</li> </ul>"},{"location":"tutorial/simple-chatbot/#step-1-create-the-file","title":"Step 1: Create the File","text":"<p>Create <code>chatbot.py</code>:</p> chatbot.py<pre><code>from rapidai import App, LLM\n\n# Initialize app and LLM\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n# Define chat endpoint\n@app.route(\"/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    \"\"\"Process a chat message and return response.\"\"\"\n    response = await llm.chat(message)\n    return {\"response\": response}\n\n# Run the server\nif __name__ == \"__main__\":\n    app.run(port=8000)\n</code></pre> <p>Model Selection</p> <p>You can use any supported model:</p> <ul> <li>Anthropic: <code>claude-3-haiku-20240307</code>, <code>claude-3-sonnet-20240229</code></li> <li>OpenAI: <code>gpt-4o-mini</code>, <code>gpt-4o</code>, <code>gpt-4</code></li> </ul> <p>RapidAI auto-detects the provider!</p>"},{"location":"tutorial/simple-chatbot/#step-2-run-the-server","title":"Step 2: Run the Server","text":"python chatbot.py <p>You should see:</p> <pre><code>INFO:     Started server process [12345]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000\n</code></pre>"},{"location":"tutorial/simple-chatbot/#step-3-test-your-chatbot","title":"Step 3: Test Your Chatbot","text":""},{"location":"tutorial/simple-chatbot/#using-curl","title":"Using curl","text":"<pre><code>curl -X POST http://localhost:8000/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"message\": \"Hello! Can you introduce yourself?\"}'\n</code></pre>"},{"location":"tutorial/simple-chatbot/#using-python","title":"Using Python","text":"test_chatbot.py<pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:8000/chat\",\n    json={\"message\": \"Hello! Can you introduce yourself?\"}\n)\n\nprint(response.json())\n</code></pre>"},{"location":"tutorial/simple-chatbot/#using-httpie","title":"Using httpie","text":"<pre><code>http POST localhost:8000/chat message=\"Hello! Can you introduce yourself?\"\n</code></pre>"},{"location":"tutorial/simple-chatbot/#step-4-add-error-handling","title":"Step 4: Add Error Handling","text":"<p>Improve your chatbot with better error handling:</p> chatbot.py<pre><code>from rapidai import App, LLM\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    \"\"\"Process a chat message and return response.\"\"\"\n    # Validate input\n    if not message or not message.strip():\n        return {\"error\": \"Message cannot be empty\"}, 400\n\n    try:\n        response = await llm.chat(message)\n        return {\"response\": response}\n    except Exception as e:\n        return {\"error\": str(e)}, 500\n\nif __name__ == \"__main__\":\n    app.run(port=8000)\n</code></pre>"},{"location":"tutorial/simple-chatbot/#step-5-add-multiple-endpoints","title":"Step 5: Add Multiple Endpoints","text":"<p>Expand your chatbot with specialized endpoints:</p> chatbot.py<pre><code>from rapidai import App, LLM\n\napp = App()\nllm = LLM(\"claude-3-haiku-20240307\")\n\n@app.route(\"/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    \"\"\"General chat.\"\"\"\n    response = await llm.chat(message)\n    return {\"response\": response}\n\n@app.route(\"/joke\", methods=[\"GET\"])\nasync def joke():\n    \"\"\"Get a random joke.\"\"\"\n    response = await llm.chat(\"Tell me a short, funny joke.\")\n    return {\"joke\": response}\n\n@app.route(\"/fact\", methods=[\"GET\"])\nasync def fact():\n    \"\"\"Get a random fact.\"\"\"\n    response = await llm.chat(\"Tell me an interesting fact.\")\n    return {\"fact\": response}\n\n@app.route(\"/health\", methods=[\"GET\"])\nasync def health():\n    \"\"\"Health check.\"\"\"\n    return {\"status\": \"healthy\"}\n\nif __name__ == \"__main__\":\n    app.run(port=8000)\n</code></pre> <p>Test the new endpoints:</p> <pre><code># Get a joke\ncurl http://localhost:8000/joke\n\n# Get a fact\ncurl http://localhost:8000/fact\n\n# Health check\ncurl http://localhost:8000/health\n</code></pre>"},{"location":"tutorial/simple-chatbot/#understanding-the-code","title":"Understanding the Code","text":""},{"location":"tutorial/simple-chatbot/#the-app-object","title":"The App Object","text":"<pre><code>app = App()\n</code></pre> <p>Creates an ASGI application with:</p> <ul> <li>Automatic request parsing</li> <li>JSON response handling</li> <li>Error handling</li> <li>Middleware support</li> </ul>"},{"location":"tutorial/simple-chatbot/#the-llm-object","title":"The LLM Object","text":"<pre><code>llm = LLM(\"claude-3-haiku-20240307\")\n</code></pre> <p>Creates a unified LLM client that:</p> <ul> <li>Auto-detects provider (Anthropic, OpenAI, etc.)</li> <li>Handles authentication</li> <li>Manages rate limits</li> <li>Provides consistent API</li> </ul>"},{"location":"tutorial/simple-chatbot/#route-decorator","title":"Route Decorator","text":"<pre><code>@app.route(\"/chat\", methods=[\"POST\"])\nasync def chat(message: str):\n    ...\n</code></pre> <ul> <li>Registers endpoint at <code>/chat</code></li> <li>Accepts POST requests</li> <li>Extracts <code>message</code> from JSON body</li> <li>Returns JSON response</li> </ul>"},{"location":"tutorial/simple-chatbot/#asyncawait","title":"Async/Await","text":"<pre><code>response = await llm.chat(message)\n</code></pre> <p>All LLM calls are async for:</p> <ul> <li>Better performance</li> <li>Non-blocking I/O</li> <li>Concurrent requests</li> </ul>"},{"location":"tutorial/simple-chatbot/#common-issues","title":"Common Issues","text":""},{"location":"tutorial/simple-chatbot/#port-already-in-use","title":"Port Already in Use","text":"<p>If port 8000 is taken:</p> <pre><code>app.run(port=8001)  # Use different port\n</code></pre>"},{"location":"tutorial/simple-chatbot/#api-key-not-found","title":"API Key Not Found","text":"<p>Ensure <code>.env</code> file exists:</p> <pre><code>ANTHROPIC_API_KEY=sk-ant-your-key-here\n</code></pre>"},{"location":"tutorial/simple-chatbot/#import-errors","title":"Import Errors","text":"<p>Install required dependencies:</p> <pre><code>pip install rapidai[anthropic]\n</code></pre>"},{"location":"tutorial/simple-chatbot/#next-steps","title":"Next Steps","text":"<p>Chatbot Complete!</p> <p>You've built a working AI chatbot!</p> <p>Enhance Your Chatbot:</p> <ul> <li>Add Streaming - Real-time responses</li> <li>Add Memory - Remember conversations</li> <li>Add Caching - Save money</li> </ul> <p>Or Explore:</p> <ul> <li>Multi-Provider Setup</li> <li>Error Handling</li> <li>Testing</li> </ul> Next: Streaming Responses \u2192"},{"location":"tutorial/streaming/","title":"Streaming Responses","text":"<p>Learn how to implement real-time streaming responses with RapidAI.</p> <p>Coming Soon</p> <p>This page is under construction. Check back soon!</p> <p>For now, see the Quick Start for a basic example.</p>"}]}